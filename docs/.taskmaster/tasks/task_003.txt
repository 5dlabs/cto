# Task ID: 3
# Title: Transcript Parser and Stats Aggregator
# Status: pending
# Dependencies: None
# Priority: medium
# Description: Parse JSONL lines into domain events (tool_use, tool_result, assistant_text, completion, error) and aggregate tokens, cost, tool counts, error counts.
# Details:
Implementation:
- Define enum EventKind { ToolUse { kind: Bash|Write|Edit|Read|WebSearch, payload }, ToolResult { stdout, stderr }, AssistantText { text }, Completion { summary }, Error { message } }
- Define TranscriptEvent { kind, ts, model, tokens_in, tokens_out, run_ids, tool_name, file, cwd, diff, stdout, stderr }
- Parser reads serde_json::Value per line, tolerant to schema variations: detect keys like "type", "event", "role", "content", "tool", "name".
- Implement filters from Helm values: includeTools, includePatterns, minStdoutLength. Implement parity_mode (no filtering, minimal truncation).
- Aggregator maintains running totals: tokens_in/out, cost estimate (if per-token pricing available in event or via model mapping), tool counts by kind, error_count, start_ts.
- Provide ComputeCost(model, tokens_in, tokens_out) -> dollars | None. If transcript contains cost, prefer it; else use configured pricing map.
- Output to formatter pipeline as Vec<RenderableItem> with contextual metadata.
Pseudo-code:
for line in lines_rx { match parse_line(line) { Ok(ev) => { update_stats(&ev); tx.send((ev, stats_snapshot())); }, Err(e) => log_warn; } }
- Ensure zero-copy where possible; trim large payloads post-formatting to conform to Discord limits.


# Test Strategy:
Unit tests with fixture JSON lines for each event type. Fuzz unknown fields to ensure parser is resilient. Verify filters work and parity_mode bypasses them. Validate stats aggregation across a sequence (tokens/cost/tool counts). Add snapshot tests for stats snapshots.

# Subtasks:
## 1. Define domain types and serde representations [pending]
### Dependencies: None
### Description: Design EventKind enum and TranscriptEvent struct with serde compatibility and zero-copy-friendly fields to model tool_use, tool_result, assistant_text, completion, and error events.
### Details:
Implement EventKind { ToolUse { kind: Bash|Write|Edit|Read|WebSearch, payload }, ToolResult { stdout, stderr }, AssistantText { text }, Completion { summary }, Error { message } } and TranscriptEvent { kind, ts, model, tokens_in, tokens_out, run_ids, tool_name, file, cwd, diff, stdout, stderr }. Use serde(untagged) where helpful, borrow Cow/Bytes for large fields, and add doc comments explaining schema intent and extensibility.

## 2. Implement pricing map and ComputeCost [pending]
### Dependencies: 3.1
### Description: Provide per-model pricing map and a ComputeCost(model, tokens_in, tokens_out) -> Option<f64> function that prefers event-provided cost.
### Details:
Create a configurable pricing map keyed by model. Implement cost precedence: if an event includes cost, use it; otherwise compute from pricing map. Support currency formatting helpers and rounding. Handle missing model or unknown pricing gracefully (return None).

## 3. Build tolerant JSONL parser core [pending]
### Dependencies: 3.1
### Description: Parse serde_json::Value per line and map to TranscriptEvent, tolerating schema drift via flexible key detection.
### Details:
Implement parse_line(line: &str) -> Result<TranscriptEvent, ParseError>. Detect keys like type/event/role/content/tool/name; normalize variants. Classify errors (malformed_json, unknown_event, missing_required, trim_only). Preserve large payloads as borrowed where possible. Record model, tokens_in/out if present.

## 4. Add filters and parity_mode support [pending]
### Dependencies: 3.1, 3.3
### Description: Implement includeTools, includePatterns, minStdoutLength filters with a parity_mode that bypasses filtering.
### Details:
Introduce a Filters config: includeTools: [ToolKind], includePatterns: [Regex], minStdoutLength: usize. Apply filters post-parse, pre-emit. parity_mode = true disables all filtering except schema corrections. Provide config plumbing and edge-case handling (empty patterns, invalid regex).

## 5. Implement stats aggregator and snapshotting [pending]
### Dependencies: 3.1, 3.2
### Description: Maintain running totals for tokens_in/out, cost, tool counts by kind, error count, and start/end timestamps.
### Details:
Create Aggregator with update_stats(&TranscriptEvent) and stats_snapshot() methods. Track per-tool kind counts, earliest ts as start_ts, latest ts as end_ts. Integrate ComputeCost, accumulate cost with event-preferred values. Ensure thread-safe access with minimal cloning.

## 6. Wire parse→aggregate→emit pipeline [pending]
### Dependencies: 3.2, 3.3, 3.5
### Description: Integrate the parser and aggregator into a streaming loop that emits (event, stats_snapshot) to the next stage.
### Details:
Implement loop: for line in lines_rx { match parse_line { Ok(ev) => { if passes_filters(&ev) { aggregator.update_stats(&ev); tx.send((ev, aggregator.stats_snapshot())); } } Err(e) => log_warn; } }. Minimize allocations, avoid unnecessary copies, and ensure backpressure-safe channel usage.

## 7. Large payload handling and deferred truncation [pending]
### Dependencies: 3.1, 3.3, 3.6
### Description: Mark and manage large fields to allow post-formatting truncation while avoiding early copies.
### Details:
Identify fields like stdout, stderr, diff, and payload. Use borrow-friendly types and annotate items for the formatter to trim to platform limits (e.g., Discord). Ensure zero-copy parsing where feasible and avoid duplicating buffers before formatting.

## 8. Telemetry and structured logging [pending]
### Dependencies: 3.3, 3.6
### Description: Add per-line parse latency, error rates, dropped events, and structured logs for observability.
### Details:
Instrument parse_line and pipeline with tracing spans. Emit counters for parsed_events, filtered_events, errors_by_kind, and histograms for parse_latency_ms. Include contextual metadata (run_ids, model) in logs. Provide hooks for metrics export.

## 9. Comprehensive test suite [pending]
### Dependencies: 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8
### Description: Create fixtures and tests for parser resilience, filters, parity_mode, aggregator totals, and snapshot outputs.
### Details:
Add unit tests per event type, fuzz tests for unknown fields and extra keys, filter behavior tests (includeTools/patterns/minStdoutLength), parity_mode bypass tests, aggregator accumulation across sequences, and snapshot tests for stats_snapshot. Validate cost precedence and formatting.

## 10. Documentation and developer guide [pending]
### Dependencies: 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9
### Description: Document schema expectations, parsing strategy, filters, pricing overrides, and extension points.
### Details:
Write docs covering domain types, tolerant parsing rules, configuration (filters, parity_mode), pricing map override guidance, telemetry usage, and examples of input→output. Include notes on performance and zero-copy considerations.

