{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Initialize Rust Workspace, Crates, and CI",
        "description": "Create a Rust workspace with two binaries: sidecar watcher and Discord bot service. Set up common utilities crate, linting, formatting, and basic CI.",
        "details": "Structure:\n- crates/\n  - watcher/ (bin)\n  - bot/ (bin)\n  - common/ (lib)\n- Use Tokio async runtime for both services.\n- Dependencies (latest stable): tokio, serde, serde_json, anyhow, thiserror, tracing, tracing-subscriber, twilight-http, twilight-model, twilight-util (builders), reqwest (for HTTP bridge only), bytes, tokio-util, futures, clap, parking_lot, time, base64, ed25519-dalek (for interaction signature verification), warp or axum (HTTP server for bot interactions and pod-local bridge), notify (optional), sysinfo (optional metrics).\n- Enable release profile optimizations for size and perf: lto=true, codegen-units=1, opt-level=z for release.\n- Add .cargo/config.toml with RUSTFLAGS to reduce binary size where appropriate.\n- Set up GitHub Actions CI: build, clippy -D warnings, fmt --check, unit tests.\n- Common crate provides: types for transcript events, embed helpers, truncation utilities, constants for colors, rate-limit/backoff utils, webhook URL parser (extract id/token), and fencing sanitizers.\n- Security: never include bot token in watcher crate; only accepts webhook URL env.\nPseudo-code:\n- cargo new --workspace\n- cargo new crates/common --lib\n- cargo new crates/watcher --bin\n- cargo new crates/bot --bin\n- Implement feature flags: watcher: \"parity_mode\".\n- Add tracing init to both bins.\nTest scaffolding:\n- Add a dev profile and run lib tests.\n- Ensure builds pass in CI.",
        "testStrategy": "CI validates clippy, formatting, and unit tests in common. Verify that crates compile on stable Rust. Smoke run both binaries with --help. Ensure no secret leakage: scan logs for token placeholders.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Scaffold Rust workspace and crate structure",
            "description": "Create a new Rust workspace with three crates: common (lib), watcher (bin), and bot (bin). Ensure a clean repository layout matching the provided structure.",
            "dependencies": [],
            "details": "Steps:\n- Initialize repository and workspace: cargo new --vcs git --workspace .\n- Create crates: cargo new crates/common --lib; cargo new crates/watcher --bin; cargo new crates/bot --bin\n- Update root Cargo.toml with [workspace] members = [\"crates/common\", \"crates/watcher\", \"crates/bot\"]\n- Verify cargo build works with default templates\n- Add .gitignore for Rust (target/, Cargo.lock for libs as appropriate), and minimal README placeholder",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configure workspace dependencies and features",
            "description": "Add and pin shared dependencies, set MSRV, and configure feature flags (watcher: parity_mode).",
            "dependencies": [
              "1.1"
            ],
            "details": "Steps:\n- Use [workspace.dependencies] to centralize versions for: tokio (rt-multi-thread, macros, fs, io-util, signal), serde, serde_json, anyhow, thiserror, tracing, tracing-subscriber, reqwest (for bot HTTP bridge), bytes, tokio-util, futures, clap (derive), parking_lot, time, base64, ed25519-dalek, axum or warp (choose one), notify (optional), sysinfo (optional), twilight-http, twilight-model, twilight-util\n- In crates/common, add dependencies: serde, serde_json, thiserror, anyhow, time, base64; optionally twilight-model and twilight-util for embed helpers\n- In crates/watcher, add: tokio, tracing, tracing-subscriber, clap, reqwest (if needed later), bytes, tokio-util, futures, parking_lot, notify (optional), sysinfo (optional), serde/serde_json, anyhow/thiserror\n- In crates/bot, add: tokio, tracing, tracing-subscriber, clap, axum or warp, twilight-http, twilight-model, twilight-util, ed25519-dalek (for signature verification), serde/serde_json, anyhow/thiserror\n- Set rust-version (MSRV) in each crate Cargo.toml (e.g., 1.75 or current stable) and document in README\n- Define feature flags: in watcher Cargo.toml, [features] parity_mode = [] (not default)\n- Ensure both bins depend on common",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Optimize Cargo profiles for size and performance",
            "description": "Configure release and dev profiles for smaller, optimized binaries per requirements.",
            "dependencies": [
              "1.1",
              "1.2"
            ],
            "details": "Steps:\n- In root Cargo.toml, set [profile.release]: lto = true, codegen-units = 1, opt-level = \"z\", strip = true if supported\n- Set [profile.dev] for faster local builds (e.g., opt-level = 1) and add [profile.dev.package.*] overrides if useful\n- Add [profile.test] with similar settings as dev where appropriate\n- Add comments explaining trade-offs and how to override locally",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add .cargo/config.toml RUSTFLAGS and build tuning",
            "description": "Provide cross-platform safe RUSTFLAGS to reduce size and control panics/linking behavior.",
            "dependencies": [
              "1.1",
              "1.3"
            ],
            "details": "Steps:\n- Create .cargo/config.toml with:\n  - For release builds: -C strip=symbols, -C panic=abort (if acceptable), -Z if not using nightly (avoid), set via build.rustflags guarded by CARGO_PROFILE if needed\n  - Target-specific sections to avoid unsupported flags on Windows/macOS\n  - Consider linker settings only when safe (e.g., no target-cpu=native by default)\n- Document how to temporarily disable panic=abort for test runs if required",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Set up formatting and linting",
            "description": "Add rustfmt and clippy configurations, ensuring clippy -D warnings and fmt --check pass locally.",
            "dependencies": [
              "1.1"
            ],
            "details": "Steps:\n- Add rustfmt.toml with stable-friendly options (edition formatting, imports_granularity, group_imports, max_width)\n- Add clippy configuration (Clippy.toml or workspace-level allow/deny list) to fail on warnings\n- Run cargo fmt --all and cargo clippy --workspace --all-features -- -D warnings to verify baseline\n- Add basic module doc headers to suppress missing_docs only where necessary",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement common crate modules and unit tests",
            "description": "Create stubs and minimal implementations for shared utilities: types, embed helpers, truncation, constants, backoff, webhook URL parser, and sanitizers, with unit tests.",
            "dependencies": [
              "1.2"
            ],
            "details": "Steps:\n- Modules:\n  - types: transcript event enums/structs with serde derives\n  - embeds: helper fns/builders scaffolding (may use twilight-model/util; return minimal builders or placeholder structs)\n  - truncate: safe truncation preserving code fences and UTF-8 boundaries\n  - constants: color palette and shared limits\n  - backoff: exponential backoff with jitter; expose iterator or fn\n  - webhook: parse Discord webhook URL into {id, token}; handle edge cases and return Result\n  - sanitize: fence/code block balancing, escaping backticks\n- Add unit tests for webhook parser, sanitizers, backoff behavior, truncation edge cases\n- Ensure crate compiles independently and tests pass",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Scaffold watcher and bot binaries",
            "description": "Add minimal runnable mains for watcher and bot using Tokio, Clap, and tracing with safe configuration and feature flags.",
            "dependencies": [
              "1.2",
              "1.6"
            ],
            "details": "Steps:\n- watcher:\n  - tokio::main, initialize tracing-subscriber with env filter\n  - Clap CLI with --help/--version and flags/env (e.g., DISCORD_WEBHOOK_URL), include --parity-mode behind feature \"parity_mode\"\n  - Ensure no bot token references; only accept webhook URL\n  - Return 0 after printing help; add placeholder logic guarded behind feature flags\n- bot:\n  - tokio::main, tracing init\n  - Clap for basic options; read DISCORD_BOT_TOKEN from env\n  - Start minimal HTTP server (axum or warp) exposing /healthz returning 200\n  - Wire twilight HTTP client creation behind a placeholder function\n- Verify both binaries compile and run with --help",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Create GitHub Actions CI workflow",
            "description": "Add CI that builds, lints, formats, and tests across OS matrix with caching and PR triggers.",
            "dependencies": [
              "1.3",
              "1.4",
              "1.5",
              "1.6",
              "1.7"
            ],
            "details": "Steps:\n- Workflow triggers: pull_request, push to main\n- Matrix: os = [ubuntu-latest, macos-latest]; toolchain = stable\n- Steps: checkout, cache Cargo registry/target, install Rust toolchain, rustfmt check, clippy -D warnings, cargo build --workspace --release, cargo test --workspace\n- Smoke run: run crates/watcher -- --help and crates/bot -- --help (Linux at least)\n- Ensure secrets are not printed; redaction for env outputs\n- Require CI to be green before merge",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Repo hygiene and acceptance verification",
            "description": "Add LICENSE and README with build/run instructions, security notes, and verify acceptance criteria including binary size and no secret leakage.",
            "dependencies": [
              "1.8"
            ],
            "details": "Steps:\n- Add LICENSE (e.g., MIT/Apache-2.0 dual) and expand README with: project overview, MSRV, workspace layout, how to build, run, and test; security note that watcher only uses webhook URL and never bot token\n- Add dev profile notes and contribution guidelines for fmt/clippy\n- Validate acceptance:\n  - cargo build/test/clippy/fmt all pass locally\n  - Both bins run with --help\n  - Common crate unit tests pass\n  - Compare binary sizes: cargo build --release and record sizes to confirm reductions from profiles/flags\n  - Confirm logs do not include secrets (document redaction and environment usage)\n- Open initial PR to exercise CI and document outcomes",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 2,
        "title": "Implement JSONL Tailer with ≤100ms Polling",
        "description": "Build an efficient tailer to stream new lines from the Claude append-only JSONL transcript with at most 100ms latency and low overhead.",
        "details": "Requirements:\n- Default path: ~/.claude/projects/<encoded-workspace>/<session>.jsonl. Allow override via WORKSPACE_PATH and SESSION env vars or direct TRANSCRIPT_PATH.\n- Handle file discovery retries and late creation; treat file as append-only; tolerate rotations.\n- Use a non-blocking poll loop every POLL_INTERVAL_MS (default 100ms) to read new bytes and split into lines.\n- Avoid busy-wait: sleep for interval when no new data.\n- Keep file descriptor and current offset; if inode changes, reopen and continue.\n- Use buffered reading via tokio::fs::File and tokio::io::AsyncReadExt with a BytesMut buffer.\nPseudo-code:\nloop {\n  path = resolve_transcript_path();\n  file = open_with_retry(path).await;\n  let mut offset = file.metadata().await?.len();\n  loop {\n    let new_len = file.metadata().await?.len();\n    if new_len > offset { read_range(file, offset..new_len).await -> buffer; offset = new_len; emit_lines(buffer); }\n    tokio::time::sleep(interval).await;\n  }\n}\n- Emit each complete line via mpsc channel to the parser stage.\n- Expose backpressure: bounded channel, drop or coalesce if overwhelmed (log rate).\n- Resource constraints: avoid allocations by reusing buffers.\nConfiguration:\n- ENV: TRANSCRIPT_PATH, POLL_INTERVAL_MS (default 100), DISCOVERY_RETRY_MS (default 1000).\nFailure handling:\n- If file missing, log once, retry discovery.\n- If read error, reopen file.\nMetrics:\n- Lines/sec, bytes/sec, tail latency.\n",
        "testStrategy": "Unit tests with a temp file: append lines in another task and assert tailer emits them within ~150ms average. Test file rotation by renaming and creating a new file. Property test for arbitrary line boundaries. Measure CPU with long idle; ensure sleep occurs. Use tokio::time::pause to simulate timing deterministically.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Path Resolution and Discovery",
            "description": "Implement transcript path resolution with env overrides and workspace/session discovery.",
            "dependencies": [],
            "details": "Implement resolve_transcript_path():\n- Resolution order: TRANSCRIPT_PATH -> (WORKSPACE_PATH + SESSION) -> error.\n- Construct default path: ~/.claude/projects/<encoded-workspace>/<session>.jsonl.\n- Expand ~ and environment variables; normalize separators; ensure parent dirs exist check is non-fatal (file may be late-created).\n- encoded-workspace: URL-safe base64 of UTF-8 WORKSPACE_PATH without padding; expose helper encode_workspace_path().\n- Return absolute PathBuf; log a single warning if path not found; do not spam logs during retries (handled by open logic).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Tailer Config and Initialization",
            "description": "Define configuration surface and builder for the tailer with sane defaults and validation.",
            "dependencies": [],
            "details": "Create TailerConfig with fields: transcript_path (Option<PathBuf>), poll_interval_ms (default 100), discovery_retry_ms (default 1000), channel_capacity (e.g., 1024), drop_policy (drop|coalesce), log_rate_window_ms (e.g., 1000), start_at_end (true), newline (\"\\n\" with CRLF tolerant).\n- Load from ENV; optionally accept clap-derived values injected by caller.\n- Validate bounds: poll_interval_ms in [10, 1000], discovery_retry_ms in [100, 60000], capacity > 0.\n- Provide Tailer::new(config) that wires path resolution (uses resolve_transcript_path if transcript_path is None).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "File Open/Reopen and Rotation Handling",
            "description": "Implement open_with_retry, late creation handling, inode change detection, and reopen on read errors.",
            "dependencies": [
              "2.1",
              "2.2"
            ],
            "details": "Implement async open_with_retry(path, discovery_retry_ms):\n- Loop: attempt tokio::fs::File::open; on NotFound, sleep discovery_retry_ms and retry; log once per minute at most.\n- On open, capture file identity: (dev, ino) on Unix via std::os::unix::fs::MetadataExt; on Windows use file_index_low/high via winapi or metadata fileid if available; store for rotation detection.\n- Determine initial offset: if start_at_end then metadata.len() else 0.\n- Provide detect_rotation(current_file): periodically compare stored identity to current metadata; if changed, close and reopen, resetting offset to new file's current len (to avoid replay) unless start_at_end is false.\n- On any read error (other than WouldBlock), close and reopen with backoff; preserve partial line buffer where possible.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Async Read Loop and Buffer Management",
            "description": "Implement non-blocking polling loop that reads appended bytes, maintains offset, and reuses buffers.",
            "dependencies": [
              "2.3",
              "2.2"
            ],
            "details": "Implement tail_loop(file, poll_interval_ms):\n- Use a reusable BytesMut read_buf and a BytesMut carry_buf for trailing partial line across iterations.\n- Each tick: get new_len = file.metadata().await?.len(). If new_len > offset: seek to offset (AsyncSeekExt), read up to (new_len - offset) into read_buf in chunks; advance offset as bytes consumed.\n- Avoid busy-wait: always tokio::time::sleep(poll_interval_ms) when no new bytes.\n- Minimize allocations: reserve capacity based on last read size; shrink_to_fit rarely (e.g., on rotations).\n- Handle CRLF and UTF-8 agnostic processing: split on '\\n'; retain preceding '\\r' stripping.\n- On rotation detected (from subtask 2.3), swap handle and update identity and offset; keep carry_buf.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Line Splitting and mpsc Emission with Backpressure",
            "description": "Split into complete lines and emit over a bounded channel with drop/coalesce policy and rate-limited logging.",
            "dependencies": [
              "2.4",
              "2.2"
            ],
            "details": "Implement line_split_and_emit(read_buf, carry_buf, sender):\n- Prepend carry_buf, split on '\\n'; for the final fragment without '\\n', store back into carry_buf.\n- Emit TailItem::Line { offset, ts_emitted, bytes } for each complete line; trim trailing '\\r'.\n- mpsc channel capacity from config; if full:\n  - drop policy: increment drop counter, skip sending, and periodically send TailItem::DropNotice { dropped_count } every log_rate_window_ms.\n  - coalesce policy: batch multiple lines into TailItem::Batch up to a limit, then send once space is available; if still full after a short try_send window, fall back to drop and log.\n- Ensure emission preserves ordering relative to file offset; avoid unbounded per-iteration batches to cap latency.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Metrics and Tracing Instrumentation",
            "description": "Expose lines/sec, bytes/sec, tail latency, drops, rotations, and idle CPU indicators with tracing spans.",
            "dependencies": [
              "2.4",
              "2.5"
            ],
            "details": "Integrate metrics crate (or prometheus exporter hooks) and tracing:\n- Counters: tailer_lines_total, tailer_bytes_total, tailer_dropped_lines_total, tailer_rotations_total, tailer_reopens_total.\n- Histograms: tailer_emit_latency_ms (time from file len increase observed to emission), tailer_poll_duration_ms, tailer_batch_size.\n- Gauges: tailer_channel_occupancy.\n- Tracing spans around poll/read/emit; log at INFO on start/rotation, WARN on persistent drops, ERROR on reopen failures.\n- Idle CPU check: track time spent sleeping vs active per minute and export tailer_idle_ratio gauge.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Test Suite: Timing, Rotation, Boundaries, and Idle",
            "description": "Write deterministic and integration-style async tests covering timing, rotation, line boundaries, and idle behavior.",
            "dependencies": [
              "2.3",
              "2.4",
              "2.5",
              "2.6"
            ],
            "details": "Tests:\n- Timing: with tokio::time::pause and advance, append lines to a temp file from another task; assert average emission latency <= 150ms under light load.\n- Rotation: write, rename current file, create a new one, append more; assert detection and continued tailing without duplicate lines.\n- Boundary property: using proptest, generate arbitrary chunk splits and CRLF/UTF-8 edge cases; assert emitted lines match ground truth joins on '\\n'.\n- Backpressure: saturate channel; verify drop/coalesce behavior and rate-limited warnings; counters increment.\n- Idle CPU: long idle period; assert sleeps occur (low poll CPU), using instrumentation of sleep vs active time.\n- Error recovery: inject read/open errors (via mock FS or fault injection) and assert reopen with backoff.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Documentation and Examples",
            "description": "Provide module docs, configuration reference, and runnable examples demonstrating usage and limitations.",
            "dependencies": [
              "2.1",
              "2.2",
              "2.3",
              "2.4",
              "2.5",
              "2.6",
              "2.7"
            ],
            "details": "Produce docs:\n- Overview of JSONL tailing assumptions (append-only, newline-terminated), path resolution rules, and rotation behavior.\n- Configuration reference for ENV and programmatic TailerConfig, including defaults and recommended values.\n- Backpressure policies and their trade-offs; guidance for choosing channel capacity.\n- Metrics catalog and example Prometheus queries; tracing examples.\n- Example: start tailer, receive mpsc lines, print or forward to parser; include code snippet with graceful shutdown.\n- Known limitations and platform notes (inode detection differences).",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 3,
        "title": "Transcript Parser and Stats Aggregator",
        "description": "Parse JSONL lines into domain events (tool_use, tool_result, assistant_text, completion, error) and aggregate tokens, cost, tool counts, error counts.",
        "details": "Implementation:\n- Define enum EventKind { ToolUse { kind: Bash|Write|Edit|Read|WebSearch, payload }, ToolResult { stdout, stderr }, AssistantText { text }, Completion { summary }, Error { message } }\n- Define TranscriptEvent { kind, ts, model, tokens_in, tokens_out, run_ids, tool_name, file, cwd, diff, stdout, stderr }\n- Parser reads serde_json::Value per line, tolerant to schema variations: detect keys like \"type\", \"event\", \"role\", \"content\", \"tool\", \"name\".\n- Implement filters from Helm values: includeTools, includePatterns, minStdoutLength. Implement parity_mode (no filtering, minimal truncation).\n- Aggregator maintains running totals: tokens_in/out, cost estimate (if per-token pricing available in event or via model mapping), tool counts by kind, error_count, start_ts.\n- Provide ComputeCost(model, tokens_in, tokens_out) -> dollars | None. If transcript contains cost, prefer it; else use configured pricing map.\n- Output to formatter pipeline as Vec<RenderableItem> with contextual metadata.\nPseudo-code:\nfor line in lines_rx { match parse_line(line) { Ok(ev) => { update_stats(&ev); tx.send((ev, stats_snapshot())); }, Err(e) => log_warn; } }\n- Ensure zero-copy where possible; trim large payloads post-formatting to conform to Discord limits.\n",
        "testStrategy": "Unit tests with fixture JSON lines for each event type. Fuzz unknown fields to ensure parser is resilient. Verify filters work and parity_mode bypasses them. Validate stats aggregation across a sequence (tokens/cost/tool counts). Add snapshot tests for stats snapshots.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define domain types and serde representations",
            "description": "Design EventKind enum and TranscriptEvent struct with serde compatibility and zero-copy-friendly fields to model tool_use, tool_result, assistant_text, completion, and error events.",
            "dependencies": [],
            "details": "Implement EventKind { ToolUse { kind: Bash|Write|Edit|Read|WebSearch, payload }, ToolResult { stdout, stderr }, AssistantText { text }, Completion { summary }, Error { message } } and TranscriptEvent { kind, ts, model, tokens_in, tokens_out, run_ids, tool_name, file, cwd, diff, stdout, stderr }. Use serde(untagged) where helpful, borrow Cow/Bytes for large fields, and add doc comments explaining schema intent and extensibility.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement pricing map and ComputeCost",
            "description": "Provide per-model pricing map and a ComputeCost(model, tokens_in, tokens_out) -> Option<f64> function that prefers event-provided cost.",
            "dependencies": [
              "3.1"
            ],
            "details": "Create a configurable pricing map keyed by model. Implement cost precedence: if an event includes cost, use it; otherwise compute from pricing map. Support currency formatting helpers and rounding. Handle missing model or unknown pricing gracefully (return None).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build tolerant JSONL parser core",
            "description": "Parse serde_json::Value per line and map to TranscriptEvent, tolerating schema drift via flexible key detection.",
            "dependencies": [
              "3.1"
            ],
            "details": "Implement parse_line(line: &str) -> Result<TranscriptEvent, ParseError>. Detect keys like type/event/role/content/tool/name; normalize variants. Classify errors (malformed_json, unknown_event, missing_required, trim_only). Preserve large payloads as borrowed where possible. Record model, tokens_in/out if present.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add filters and parity_mode support",
            "description": "Implement includeTools, includePatterns, minStdoutLength filters with a parity_mode that bypasses filtering.",
            "dependencies": [
              "3.1",
              "3.3"
            ],
            "details": "Introduce a Filters config: includeTools: [ToolKind], includePatterns: [Regex], minStdoutLength: usize. Apply filters post-parse, pre-emit. parity_mode = true disables all filtering except schema corrections. Provide config plumbing and edge-case handling (empty patterns, invalid regex).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement stats aggregator and snapshotting",
            "description": "Maintain running totals for tokens_in/out, cost, tool counts by kind, error count, and start/end timestamps.",
            "dependencies": [
              "3.1",
              "3.2"
            ],
            "details": "Create Aggregator with update_stats(&TranscriptEvent) and stats_snapshot() methods. Track per-tool kind counts, earliest ts as start_ts, latest ts as end_ts. Integrate ComputeCost, accumulate cost with event-preferred values. Ensure thread-safe access with minimal cloning.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Wire parse→aggregate→emit pipeline",
            "description": "Integrate the parser and aggregator into a streaming loop that emits (event, stats_snapshot) to the next stage.",
            "dependencies": [
              "3.2",
              "3.3",
              "3.5"
            ],
            "details": "Implement loop: for line in lines_rx { match parse_line { Ok(ev) => { if passes_filters(&ev) { aggregator.update_stats(&ev); tx.send((ev, aggregator.stats_snapshot())); } } Err(e) => log_warn; } }. Minimize allocations, avoid unnecessary copies, and ensure backpressure-safe channel usage.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Large payload handling and deferred truncation",
            "description": "Mark and manage large fields to allow post-formatting truncation while avoiding early copies.",
            "dependencies": [
              "3.1",
              "3.3",
              "3.6"
            ],
            "details": "Identify fields like stdout, stderr, diff, and payload. Use borrow-friendly types and annotate items for the formatter to trim to platform limits (e.g., Discord). Ensure zero-copy parsing where feasible and avoid duplicating buffers before formatting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Telemetry and structured logging",
            "description": "Add per-line parse latency, error rates, dropped events, and structured logs for observability.",
            "dependencies": [
              "3.3",
              "3.6"
            ],
            "details": "Instrument parse_line and pipeline with tracing spans. Emit counters for parsed_events, filtered_events, errors_by_kind, and histograms for parse_latency_ms. Include contextual metadata (run_ids, model) in logs. Provide hooks for metrics export.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Comprehensive test suite",
            "description": "Create fixtures and tests for parser resilience, filters, parity_mode, aggregator totals, and snapshot outputs.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4",
              "3.5",
              "3.6",
              "3.7",
              "3.8"
            ],
            "details": "Add unit tests per event type, fuzz tests for unknown fields and extra keys, filter behavior tests (includeTools/patterns/minStdoutLength), parity_mode bypass tests, aggregator accumulation across sequences, and snapshot tests for stats_snapshot. Validate cost precedence and formatting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Documentation and developer guide",
            "description": "Document schema expectations, parsing strategy, filters, pricing overrides, and extension points.",
            "dependencies": [
              "3.1",
              "3.2",
              "3.3",
              "3.4",
              "3.5",
              "3.6",
              "3.7",
              "3.8",
              "3.9"
            ],
            "details": "Write docs covering domain types, tolerant parsing rules, configuration (filters, parity_mode), pricing map override guidance, telemetry usage, and examples of input→output. Include notes on performance and zero-copy considerations.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 4,
        "title": "Discord Embed Builder, Truncation, and Attachments",
        "description": "Convert parsed events and stats into Discord embeds adhering to UX and constraints, with safe truncation and optional file attachments.",
        "details": "Implementation using twilight-model and twilight-util builders:\n- Build per-event embeds based on templates:\n  - ToolUse Bash: title \"⚡ Bash\", fields: Command (```bash), Working Dir, Tool Count, color=0xF39C12.\n  - Write/Edit/Read: title \"📝 Write\" / \"👁️ Read\", fields: File, Summary, Tool Count; include compact diff rendered as ```diff with ± lines; generate attachment for full diff if >60 lines.\n  - AssistantText: title optional, description with text; fields: Tokens In/Out, Session Cost, Model; color=0x3498DB.\n  - ToolResult error: title \"❌ Error\", description with stderr as ```text, Total Errors field; color=0xE74C3C.\n  - Completion: title \"✅ Complete\", fields: Cost, Duration, Tokens total, Tools Used; color=0x27AE60.\n- Truncation:\n  - Enforce: embed description ≤4096 chars, ≤25 fields per embed; message ≤10 embeds; use ellipses … and note \"(truncated)\".\n  - Large outputs (>1200 chars) attached as files. For webhooks, use ExecuteWebhook with attached files.\n- Sanitize code fences: escape backticks in content; ensure language fences are properly closed.\n- Batch builder: coalesce up to BATCH_SIZE embeds into a single message. If overflow, split into multiple messages.\n- Threading (optional): provide API to open a thread for long logs if flagged; MVP disabled.\nPseudo-code:\nfn build_embeds(ev, stats) -> (Vec<Embed>, Vec<Attachment>) { match ev.kind { ... } apply_truncation(); return (embeds, files); }\n",
        "testStrategy": "Unit tests: feed synthetic events and assert embeds conform to size limits (length checks). Verify code block fencing is balanced. Snapshot tests of embeds for deterministic formatting. Tests for diff compaction (hunk extraction). Test attachment generation when content exceeds thresholds.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define limits, types, and builders scaffold",
            "description": "Establish Discord limits, shared types, and builder scaffolding for embeds and attachments.",
            "dependencies": [],
            "details": "Implement constants: EMBED_DESC_MAX=4096, FIELDS_MAX=25, EMBEDS_PER_MESSAGE_MAX=10, LARGE_OUTPUT_ATTACH_THRESHOLD=1200 chars, DIFF_EMBED_MAX_LINES=60, BATCH_SIZE (configurable). Define structs: BuildConfig { thresholds, parity_mode }, BuildResult { embeds: Vec<Embed>, files: Vec<Attachment> }, MessagePayload { embeds, files }. Integrate twilight-model and twilight-util::builder::EmbedBuilder. Provide helper to create colored embeds, add fields, and convert to Vec<Embed>.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Sanitization utilities",
            "description": "Create safe text sanitization and code-fence helpers to prevent malformed blocks and pings.",
            "dependencies": [
              "4.1"
            ],
            "details": "Implement sanitize_text(s: &str) -> String: normalize newlines to \\n, escape backticks inside by breaking ``` into ``\\u200b``, neutralize mentions by inserting zero-width space after '@' for @everyone/@here and user/role mentions (<@...>). Implement code_block(lang: &str, content: &str) -> String ensuring starting ```{lang}\\n and ending ```; if content contains ``` ensure it's escaped/split to keep fences balanced. Provide finalize_fences() to guarantee closing fence even after truncation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Diff rendering and compaction",
            "description": "Render compact diffs with ± lines and prepare full diff for attachment when large.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Implement build_compact_diff(old: &str, new: &str, cfg: &BuildConfig) -> { embed_snippet: String, full_diff: String, total_lines: usize }. Use a simple hunk extraction: compute line-by-line diff, group changes with small context, prefix '+'/'-' for adds/removes, and limit snippet to DIFF_EMBED_MAX_LINES (post-sanitization). Wrap snippet in code_block(\"diff\", ...). If total_lines > DIFF_EMBED_MAX_LINES, mark for attachment generation. Include summary header like \"± lines: +A/−D (~T)\" optionally.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Truncation engine",
            "description": "Apply Discord limits with safe ellipses and markers after sanitization.",
            "dependencies": [
              "4.1",
              "4.2"
            ],
            "details": "Implement apply_truncation_to_desc(desc: &str, max: usize) -> (String, TruncMeta) that truncates at character boundaries, adds \"… (truncated)\", and ensures code fences remain balanced via finalize_fences(). Implement apply_truncation_to_fields(fields: Vec<(name, value, inline)>) limiting to FIELDS_MAX; if dropping fields, append a final Note field indicating truncation. All counts measured after sanitization. Provide helper to compute remaining space and prioritize important fields.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Per-event embed builders",
            "description": "Implement embeds for Bash, Write/Edit/Read, AssistantText, ToolResult error, and Completion.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4"
            ],
            "details": "For ToolUse Bash: title \"⚡ Bash\", color 0xF39C12, fields: Command (code_block(\"bash\", cmd)), Working Dir, Tool Count. For Write/Edit/Read: title \"📝 Write\" or \"👁️ Read\", fields: File, Summary, Tool Count; include compact diff snippet from diff renderer and note when full diff is attached. For AssistantText: optional title, description is sanitized text, fields: Tokens In/Out, Session Cost, Model; color 0x3498DB. For ToolResult error: title \"❌ Error\", description code_block(\"text\", stderr tail if necessary), field Total Errors; color 0xE74C3C. For Completion: title \"✅ Complete\", fields: Cost, Duration, Tokens total, Tools Used; color 0x27AE60. After building, run truncation engine on description and fields.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Attachment generation and integration",
            "description": "Generate file attachments for large outputs/diffs and wire into result.",
            "dependencies": [
              "4.1",
              "4.3",
              "4.4",
              "4.5"
            ],
            "details": "Implement generate_attachments(ev) producing Vec<Attachment> when content exceeds thresholds: full diffs when total_lines > DIFF_EMBED_MAX_LINES, large text outputs > LARGE_OUTPUT_ATTACH_THRESHOLD (assistant text, stderr/stdout). Use names like diff_full_{file}.diff, assistant_text.txt, stderr.txt with content-type text/plain; sanitize filenames. Ensure embeds reference attachments via notes like \"Full diff attached\". Return BuildResult { embeds, files } shaped for webhook execution (Task 5).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Batching and message splitting",
            "description": "Coalesce embeds into messages up to limits and split overflow deterministically.",
            "dependencies": [
              "4.1",
              "4.4",
              "4.5",
              "4.6"
            ],
            "details": "Implement batch_messages(items: Vec<BuildResult>, cfg) -> Vec<MessagePayload> that concatenates embeds in order until reaching EMBEDS_PER_MESSAGE_MAX, then starts a new payload. Coalesce attachments with the message that references them. Preserve input order (by event ts/index). Ensure each MessagePayload has allowed_mentions set to none (for webhook sender) and adheres to Discord max embeds/message.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Config toggles and thresholds",
            "description": "Expose configuration for thresholds and parity_mode to influence truncation/attachments.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4",
              "4.5",
              "4.6",
              "4.7"
            ],
            "details": "Define BuildConfig with fields: parity_mode (bool), batch_size, large_output_attach_threshold, diff_embed_max_lines, embeds_per_message_max, embed_desc_max, fields_max. When parity_mode=true, prefer minimal truncation: increase attach usage earlier, avoid modifying content formatting beyond sanitization, and reduce ellipsis usage. Thread config through all builders and engines.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Tests and documentation",
            "description": "Add unit/snapshot tests and author style guide with examples and known limits.",
            "dependencies": [
              "4.1",
              "4.2",
              "4.3",
              "4.4",
              "4.5",
              "4.6",
              "4.7",
              "4.8"
            ],
            "details": "Tests: length checks for descriptions/fields, fence balance after sanitization and truncation, diff compaction with hunk extraction and line caps, attachment generation boundaries at 60 lines and 1200 chars, batching/splitting at 10 embeds. Snapshot embeds for deterministic formatting. Documentation: per-event template guide, truncation rules, sanitization rules, attachment naming/MIME, batching behavior, config reference, and acceptance criteria.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 5,
        "title": "Webhook Sender with Batching, Backoff, and Rate-Limit Handling",
        "description": "Implement Discord webhook posting layer using twilight-http, supporting up to 10 embeds per request, handling 429s with backoff and retries.",
        "details": "Implementation:\n- Parse webhook URL into (id, token): https://discord.com/api/webhooks/{id}/{token}. Store securely in memory.\n- Use twilight_http::Client (no bot token needed for webhook execution). ExecuteWebhook(id, token) with embeds, allowed_mentions none, and files.\n- Batch queue: bounded channel receives PostItem { embeds, files }. Coalesce until size limit or timeout (e.g., 50ms) then send.\n- Handle rate limiting: twilight-http handles bucketed limits. Also implement jittered exponential backoff for network errors and global 429s.\n- Retry policy: max 5 attempts, exponential backoff starting at 250ms with jitter, log and drop on exhaustion.\n- Idempotency: not required; but avoid duplicate on retry by not reusing attachments across attempts unless buffered.\n- Respect Discord limits: ≤10 embeds per message; if more pending, split into multiple requests.\n- Telemetry: success/failure counters, last 429 info, queue depth, send latency.\nPseudo-code:\nloop { batch = collect_items(); let resp = client.execute_webhook(id, token).embeds(batch.embeds).attachments(files).await; match resp { Ok(_) => ok, Err(HttpError::Ratelimited(r)) => sleep(r.retry_after), Err(e) => backoff_retry; } }\n",
        "testStrategy": "Integration test with a mocked Discord API using wiremock-rs to simulate 200, 429 (with retry_after), and transient 5xx. Assert batching respects limits and retries obey backoff. Unit test webhook URL parser with varied URL formats.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Webhook URL Parser and Validator",
            "description": "Extract and validate (id, token) from Discord webhook URLs; support common URL variants and add unit tests.",
            "dependencies": [],
            "details": "Implement parse_webhook_url(&str) -> Result<WebhookAuth>. Accept https://discord.com/api/webhooks/{id}/{token}, https://discordapp.com, https://ptb.discord.com, https://canary.discord.com, with or without trailing slashes and optional query params. Validate id is a u64-like snowflake and token is non-empty. Return a struct storing id and token securely in memory (no logging; redact token with last 4 chars when needed). Provide comprehensive unit tests for valid/invalid cases.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configuration and Tunables",
            "description": "Expose batch, retry, and timeout settings via env/clap with sensible defaults and validation.",
            "dependencies": [],
            "details": "Define config: batch_window_ms (default 50), queue_bound (e.g., 1024), max_embeds_per_message (10), max_attempts (5), initial_backoff_ms (250), max_backoff_ms (8000), jitter (true), http_timeout_ms (10000), file_in_memory_limit_mb (8), proxy (optional). Load from env and CLI (clap), validate ranges, and make available to components via Arc<Config>.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Twilight HTTP Client Initialization",
            "description": "Initialize twilight_http::Client for webhook execution with timeouts and optional proxy.",
            "dependencies": [
              "5.2"
            ],
            "details": "Build a shared Arc<Client> with configured connector/timeout and optional proxy from Config. No bot token required. Set a descriptive user-agent. Ensure the client honors bucketed rate limits built into twilight-http. Provide a small wrapper with execute_webhook(id, token) helper to encapsulate common options.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Bounded Queue and Batcher",
            "description": "Implement a bounded mpsc queue and coalescing batcher that flushes by size or timeout.",
            "dependencies": [
              "5.2"
            ],
            "details": "Define PostItem { embeds: Vec<Embed>, files: Vec<FileAttachment> }. Create a bounded tokio::mpsc channel (capacity from Config). Batcher task collects items until: adding next would exceed 10 embeds, attachments limit, or bytes budget; or batch_window_ms elapses. Use select! with timeout to flush. Ensure graceful shutdown and backpressure handling when queue is full.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Request Construction and Dispatch",
            "description": "Build webhook requests from batches respecting Discord limits and dispatch via twilight.",
            "dependencies": [
              "5.1",
              "5.3",
              "5.4"
            ],
            "details": "Split batches into one or more messages so each request has ≤10 embeds. Construct ExecuteWebhook with allowed_mentions set to none. Attach files from batch, mapping to twilight attachment types. Measure send latency per request. Provide a send_batch(batch, auth) -> Result that returns granular outcomes for retry logic, including which segments were sent.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Rate-Limit and Retry/Backoff Handling",
            "description": "Implement jittered exponential backoff for network/5xx and respect 429 retry-after, with max attempts.",
            "dependencies": [
              "5.2",
              "5.5"
            ],
            "details": "Wrap dispatch with retry loop: Ok => success; HttpError::Ratelimited(r) => sleep(r.retry_after) and retry; network/timeout/5xx => exponential backoff starting at initial_backoff_ms with full jitter, capped at max_backoff_ms, up to max_attempts. Honor twilight buckets implicitly; handle global 429s (r.global) by pausing all sends. On exhaustion, log and drop. Idempotency not required; avoid duplicate attachments by coordinating with attachment buffering.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Attachment Streaming and Retry Safety",
            "description": "Ensure file attachments are safe to retry without duplication or stream reuse issues.",
            "dependencies": [
              "5.4",
              "5.5"
            ],
            "details": "Represent attachments as retryable sources: in-memory Bytes for small files (<= file_in_memory_limit_mb), or reopenable file handles with pre-read buffers. For retries, clone Bytes or re-seek/reopen files. If an attachment cannot be safely retried (too large/unseekable), mark the send as single-attempt or buffer explicitly. Verify that no consumed streams are reused across attempts.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Telemetry: Metrics, Logging, and Tracing",
            "description": "Emit counters, gauges, histograms, and structured logs for observability, redacting secrets.",
            "dependencies": [
              "5.4",
              "5.5",
              "5.6"
            ],
            "details": "Metrics: counters (messages_sent, messages_failed, batches_flushed), gauge (queue_depth), histograms (batch_embeds_count, send_latency_ms, attempts_per_send). Track last_429_at and last_retry_after_ms. Add tracing spans for batch lifecycle and per-request attempts; redact webhook token. Expose optional Prometheus metrics if available; otherwise rely on tracing.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Integration Tests with Wiremock and Operational Docs",
            "description": "Add wiremock-based tests for 200/429/5xx, batching, and backoff; write tuning and operations docs.",
            "dependencies": [
              "5.1",
              "5.2",
              "5.3",
              "5.4",
              "5.5",
              "5.6",
              "5.7",
              "5.8"
            ],
            "details": "Wiremock: mock /api/webhooks/{id}/{token} for 200, 429 with Retry-After, and transient 5xx. Assert batching flush by size (≤10 embeds) and time (≈50ms), retry/backoff timing with tokio::time::pause, and global 429 waits. Unit-test URL parser edge cases. Document configuration, tuning guidance (batch window, queue size, retries), interpreting telemetry, and common failure scenarios (invalid token, deleted channel, persistent 429s).",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 6,
        "title": "Sidecar Watcher Main Pipeline and Configuration",
        "description": "Wire tailer → parser → formatter → webhook sender into a resilient async pipeline with health checks, metrics, and configuration.",
        "details": "Implementation:\n- CLI/ENV with clap: DISCORD_WEBHOOK_URL (required), TRANSCRIPT_PATH or WORKSPACE_PATH+SESSION, POLL_INTERVAL_MS (default 100), BATCH_SIZE (default 10), PARITY_MODE (bool), FILTERS (JSON or env-specific), LOG_LEVEL.\n- Build pipeline with bounded channels to avoid memory bloat.\n- Independent restart safety: if any stage fails, attempt to restart stage without terminating process; ensure no agent impact.\n- Health endpoints: optional local TCP probe or UNIX socket that reports OK; or write a heartbeat file for K8s liveness/readiness probes.\n- Metrics: expose Prometheus text on a local HTTP port (optional) or log-based counters if port not allowed.\n- Graceful shutdown: SIGTERM handling to flush pending messages and send final summary if available.\nPseudo-code:\nspawn tailer -> tx_lines;\nspawn parser+aggregator(rx_lines) -> tx_events;\nspawn formatter(rx_events) -> tx_posts;\nspawn sender(rx_posts);\nawait shutdown signal;\n",
        "testStrategy": "End-to-end local test: write a temp transcript file with a sequence of events; run watcher with a mock webhook endpoint; assert end-to-end messages posted and final summary present. Verify average latency ≤100ms across multiple events (using timestamps in events). Stress test with 10k lines to check memory stays <64Mi (measure RSS).",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "CLI and ENV configuration with clap",
            "description": "Define all flags, env vars, defaults, help text, and mutual exclusions for the watcher.",
            "dependencies": [],
            "details": "Add flags and env mapping: DISCORD_WEBHOOK_URL (required), TRANSCRIPT_PATH or WORKSPACE_PATH plus SESSION (mutually exclusive group), POLL_INTERVAL_MS (default 100), BATCH_SIZE (default 10), PARITY_MODE (bool), FILTERS (inline JSON or file path), LOG_LEVEL (default info). Add optional ops flags: METRICS_PORT (optional; if absent use log), HEALTH_TARGET (tcp://host:port, unix:///path, or file:///path), --health (one-shot probe mode), SHUTDOWN_GRACE_MS (default 5000). Provide clear help text and examples; redact secrets in clap debug; validate basic types.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configuration loading, precedence, and validation",
            "description": "Load config from CLI and env, enforce precedence, validate values, and compute derived settings.",
            "dependencies": [
              "6.1"
            ],
            "details": "Precedence: CLI > ENV > defaults. Validate DISCORD_WEBHOOK_URL scheme; ensure exactly one of TRANSCRIPT_PATH or WORKSPACE_PATH+SESSION; resolve default transcript path ~/.claude/projects/<encoded-workspace>/<session>.jsonl when needed; expand ~ and environment variables. Parse FILTERS JSON into structured filters; validate BATCH_SIZE >= 1 and POLL_INTERVAL_MS in [10, 1000]. Parse HEALTH_TARGET into enum {Tcp(addr), Unix(path), File(path)}; choose metrics mode based on METRICS_PORT. Compute derived capacities for bounded channels using BATCH_SIZE (e.g., lines=BATCH_SIZE*5, events=BATCH_SIZE*3, posts=BATCH_SIZE*2) with minimum 1 and caps to prevent memory bloat. Derive timeouts for sender and shutdown using SHUTDOWN_GRACE_MS.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Channel topology and backpressure plan",
            "description": "Design and implement bounded channels between stages with capacity tuning and backpressure behavior.",
            "dependencies": [
              "6.2"
            ],
            "details": "Instantiate tokio mpsc channels: tailer→parser (lines), parser+aggregator→formatter (events), formatter→sender (posts) with capacities from config. Document send semantics to await on full channels to apply backpressure; no message drops. Add periodic queue depth sampling for metrics. Estimate memory footprint per queue element; adjust defaults to keep under constraints. Define batching boundaries influenced by BATCH_SIZE for parser/aggregator.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Stage supervision and restart-on-failure",
            "description": "Introduce per-stage supervisors that monitor tasks, classify errors, and restart failed stages without process exit.",
            "dependencies": [
              "6.3"
            ],
            "details": "Wrap each stage (tailer, parser+aggregator, formatter, sender) in a supervisor that owns its JoinHandle and restart policy. Error classification: transient (IO, HTTP 5xx, parse recoverable) vs fatal (schema incompatibility, invalid config). Implement jittered exponential backoff (100ms to 5s) with cap and restart counters; add circuit-breaker logic to avoid hot-loop restarts. Preserve channels across restarts; ensure backpressure holds while a stage is down. Surface restart events to metrics and logs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Health checks: endpoints and heartbeat",
            "description": "Provide liveness/readiness reporting via TCP/UNIX or heartbeat file, plus a one-shot --health probe mode.",
            "dependencies": [
              "6.2"
            ],
            "details": "If HEALTH_TARGET is tcp:// or unix://, serve a minimal handler returning OK for /livez and /readyz (HTTP if tcp port is HTTP; otherwise line-based OK). For file://, update a heartbeat file with current timestamp periodically and on successful pipeline activity; readiness indicated by an additional ready marker or timestamp freshness threshold. Readiness should reflect all stages healthy recently; liveness indicates process up. Implement --health to probe HEALTH_TARGET once and exit 0/1 for integration with exec probes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Metrics exposure: Prometheus text or log-based counters",
            "description": "Expose key pipeline metrics via an optional Prometheus endpoint or periodic structured log summaries.",
            "dependencies": [
              "6.2",
              "6.3"
            ],
            "details": "If METRICS_PORT is set, expose /metrics Prometheus text with counters and gauges: lines_read_total, events_parsed_total, posts_formatted_total, webhook_posts_sent_total, errors_total{stage,kind}, restarts_total{stage}, queue_depth_{lines,events,posts}, tailer_lag_bytes, tailer_lag_ms, end_to_end_latency_ms histogram. If not set, emit periodic structured log snapshots of the same metrics. Provide lightweight metrics registry and helpers for per-stage instrumentation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Graceful shutdown and drain",
            "description": "Handle SIGTERM/SIGINT to stop intake, drain queues, and send a final summary within a grace window.",
            "dependencies": [
              "6.3",
              "6.4"
            ],
            "details": "Install signal listeners; on shutdown start, stop tailer intake, allow parser/formatter/sender to drain; enforce SHUTDOWN_GRACE_MS with a timeout. If configured and available, format and send a final summary message before exit. Ensure idempotent shutdown and proper closing of health and metrics endpoints. Log durations and any dropped items; update metrics for shutdown outcomes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Pipeline integration and wiring with existing modules",
            "description": "Wire tailer, parser+aggregator, formatter, and sender modules behind supervised tasks and bounded channels.",
            "dependencies": [
              "6.2",
              "6.3",
              "6.4"
            ],
            "details": "Integrate Task 2 tailer to emit lines to tx_lines; integrate parser+aggregator to produce events applying FILTERS; connect formatter to build payloads; connect sender to post to DISCORD_WEBHOOK_URL with retries and rate-limit handling. Add feature gating for PARITY_MODE to alter formatting and event selection. Ensure modules expose cancellation and error surfaces compatible with supervision. Close channels on terminal errors only during shutdown.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Structured logging and tracing",
            "description": "Emit structured JSON logs with spans per stage, log-level control, and sensitive data redaction.",
            "dependencies": [
              "6.1",
              "6.2"
            ],
            "details": "Use tracing with JSON formatter; configure LOG_LEVEL. Establish spans for tailer, parser, formatter, sender, and supervisor. Include fields: run_id or short_id, pipeline_id, stage, restart_count, queue_depths, and latency samples. Redact tokens in DISCORD_WEBHOOK_URL. Map errors to codes and include context for troubleshooting. Optionally enable sampling for high-volume logs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "End-to-end test harness and resilience verification",
            "description": "Build tests that validate E2E functionality, latency, memory bounds, supervision restarts, health, and metrics.",
            "dependencies": [
              "6.4",
              "6.5",
              "6.6",
              "6.7",
              "6.8",
              "6.9"
            ],
            "details": "Create a mock transcript file and a mock webhook server; run the watcher against it. Assert that events propagate end-to-end and a final summary is posted. Measure average end-to-end latency across multiple events and ensure it is ≤100ms on CI-grade hardware by injecting timestamps. Stress with 10k lines to confirm no memory bloat and bounded queue sizes. Inject failures by killing a stage; assert supervisor restarts it and pipeline recovers without process exit. Verify health endpoints report OK and metrics counters/gauges are exposed as expected.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Operations documentation and runbook",
            "description": "Produce user-facing docs for local and Kubernetes sidecar usage, configuration reference, and troubleshooting.",
            "dependencies": [
              "6.1",
              "6.2",
              "6.3",
              "6.4",
              "6.5",
              "6.6",
              "6.7",
              "6.8",
              "6.9",
              "6.10"
            ],
            "details": "Write a runbook covering installation, CLI examples, env variables, defaults, and health probe usage (--health, HEALTH_TARGET). Document metrics endpoint and example Prometheus scrapings; provide log-based metrics fallback. Include K8s sidecar examples aligning with Task 9: liveness/readiness probes (HTTP/TCP/exec), heartbeat file option, and env configuration. Add tuning guidance for BATCH_SIZE and channel capacities, and troubleshooting steps for common failures and restarts.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 7,
        "title": "Discord Bot Service: Channel Lifecycle and Webhook Provisioning",
        "description": "Build a RESTful bot service (Twilight REST-first) that creates per-run channels in the 'Agent Runs' category, creates a channel-scoped webhook, and returns {channelId, webhookUrl}.",
        "details": "Implementation:\n- Bot service runs with a bot token (env: DISCORD_BOT_TOKEN) in a central deployment (never in pods running agents).\n- Provide POST /runs API: { taskId, attempt, shortId, guildId, retentionHours }.\n  - Find or create category 'Agent Runs' in the guild.\n  - Create text channel name: run-{taskId}-{attempt}-{shortId} under category.\n  - Create webhook in that channel with name 'Agent Run {shortId}'.\n  - Return { channelId, webhookUrl }.\n- Provide POST /runs/{channelId}/finalize: archive or delete channel after retention window; delete webhook.\n- Post initial header embed via bot using twilight-http CreateMessage with run metadata, or leave to watcher once webhook is used.\n- Use twilight_http::Client for all operations. Respect REST rate limits.\n- Security: AuthN for our API (e.g., internal token), allow only trusted callers (CI or operator tool). Validate channel naming to prevent collisions.\nPseudo-code:\nPOST /runs -> create_category_if_needed(); create_channel(); create_webhook(); respond JSON.\n",
        "testStrategy": "Integration tests using a Discord test guild (with caution) or mocked HTTP. Unit tests for name formatting and category resolution. Verify that webhook is scoped to channel. Test finalize endpoint deleting or archiving channel and revoking webhook. Negative tests: invalid guild/category name, permission errors.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Service Skeleton and Configuration",
            "description": "Set up the REST service with Axum/Warp, configuration, structured logging, and error handling.",
            "dependencies": [],
            "details": "Initialize a Rust binary crate with axum (or warp) HTTP server. Load config from env: DISCORD_BOT_TOKEN, API_AUTH_TOKEN (or mTLS), PORT, INITIAL_HEADER_ENABLED, DEFAULT_RETENTION_HOURS. Implement healthz/ready endpoints, graceful shutdown, tracing-based structured logging, and a unified error type with mappable HTTP status codes. Define base routes for POST /runs and POST /runs/{channelId}/finalize (handlers stubbed).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "API Authentication and Request Logging Middleware",
            "description": "Add internal authentication and safe request logging without leaking secrets.",
            "dependencies": [
              "7.1"
            ],
            "details": "Implement middleware for Authorization: Bearer <API_AUTH_TOKEN> (or pluggable mTLS). Enforce auth on all mutating endpoints. Add request/response logging with redaction of tokens and webhook URLs. Include simple per-IP or token rate limiting to protect the service. Return 401/403 as appropriate, and include correlation IDs in logs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Discord REST Client Initialization and Permission Validation",
            "description": "Create twilight_http::Client with bot token and validate required permissions.",
            "dependencies": [
              "7.1"
            ],
            "details": "Instantiate twilight_http::Client using DISCORD_BOT_TOKEN and custom User-Agent. On startup, fetch current user (/users/@me) to verify token. Provide utility functions for channel, category, webhook operations. Optionally probe configured guild(s) to ensure Manage Channels and Manage Webhooks permissions or surface clear errors at runtime. Ensure client respects Twilight's rate-limit handling.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "In-Memory Store and Caching Layer",
            "description": "Design and implement caches for category IDs and run metadata with safe secret handling.",
            "dependencies": [
              "7.1"
            ],
            "details": "Introduce a concurrency-safe store (DashMap/RwLock) to cache: guildId -> categoryId for 'Agent Runs'; channelId -> run metadata (taskId, attempt, shortId, guildId, retention deadline); channelId -> webhook id/token (token handled carefully, never logged). Add TTL/refresh policy for categories. On restart, tolerate empty caches and re-resolve from Discord as needed.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Category Manager: Find-or-Create 'Agent Runs'",
            "description": "Implement find-or-create logic for the 'Agent Runs' category per guild with caching and perms handling.",
            "dependencies": [
              "7.3",
              "7.4"
            ],
            "details": "Implement get_or_create_category(guildId, name='Agent Runs'): list guild channels, find matching category; if absent, create category. Cache the categoryId in the store. Handle permission errors with clear diagnostics. Ensure idempotency under concurrency (double-check after create). Optionally set permission overwrites if needed.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "POST /runs: Request Schema and Validation",
            "description": "Define payload schema and validate inputs, including safe channel name generation.",
            "dependencies": [
              "7.1",
              "7.2"
            ],
            "details": "Define DTO: { taskId: String, attempt: u32, shortId: String, guildId: Snowflake, retentionHours: Option<u32> }. Validate length/charset; shortId constrained to safe characters. Generate channel name run-{taskId}-{attempt}-{shortId} with normalization (lowercase, hyphenated, max length). Compute retention_deadline = now + retentionHours or default. Reject malformed inputs early with 400.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Channel Creation Under Category with Collision Handling",
            "description": "Create the text channel under the category and handle name collisions and metadata.",
            "dependencies": [
              "7.3",
              "7.5",
              "7.6"
            ],
            "details": "Using the validated name and categoryId, create a text channel in the guild. If a channel with the same name exists, detect and avoid collisions (e.g., append a short suffix or fail with 409). Set channel topic with metadata (taskId/attempt/shortId). Store run metadata in the cache. Ensure operations respect rate limits and surface retriable vs fatal errors.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Webhook Provisioning and Secure Response",
            "description": "Create a channel-scoped webhook and return {channelId, webhookUrl} securely.",
            "dependencies": [
              "7.7",
              "7.3"
            ],
            "details": "Create a webhook named 'Agent Run {shortId}' in the channel. Construct https://discord.com/api/webhooks/{id}/{token}. Store id/token in cache for finalize, but redact in logs. Return JSON response { channelId, webhookUrl } to the caller. Do not log the URL or token; ensure headers and traces are scrubbed. Handle conflicts (existing webhook) idempotently.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Optional Initial Header Embed Posting",
            "description": "Post an initial header embed with run metadata via bot client when enabled.",
            "dependencies": [
              "7.8",
              "7.3"
            ],
            "details": "If INITIAL_HEADER_ENABLED=true, post a CreateMessage to the channel with an embed summarizing taskId, attempt, shortId, retention window, and timestamps. Enforce embed size limits and handle errors without failing the main /runs flow. If disabled, rely on the watcher to post via webhook later.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "POST /runs/{channelId}/finalize: Retention and Idempotent Cleanup",
            "description": "Implement finalize to archive/delete the channel and delete the webhook respecting retention.",
            "dependencies": [
              "7.3",
              "7.4",
              "7.5",
              "7.7",
              "7.8"
            ],
            "details": "Authorize request, look up run by channelId. If retention has not elapsed, archive (lock/rename/move) or no-op with 202. If elapsed, delete webhook (using cached id/token or by listing channel webhooks) and delete or archive the channel. Make idempotent: missing resources should return 200 with status noted. Record cleanup state in cache. Emit audit logs without secrets.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Tests, Mocks, Rate-Limit Scenarios, and Operational Docs",
            "description": "Build test suites and write deployment/operations documentation.",
            "dependencies": [
              "7.1",
              "7.2",
              "7.3",
              "7.4",
              "7.5",
              "7.6",
              "7.7",
              "7.8",
              "7.9",
              "7.10"
            ],
            "details": "Unit tests for name formatting, request validation, auth middleware, category resolution. Mocked Discord HTTP (e.g., wiremock) for create/delete channel/webhook paths, 429 with retry-after, and transient 5xx. Optional integration tests in a test guild with caution. Document required Discord permissions, environment variables, rate-limit behavior, error triage, and deployment steps.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 8,
        "title": "Optional Input Path: Slash Command Handling and Pod-Local HTTP Bridge",
        "description": "Enable operators to send input to the running agent via Discord slash commands in the run channel, forwarding JSON to a pod-local HTTP bridge (POST /input).",
        "details": "Approach without gateway (REST-first):\n- Register slash command /input with an optional string parameter text and optional json attachment. Use application command interactions with public key verification.\n- Bot service exposes POST /interactions endpoint; verify Discord signatures (X-Signature-Ed25519, X-Signature-Timestamp) using ed25519-dalek.\n- Maintain a channel→bridge mapping store when /runs is created: {channel_id, bridge_url} stored securely. Only forward inputs for mapped channels.\n- On /input usage: validate the channel is a run channel and user permissions if needed; construct payload { user, channelId, runId, text, timestamp } and POST to http://127.0.0.1:PORT/input bridged via a per-pod Service or via injected env variable BRIDGE_URL when creating the run (out-of-band). For cluster routing, use a short-lived secret mapping or pre-provisioned per-run endpoint.\n- Pod-local bridge (small HTTP server in watcher or a separate sidecar binary):\n  - Expose POST /input on 127.0.0.1 only; write streaming JSON to an agent FIFO/stdin path (env: AGENT_STDIN or FIFO_PATH). Use Tokio to write and flush.\n  - Return 200 fast; do not block on agent.\n- Security: validate channel/run mapping in bot; bridge only binds to localhost; optional HMAC between bot and bridge.\nInteraction response:\n- Acknowledge with a short ephemeral message confirming delivery or error.\nPseudo-code (bot):\nfn handle_interaction(req) { verify_signature(); match command.name { \"input\" => { if !is_run_channel(channel_id) { respond_ephemeral(\"Not allowed here\"); } else { post_to_bridge(bridge_url, payload).await; respond_ephemeral(\"Sent\"); } } }\nPseudo-code (bridge):\nPOST /input -> read JSON -> write to stdin FIFO -> 200.\n",
        "testStrategy": "Unit tests for signature verification using known public/private key pairs. Integration test with a local bridge server that captures payloads; simulate interaction JSON and assert forwarding only for authorized channel IDs. Test FIFO writing with a temp named pipe. Security tests: attempt requests from wrong channel → rejected.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Define and register /input slash command",
            "description": "Create the Discord application command with optional text string and optional JSON attachment parameters.",
            "dependencies": [],
            "details": "Specify command name 'input' with: param 'text' (STRING, optional) and param 'json' (ATTACHMENT, optional). Disable in DMs. Choose registration strategy (startup sync or out-of-band script). Make registration idempotent and upsert by command name. Document required OAuth2 scopes and permissions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Discord signature verification",
            "description": "Add ed25519 verification for POST /interactions requests with replay protection.",
            "dependencies": [],
            "details": "Verify X-Signature-Ed25519 and X-Signature-Timestamp using ed25519-dalek against the app public key. Enforce a 5-minute replay window and reject non-POST or missing headers with 401. Use constant-time comparisons and return PLAIN 401 without body details on failures. Log attempt metadata without sensitive contents.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build /interactions handler and command routing",
            "description": "Parse interaction payloads, handle PING, and route to the /input command handler.",
            "dependencies": [
              "8.2"
            ],
            "details": "Implement axum/warp handler for POST /interactions. Respond to PING with PONG. For application_command, extract command name and options. Provide a hook to defer responses when forwarding may exceed 3 seconds. Include versioning of payload schema in context for downstream steps.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Channel-to-bridge mapping store",
            "description": "Create secure mapping storage for {channel_id, run_id, bridge_url} with TTL, set at /runs creation time.",
            "dependencies": [],
            "details": "Implement get/put/delete APIs with TTL expiration. Store fields: channel_id, run_id, bridge_url, created_at, expires_at, optional hmac_secret. Prefer in-memory cache with optional persistent backing. Validate inputs and ensure only known channels are mapped. Expose metrics for hits/misses/expired.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Authorization and channel validation",
            "description": "Enforce that /input is used in a mapped run channel and optionally check user roles/allowlist.",
            "dependencies": [
              "8.3",
              "8.4"
            ],
            "details": "Lookup channel_id in mapping; reject if absent or expired. Optionally enforce role IDs or user allowlist from config. Log denied attempts with minimal fields. Provide clear ephemeral error messages and do not leak internal details.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Payload construction and attachment handling",
            "description": "Build sanitized payload {user, channelId, runId, text, timestamp, json} and enforce size/type limits.",
            "dependencies": [
              "8.3",
              "8.5"
            ],
            "details": "Assemble payload with schema_version and correlation_id. Sanitize text (limit length, strip control chars). If 'json' attachment present, ensure content type is application/json and size within limit (e.g., 1 MB); fetch from CDN with bot token and parse or pass-through as string. Add user metadata (id, username, roles).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "HTTP client to pod-local bridge with retries and HMAC",
            "description": "POST payload to bridge_url/input with robust error handling and optional HMAC signing.",
            "dependencies": [
              "8.6",
              "8.4"
            ],
            "details": "Use reqwest with connect/read timeouts (e.g., 200ms/1s). Retry transient failures (3 attempts, jittered backoff). Include HMAC header (e.g., X-Input-Signature: hex(hmac_sha256(secret, body))) when secret available. Treat non-2xx as errors. Never block interaction thread; run async task.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Ephemeral interaction responses and deferrals",
            "description": "Send ephemeral ACKs or errors per Discord interaction lifecycle.",
            "dependencies": [
              "8.3",
              "8.7"
            ],
            "details": "If forwarding may take >3s, send deferred ephemeral response, then follow up with success/failure. On quick success, respond immediately with ephemeral 'Sent'. On errors, show concise message and correlation_id. Ensure proper interaction token usage and followup message deletion policies.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Pod-local bridge HTTP server",
            "description": "Implement 127.0.0.1-bound server with POST /input writing to FIFO/STDIN asynchronously.",
            "dependencies": [
              "8.6"
            ],
            "details": "Use axum/warp bound to 127.0.0.1:PORT (env BRIDGE_PORT). Accept JSON payload and stream a single line into FIFO_PATH or AGENT_STDIN via Tokio, flush promptly, and return 200 without waiting for agent processing. Handle FIFO open semantics and backpressure gracefully. Add basic health endpoint if needed.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Bridge security hardening",
            "description": "Enforce localhost binding, validate optional HMAC, and apply body limits and validation.",
            "dependencies": [
              "8.9",
              "8.7"
            ],
            "details": "Ensure server only binds to 127.0.0.1. Enforce max body size (e.g., 1 MB). Verify X-Input-Signature HMAC when configured; reject mismatches with 401. Validate required fields and schema_version. Add minimal structured error responses and audit logs for rejected requests.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 11,
            "title": "Unit and integration tests",
            "description": "Add tests for signature verification, authorization, payload building, forwarding, and FIFO writes.",
            "dependencies": [
              "8.2",
              "8.5",
              "8.7",
              "8.9",
              "8.10"
            ],
            "details": "Use known ed25519 keypair for signature tests. Simulate interaction JSON and assert only mapped channels are accepted. Stand up a test bridge to capture payloads. Create a temp named pipe for FIFO tests. Include negative tests (bad signature, wrong channel, oversized attachment, HMAC mismatch).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 12,
            "title": "Observability and documentation",
            "description": "Add tracing/metrics and write setup/security docs with acceptance criteria mapping.",
            "dependencies": [
              "8.8",
              "8.10",
              "8.11"
            ],
            "details": "Emit tracing spans with run/channel IDs and outcome tags. Export counters for forwarded inputs, failures, and histograms for end-to-end latency. Document setup: command registration, environment variables (BRIDGE_URL, FIFO_PATH, HMAC secret), security model, and failure modes. Provide acceptance test checklist.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 9,
        "title": "Kubernetes Sidecar Template and Helm Packaging",
        "description": "Provide K8s manifests and Helm charts for deploying the watcher as a sidecar and the bot as a central service, including configurable values from the PRD.",
        "details": "Implementation:\n- Helm chart values per PRD sketch:\ndiscord.monitoring.enabled, image, polling.interval (ms), polling.batchSize, filters.includeTools/includePatterns/minStdoutLength, stats.trackCost/tokens/errors, resources (requests/limits).\n- Sidecar container:\n  - Env: DISCORD_WEBHOOK_URL (from bot), WORKSPACE_PATH or TRANSCRIPT_PATH, POLL_INTERVAL_MS, BATCH_SIZE, PARITY_MODE, FILTERS JSON, LOG_LEVEL.\n  - Liveness/readiness probes (HTTP or exec to watcher --health).\n  - SecurityContext: runAsNonRoot, readOnlyRootFilesystem where possible.\n- Bot deployment:\n  - Secret for DISCORD_BOT_TOKEN and PUBLIC_KEY.\n  - Service and Ingress for /runs and /interactions endpoints (mTLS or IP allow-list recommended).\n- Optional bridge:\n  - Expose localhost-only port inside the pod; no Service.\n- RBAC not required beyond defaults; no tokens mounted into agent pods.\n- Templates for channel retention window config.\n- Provide example values.yaml matching PRD defaults.\n",
        "testStrategy": "helm template and kubeval to validate manifests. Kind-based e2e: deploy bot, then a sample job with watcher sidecar; run a mocked transcript to verify Discord posts. Verify resource usage via metrics-server stays within limits. Confirm environment variables propagate.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Helm chart scaffolding and layout",
            "description": "Create umbrella and subchart structure for watcher sidecar and bot service with shared helpers.",
            "dependencies": [],
            "details": "• Create charts/discord-monitoring (umbrella), charts/watcher-sidecar, charts/bot-service\n• Add Chart.yaml, values.yaml, .helmignore for each; set versions/appVersion\n• Add common templates/_helpers.tpl (name, labels, image, resources, feature flags)\n• Gate renders with .Values.discord.monitoring.enabled where appropriate\n• Provide NOTES.txt stubs and standard labels/annotations",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Values schema and PRD defaults",
            "description": "Define values.schema.json and defaults aligning with PRD; provide example values.yaml.",
            "dependencies": [
              "9.1"
            ],
            "details": "• Add values.schema.json enforcing keys: discord.monitoring.enabled, image.{repository,tag,pullPolicy}, polling.intervalMs, polling.batchSize, filters.{includeTools,includePatterns,minStdoutLength}, stats.{trackCost,tokens,errors}, resources.{requests,limits}, parityMode, logging.level, retention.channelHours\n• Bot settings: service.ports, ingress, networkPolicy, secrets (existingSecret or inline), probes\n• Sidecar settings: transcript volume, probes, securityContext, bridge options\n• Provide example-values.yaml matching PRD defaults",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Watcher sidecar container template and injection helpers",
            "description": "Implement sidecar container spec with env wiring and volume mounts, exposed as a reusable Helm include.",
            "dependencies": [
              "9.2"
            ],
            "details": "• Add templates/_sidecar.tpl rendering a Container with image, args, env, resources\n• Env: DISCORD_WEBHOOK_URL (from existingSecret or value), WORKSPACE_PATH or TRANSCRIPT_PATH, POLL_INTERVAL_MS, BATCH_SIZE, PARITY_MODE, FILTERS (JSON), LOG_LEVEL\n• Support transcript volume and mount via values (existingClaim, hostPath, emptyDir)\n• Provide helper to inject sidecar into user workloads (library chart pattern) with examples",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Watcher liveness/readiness probes",
            "description": "Add configurable HTTP or exec probes for the watcher sidecar.",
            "dependencies": [
              "9.3"
            ],
            "details": "• values: watcher.probes.type (http|exec), http.port, http.path, exec.command, initialDelaySeconds, periodSeconds, timeoutSeconds, failureThreshold\n• If http: expose containerPort and configure probes; if exec: run watcher --health (or configured command)\n• Template into sidecar container via _sidecar.tpl",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Security context and pod hardening for sidecar",
            "description": "Apply secure-by-default settings for sidecar and pods.",
            "dependencies": [
              "9.3"
            ],
            "details": "• Container securityContext: runAsNonRoot=true, runAsUser=65532 (configurable), readOnlyRootFilesystem=true, allowPrivilegeEscalation=false, capabilities drop: [ALL]\n• PodSecurityContext: seccompProfile=RuntimeDefault; optional fsGroup if needed by volume\n• Set automountServiceAccountToken=false for workloads using sidecar (toggle via values)",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Bot Deployment, Service, Ingress, and Secrets",
            "description": "Create bot service chart with secure defaults and configurable networking.",
            "dependencies": [
              "9.2"
            ],
            "details": "• Deployment with env from Secret (DISCORD_BOT_TOKEN, PUBLIC_KEY); support existingSecret and stringData\n• Service exposing /runs and /interactions; ports configurable; health probes\n• Optional Ingress with TLS; support IP allow-list annotations; document mTLS recommendation\n• Optional NetworkPolicy restricting ingress/egress\n• Resources, image, logging, and replica settings via values",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Optional localhost bridge container",
            "description": "Support optional bridge sidecar exposing a localhost-only port with no Service.",
            "dependencies": [
              "9.3"
            ],
            "details": "• Add bridge.enabled, image, port, args in values\n• Render optional bridge container; no Service created; hostNetwork=false; containerPort only\n• Wire watcher BRIDGE_URL env to http://127.0.0.1:<port> when enabled",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Retention window configuration templates",
            "description": "Template retention settings for channels and propagate to components.",
            "dependencies": [
              "9.3",
              "9.6"
            ],
            "details": "• Add retention.channelHours to values and schema\n• Provide ConfigMap or direct env for bot and watcher to consume retention settings\n• If retained via FILTERS, include computed duration; else pass dedicated env RETENTION_HOURS",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "CI validation: linting and schema conformance",
            "description": "Automate helm lint/template and kubeconform checks in CI.",
            "dependencies": [
              "9.1",
              "9.2",
              "9.3",
              "9.4",
              "9.5",
              "9.6",
              "9.7",
              "9.8"
            ],
            "details": "• GitHub Actions workflow: setup-helm, helm lint, helm template\n• Run kubeval/kubeconform against multiple Kubernetes versions\n• Optionally use chart-testing (ct) lint steps; produce packaged charts artifacts",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Kind e2e tests and example deployments",
            "description": "Validate charts on Kind with a sample workload and mock endpoints; provide docs.",
            "dependencies": [
              "9.9"
            ],
            "details": "• Spin up Kind with metrics-server; install bot-service and sample job/deployment using watcher sidecar include\n• Deploy mock Discord webhook receiver (e.g., httpbin/wiremock); point DISCORD_WEBHOOK_URL to it\n• Provide mock transcript via ConfigMap/initContainer; verify posts arrive; verify probes succeed; check resources within limits\n• Add README with deployment steps and example-values.yaml; map checks to acceptance criteria",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 10,
        "title": "Docs, Logging/Monitoring, and Acceptance Testing",
        "description": "Write architecture and deployment docs; add structured logging and basic metrics; run acceptance tests against the PRD criteria.",
        "details": "Documentation:\n- architecture.md: data flow (tail → parse → embeds → webhook), bot lifecycle, security model (no bot token in pods), optional input.\n- deploy.md: bot setup with Twilight, registering slash commands, required Discord permissions, Helm deploy steps, environment configuration, troubleshooting (rate limits, missing category), parity_mode usage.\nLogging & Monitoring:\n- Use tracing with JSON logs for ingestion; include run/channel IDs, rate-limit events, backoff retries. Optional Prometheus metrics.\nAcceptance Tests:\n- Criteria: dedicated channel created; watcher streams embeds for tool use, assistant messages, errors, final summary; no measurable slowdown; retention/cleanup works.\n- Load test: simulate high-frequency events to trigger batching and rate-limit code paths.\n- Reliability test: restart watcher; ensure it resumes without impacting agent.\n",
        "testStrategy": "Manual runbook plus automated e2e scripts: \n- Spin up bot and watcher (with mock transcript). \n- Verify Discord channel creation and embeds content. \n- Confirm final summary embed fields (tokens, cost, tool count, error count). \n- Measure latency from file append to webhook POST using timestamps and ensure ≤100ms average (in local network conditions). \n- Verify channel archive/delete after retention. \n- Review logs for errors and ensure no bot token present in pod logs.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Architecture Documentation (architecture.md)",
            "description": "Author architecture.md covering data flow, bot lifecycle, security, and optional input path.",
            "dependencies": [],
            "details": "Deliverables: architecture.md with diagrams and narrative. Include: 1) Data flow: tail → parse → embeds → webhook (sequence and component diagrams), 2) Bot lifecycle: run creation, channel mapping, streaming embeds, final summary, shutdown, 3) Security model: no bot token in pods/sidecars, secret handling, log redaction boundaries, 4) Optional input path: slash command → bot → pod-local HTTP bridge (/input), auth and channel mapping, 5) Failure/retry and rate-limit handling paths, 6) Key configuration knobs (poll interval, batch size, parity_mode, filters). Acceptance: PR reviewed doc with diagrams checked in under docs/architecture.md.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Deployment Documentation (deploy.md)",
            "description": "Write deploy.md covering Discord setup, permissions, commands, Helm install, config, troubleshooting.",
            "dependencies": [],
            "details": "Deliverables: deploy.md including: 1) Discord app setup with Twilight, OAuth2, public key, token storage, required permissions/intents, 2) Slash command registration steps (/runs, /input) and examples, 3) Helm install for bot service and watcher sidecar with sample values.yaml, 4) Environment configuration (DISCORD_WEBHOOK_URL, TRANSCRIPT_PATH or WORKSPACE_PATH+SESSION, POLL_INTERVAL_MS, BATCH_SIZE, PARITY_MODE, FILTERS, LOG_LEVEL), 5) Troubleshooting: rate limits, missing category, permissions errors, parity_mode usage, health probes, 6) Example end-to-end quickstart. Acceptance: validated by a fresh deploy following the doc.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Structured JSON Logging with Tracing",
            "description": "Implement tracing-based JSON logs across bot and watcher with IDs, levels, and redaction.",
            "dependencies": [],
            "details": "Implement: 1) tracing subscriber emitting JSON, 2) Correlation fields on every event: run_id, channel_id, component, event_type, request_id, 3) Explicit logs for 429 rate-limit events, backoff/retry, webhook responses, parsing errors, 4) Configurable log levels via env, 5) Secret redaction (tokens, signatures) and PII minimization, 6) Sampling for high-volume debug logs. Document fields and examples. Acceptance: logs validated in local run showing required fields and redaction; 429/backoff paths produce expected entries.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Basic Prometheus Metrics and Dashboards",
            "description": "Expose minimal Prometheus metrics and provide example Grafana dashboard.",
            "dependencies": [],
            "details": "Implement: 1) /metrics endpoint with counters and histograms (events_ingested_total, embeds_posted_total, webhook_post_latency_ms, parse_errors_total, rate_limit_events_total), 2) Gauges for in-flight batch size and memory footprint, 3) Labels: component, channel_id (hashed), outcome, 4) Docs for scraping and sample Grafana JSON with panels for latency, error rate, and rate-limit events. Acceptance: metrics scrapeable locally; dashboard visualizes test run.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Acceptance Criteria Definition and Checklist",
            "description": "Codify PRD acceptance criteria and thresholds as a checklist.",
            "dependencies": [],
            "details": "Create a definitive checklist enumerating: 1) Dedicated channel created, 2) Watcher streams embeds for tool use, assistant messages, errors, final summary (with tokens, cost, tool count, error count), 3) No measurable slowdown (target latency budget defined), 4) Retention/cleanup works, 5) Security: no token leakage, 6) Observability present (logs/metrics). Include pass/fail thresholds for latency and error rates and how to measure them. Acceptance: checklist merged under docs/acceptance.md.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Automated E2E Acceptance Test Harness",
            "description": "Build scripts to spin up bot+watcher with mock transcript and verify PRD criteria.",
            "dependencies": [
              "10.5"
            ],
            "details": "Implement: 1) Local harness to run bot and watcher against a mocked Discord webhook server, 2) Generate mock transcript covering tools, assistant messages, errors, and final summary, 3) Assertions: channel created, embed counts and required fields, final summary fields present, 4) Latency measurement from file append to webhook POST using timestamps; report p50/p95, 5) Exit non-zero on failures; CI job integration. Acceptance: all checks pass on a clean run within thresholds.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Load Testing for Batching and Rate-Limit Paths",
            "description": "Simulate high-frequency events to trigger batching and 429 handling; collect stats.",
            "dependencies": [
              "10.3",
              "10.4",
              "10.6"
            ],
            "details": "Implement: 1) Load generator that appends bursts to transcript to exceed posting rate, 2) Verify batching behavior and 429/backoff code paths triggered (via logs and metrics), 3) Collect latency histograms and error rates; export run report, 4) Define pass thresholds for p95 latency and 429 recovery time. Acceptance: report shows batching engaged, 429 handled without data loss, and metrics within thresholds.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Reliability and Restart-Resume Testing",
            "description": "Test watcher restart mid-run and ensure resume without agent impact; measure resource usage.",
            "dependencies": [
              "10.3",
              "10.6"
            ],
            "details": "Implement scenarios: 1) Kill and restart watcher during active stream; assert no duplicate or missing embeds and proper resume from offset, 2) Crash individual pipeline stages and verify auto-restart and health probes, 3) Measure memory footprint and CPU under steady state and after restarts; ensure no leak, 4) Validate independence from agent process. Acceptance: tests pass with zero data loss and stable resource usage.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Security Review and Token Leakage Verification",
            "description": "Perform security review of secrets handling and confirm no bot token in pods/logs.",
            "dependencies": [
              "10.3"
            ],
            "details": "Actions: 1) Configure secret scanning on logs and artifacts to detect tokens/keys, 2) Manual and automated grep for sensitive patterns in logs, 3) Verify deployment ensures bot token not mounted/exposed in sidecars; review env and volumes, 4) Document redaction policies and a security checklist. Acceptance: zero findings for token leakage; checklist merged under docs/security.md.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 10,
            "title": "Documentation Publishing and Operator Runbook",
            "description": "Polish READMEs, link docs, and publish an operator runbook for on-call procedures.",
            "dependencies": [
              "10.1",
              "10.2",
              "10.4",
              "10.5",
              "10.6",
              "10.7",
              "10.8",
              "10.9"
            ],
            "details": "Deliverables: 1) Root README with overview and quickstart linking architecture, deploy, acceptance, and security docs, 2) Operator runbook: dashboards to check, common alerts, troubleshooting playbooks (rate limits, missing category, webhook failures), 3) Known issues and SLOs, 4) Versioned doc links and release notes checklist. Acceptance: docs linted and reviewed; runbook validated by a dry-run exercise.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-11T07:38:41.531Z",
      "updated": "2025-08-11T07:38:41.531Z",
      "description": "Tasks for master context"
    }
  }
}