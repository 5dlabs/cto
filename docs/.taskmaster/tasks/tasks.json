{
  "master": {
    "tasks": [
      {
        "id": 11,
        "title": "Initialize Rust Monorepo and Project Scaffolding",
        "description": "Create a Cargo workspace with crates for watcher, bot, input-bridge, and a shared crate for formatting and common types. Set up tooling, CI, and basic configs to enable fast, low-overhead development per PRD.",
        "details": "- Create Cargo workspace structure:\n  - crates/watcher (sidecar tailer)\n  - crates/bot (Discord bot using serenity)\n  - crates/input-bridge (pod-local HTTP → FIFO/stdin)\n  - crates/shared (event types, embed builders, truncation, cost calc)\n- Dependencies (latest stable compatible): tokio (multi-thread), serde/serde_json, reqwest, serenity, axum, tower, thiserror, tracing/tracing-subscriber, anyhow, clap, clap-derive, notify, similar (diff), time, governor (rate limit optional), parking_lot, bytes.\n- Lint/format: rustfmt, clippy; set RUSTFLAGS for opt-level z (release) in watcher to reduce mem.\n- Features: shared: diff, embeds, cost-estimator; watcher: parity-mode, filters; bot: http-server; input-bridge: fifo-io.\n- .env loading via dotenvy (development only). In prod use env vars.\n- CI skeleton: cargo fmt --check, cargo clippy -D warnings, cargo test --all, build --release.\n- Tracing setup (JSON logs optional; include RUST_LOG config guidance).\n- Pseudo-code structure:\n  workspace/Cargo.toml:\n  [workspace]\n  members=[\"crates/*\"]\n  [profile.release]\n  opt-level=\"z\"\n  lto=true\n  codegen-units=1\n- README with quickstart and workspace commands.",
        "testStrategy": "- Build all crates: cargo build --release --all.\n- Run cargo test --all, ensure no panics.\n- Lint: cargo fmt -- --check; cargo clippy -D warnings.\n- Verify minimal binary sizes and memory usage baseline using /usr/bin/time on watcher hello-world run.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create workspace and crate skeleton",
            "description": "Initialize a Cargo workspace and scaffold crates: watcher, bot, input-bridge, shared with release profiles optimized for size.",
            "dependencies": [],
            "details": "- Create directory layout: crates/watcher, crates/bot, crates/input-bridge, crates/shared\n- Top-level Cargo.toml:\n  - [workspace] members = [\"crates/*\"]\n  - [profile.release] opt-level = \"z\", lto = true, codegen-units = 1\n  - [profile.dev] opt-level = 0\n  - [profile.release.package.\"watcher\"] panic = \"abort\" (override)\n- Add .cargo/config.toml:\n  - [build] target-dir = \"target\"\n  - Optionally add platform rustflags for stripping on Linux (link-arg=-s)\n- Crate scaffolds:\n  - shared: library crate (src/lib.rs) with placeholder modules: types, embeds, truncation, cost\n  - watcher: binary crate (src/main.rs) with clap boilerplate and hello-world main\n  - bot: binary crate (src/main.rs) with hello-world main\n  - input-bridge: binary crate (src/main.rs) with hello-world main\n- Ensure all crates compile with cargo build --all",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Wire dependencies and crate feature flags",
            "description": "Add required dependencies and define crate-level features per PRD across workspace and crates.",
            "dependencies": [
              "11.1"
            ],
            "details": "- Use [workspace.dependencies] in root Cargo.toml to pin versions centrally (latest stable-compatible): tokio (rt-multi-thread, macros), serde, serde_json, reqwest, serenity, axum, tower, thiserror, tracing, tracing-subscriber, anyhow, clap (derive, env), notify, similar, time, governor (optional), parking_lot, bytes\n- shared/Cargo.toml:\n  - deps: serde, serde_json, thiserror, tracing, time\n  - optional deps: similar (feature \"diff\")\n  - features: diff = [\"dep:similar\"], embeds = [], cost-estimator = [\"time\"], default = []\n- watcher/Cargo.toml:\n  - deps: tokio, notify, serde, serde_json, anyhow, thiserror, tracing, clap, parking_lot, bytes, shared = { path = \"../shared\", default-features = false, features = [\"diff\", \"embeds\", \"cost-estimator\"] }\n  - features: parity-mode = [], filters = [], default = []\n- bot/Cargo.toml:\n  - deps: serenity, tokio, reqwest, clap, anyhow, thiserror, tracing, shared = { path = \"../shared\" }\n  - optional deps: axum (feature \"http-server\"), governor (feature \"rate-limit\")\n  - features: http-server = [\"dep:axum\"], rate-limit = [\"dep:governor\"], default = []\n- input-bridge/Cargo.toml:\n  - deps: axum, tower, tokio, serde, serde_json, anyhow, thiserror, tracing, bytes\n  - features: fifo-io = [], default = []\n- Verify cargo check -p <crate> passes for each crate",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Set up dev tooling, formatting, and local env loading",
            "description": "Configure rustfmt, clippy, EditorConfig, dotenvy (dev-only), and size-focused build settings for watcher.",
            "dependencies": [
              "11.1"
            ],
            "details": "- Add rustfmt.toml (e.g., max_width = 100, use_small_heuristics = \"Max\")\n- Add .editorconfig for consistent whitespace/line endings\n- Add clippy config (optional) or rely on CI with -D warnings\n- Add dotenvy as a dev-dependency only where needed (watcher, bot) or behind feature:\n  - shared: add feature dev-env = [\"dep:dotenvy\"], and optional dependency dotenvy (dev only)\n  - Provide a helper fn shared::env::load_dev() that calls dotenvy::dotenv().ok() when debug_assertions and feature enabled\n- Optimize watcher binary size/perf:\n  - Confirm profile.release.package.\"watcher\" panic = \"abort\" (already set)\n  - Document RUSTFLAGS usage for local builds: RUSTFLAGS=\"-C strip=symbols\" cargo build -p watcher --release\n- Add rust-toolchain.toml with channel = \"stable\"",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Configure CI workflows (GitHub Actions)",
            "description": "Add CI to run fmt check, clippy with -D warnings, tests, and release build with proper caching.",
            "dependencies": [
              "11.2",
              "11.3"
            ],
            "details": "- .github/workflows/ci.yml:\n  - on: [push, pull_request]\n  - jobs.build: ubuntu-latest\n  - steps: checkout, set up Rust (use rust-toolchain), cache (Swatinem/rust-cache@v2), cargo fmt -- --check, cargo clippy --all-targets --all-features -D warnings, cargo test --all --all-features, cargo build --release --all\n  - concurrency: group by ref to avoid duplicate runs\n- Artifacts: optionally upload target/release sizes summary (from scripts in subtask 7)\n- Branch protection guidance for main to require CI green",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement tracing/logging initialization and env examples",
            "description": "Provide shared tracing setup (JSON optional) and example environment configuration including RUST_LOG guidance.",
            "dependencies": [
              "11.2"
            ],
            "details": "- In shared, add logging module with init_tracing(format: env) that:\n  - Reads LOG_FORMAT (\"json\" or \"pretty\"), default pretty\n  - Configures tracing_subscriber with EnvFilter (default \"info\" if RUST_LOG unset)\n  - Enables JSON output when LOG_FORMAT=json\n- Call shared::logging::init_tracing() from each binary's main before other init\n- Add .env.example with common vars:\n  - RUST_LOG=info\n  - LOG_FORMAT=pretty\n  - DISCORD_WEBHOOK_URL=... (bot/watcher)\n  - INPUT_FIFO=/agent/input.fifo (input-bridge)\n- Document JSON logs optionality and how to override per binary via env",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Author README and CONTRIBUTING",
            "description": "Write README with quickstart, workspace commands, run targets, and CONTRIBUTING notes covering tooling and CI expectations.",
            "dependencies": [
              "11.2",
              "11.3",
              "11.4",
              "11.5"
            ],
            "details": "- README contents:\n  - Overview of monorepo and crates\n  - Quickstart: install Rust, cargo build --all, cargo test --all\n  - Running: cargo run -p watcher|bot|input-bridge -- --help\n  - Env setup: .env for development only (dotenvy), use real env in production\n  - Logging: RUST_LOG guidance and LOG_FORMAT\n  - Profiles and size optimization rationale (opt-level z, LTO, codegen-units)\n  - Workspace commands cheat sheet\n- CONTRIBUTING.md:\n  - Code style (rustfmt), linting (clippy -D warnings), testing, PR checks\n  - Commit/PR guidelines and CI status requirements",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Baseline build, size checks, and automation scripts with DoD",
            "description": "Add scripts and Makefile for common tasks; capture baseline binary sizes and memory for watcher; define DoD.",
            "dependencies": [
              "11.1",
              "11.2",
              "11.3",
              "11.4",
              "11.5",
              "11.6"
            ],
            "details": "- scripts/size.sh:\n  - Build release: cargo build --release --all\n  - Strip symbols where supported\n  - Print sizes for target/release/{watcher,bot,input-bridge}\n  - Run /usr/bin/time -v ./target/release/watcher --help to capture baseline RSS\n  - Output summary to SIZES.md\n- Makefile targets:\n  - fmt, lint, test, build, run-watcher, run-bot, run-bridge, size\n- Ensure watcher has minimal clap CLI so --help exits quickly for measurement\n- Definition of Done:\n  - All crates compile and basic hello-world mains run\n  - CI green on fmt/clippy/test/build\n  - README and CONTRIBUTING present\n  - SIZES.md committed with baseline sizes and watcher memory metrics",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement Transcript JSONL Tailer and Event Parser (Watcher Core)",
        "description": "Efficiently tail Claude’s append-only JSONL transcript with ≤100ms latency, parse events into structured internal types, and expose a stream to downstream formatting/posting pipeline.",
        "details": "- Path resolution: prefer env WORKSPACE_PATH; else default ~/.claude/projects/<encoded-workspace>/<session>.jsonl. Provide discovery scan and retry with backoff.\n- Tailing approach: use tokio for async I/O; use notify crate (inotify/kqueue/FSEvents) to trigger reads; fallback to 100ms poll if events missed. Seek to end on startup (unless parity-mode) to avoid replay noise.\n- Robustness: handle file rotation or creation late. Reopen on rename/truncate.\n- Define event model in shared:\n  enum Event { ToolUse{kind:String, cmd:Option<String>, file:Option<String>, patch:Option<String>, cwd:Option<String>, ts:Option<String>}, Assistant{text:String, tokens_in:Option<u64>, tokens_out:Option<u64>, model:Option<String>, ts:Option<String>}, ToolResult{stdout:Option<String>, stderr:Option<String>, ts:Option<String>}, Completion{summary:Option<String>, totals:Option<Totals>, ts:Option<String>}, Error{message:String, ts:Option<String>} }\n  struct Totals{ tokens_in:u64, tokens_out:u64, tools:u64, errors:u64, duration_ms:Option<u64>, cost_usd:Option<f64>, model:Option<String> }\n- Parser: map Claude JSONL fields to Event, ignoring unknowns. Use serde_json::from_str per line, with defensive parsing to prevent crashes.\n- Filters from config (includeTools, includePatterns, minStdoutLength). Parity-mode bypasses filters and truncation decisions later.\n- Pseudo-code:\n  let mut file=open_and_seek(path, parity_mode);\n  let (tx, rx)=mpsc::channel(1024);\n  spawn(notify_watcher(path, tx.clone()));\n  loop { if let Some(line)=read_next_line(&mut file, 100ms)? { if let Ok(v)=serde_json::from_str::<Value>(&line) { if let Some(ev)=into_event(v) { if passes_filters(&ev) { tx.send(ev).await } } } } }\n- Performance: reuse buffers, avoid allocations; BufReader with capacity; parse small subset of fields.",
        "testStrategy": "- Unit: feed synthetic JSONL lines for all event types; assert parsing correctness and filter behavior.\n- Integration: write to a temp file in another task simulating append every 50ms; ensure latency from write to rx.recv < 100ms avg (tokio time + Instant).\n- Rotation test: truncate/rename file mid-stream and continue.\n- Fuzz test parser with random junk lines to ensure no panics.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Path resolution and discovery",
            "description": "Resolve transcript JSONL path via WORKSPACE_PATH or default discovery in ~/.claude/projects, with retry and backoff.",
            "dependencies": [],
            "details": "Implement path resolution: check WORKSPACE_PATH env first; else derive default ~/.claude/projects/<encoded-workspace>/<session>.jsonl. Provide a discovery scan to find the most recent/active session file. Expand ~ and handle cross-platform path rules. Implement retry with exponential backoff when file not found, and surface a resolved PathBuf once available. Prepare helpers to locate parent directory for watcher setup.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Async tailer core (tokio) with ≤100ms latency",
            "description": "Build async tail loop that reads appended JSONL lines, parses into Events, and streams them via mpsc.",
            "dependencies": [
              "12.1",
              "12.5"
            ],
            "details": "Create tail_transcript(path, parity_mode) -> mpsc::Receiver<Event>. Use tokio async I/O with BufReader and reusable buffers to minimize allocations. Seek to end on startup unless parity_mode. Implement a loop that waits on a read-trigger (pluggable) or falls back to timed polling every 100ms; when data is available, read complete lines and parse via serde_json into Value then into Event. Apply a passes_filters() hook (no-op initially; wired in subtask 12.6). Expose a bounded channel (capacity ~1024) to downstream pipeline. Ensure graceful shutdown and backpressure handling.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Filesystem notifications with fallback polling",
            "description": "Integrate notify crate to watch file and parent dir, with missed-event recovery and 100ms polling fallback.",
            "dependencies": [
              "12.1"
            ],
            "details": "Set up notify watcher (inotify/kqueue/FSEvents) on the target file if present and its parent directory. Translate create, write, rename, remove, and truncate events into lightweight triggers for the tail loop. Coalesce bursts and handle spurious events. Provide a periodic 100ms timer to ensure progress if events are missed. Expose a Send + Sync trigger channel compatible with the tailer core.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Rotation and late-creation handling",
            "description": "Detect and recover from file rename/truncate and handle files that appear after startup.",
            "dependencies": [
              "12.1",
              "12.2",
              "12.3"
            ],
            "details": "Detect truncation (file size smaller than last offset) and seek to start or end based on parity_mode. On rename or removal, close the old handle and attempt to reopen the new path discovered via parent directory events or discovery scan. Support late file creation by waiting on parent notifications and opening when the file appears. Track inode/metadata to avoid duplicate reads. Preserve offsets across reopen events safely.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Shared event model and defensive parser",
            "description": "Define Event and Totals types and implement robust JSONL-to-Event mapping that ignores unknown fields.",
            "dependencies": [],
            "details": "Add shared types: enum Event { ToolUse{kind, cmd?, file?, patch?, cwd?, ts?}, Assistant{text, tokens_in?, tokens_out?, model?, ts?}, ToolResult{stdout?, stderr?, ts?}, Completion{summary?, totals?, ts?}, Error{message, ts?} } and struct Totals{ tokens_in, tokens_out, tools, errors, duration_ms?, cost_usd?, model? }. Implement into_event(Value) -> Option<Event> and serde_json::from_str per line with defensive parsing: ignore unknowns, accept Option fields, clamp excessively large strings, and never panic on malformed input (log and skip).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Configurable filters and parity-mode semantics",
            "description": "Implement includeTools, includePatterns, minStdoutLength filters and parity-mode bypass.",
            "dependencies": [
              "12.5",
              "12.2"
            ],
            "details": "Define a Filters config struct: includeTools (bool), includePatterns (list of regex or glob patterns applied to text/stdout/stderr), minStdoutLength (usize). Implement passes_filters(&Event, &Filters, parity_mode) such that parity_mode disables all filtering. Wire this into the tailer core so only passing events are sent downstream. Ensure patterns compile once and are reused; handle invalid patterns gracefully.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Performance profiling and optimizations",
            "description": "Instrument latency and optimize allocations, buffer sizes, and channel capacities to meet ≤100ms target.",
            "dependencies": [
              "12.2",
              "12.3",
              "12.4",
              "12.6"
            ],
            "details": "Add tracing spans and Instant-based metrics for write-to-receive latency and parse duration. Tune BufReader capacity (e.g., 64–256 KiB) and reuse String/Vec buffers to avoid reallocations. Parse only required fields from JSON to minimize overhead. Set mpsc capacity to balance throughput and memory (e.g., 1024). Validate missed-event recovery path cost and adjust poll interval (default 100ms) via config. Document tunables and demonstrate reduced allocations via heap profiling where possible.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Testing and DoD verification",
            "description": "Implement unit and integration tests, including rotation and fuzzing, and verify <100ms average tail latency.",
            "dependencies": [
              "12.1",
              "12.2",
              "12.3",
              "12.4",
              "12.5",
              "12.6",
              "12.7"
            ],
            "details": "Unit tests: feed synthetic JSONL lines for every Event type and assert mapping correctness and filter behavior; ensure unknown fields are ignored and malformed lines do not panic. Integration tests: append to a temp file every ~50ms and assert average latency from write to rx.recv is <100ms across sustained runs. Rotation tests: simulate truncate and rename; verify tailing continues without duplication or loss. Fuzz tests: random and junk lines to ensure parser robustness. DoD: all tests pass and sustained average tail latency <100ms in integration.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 13,
        "title": "Embed Formatting, Truncation, Diffing, and Batching Library",
        "description": "Convert parsed events into Discord-compliant embeds and attachments with safe truncation, syntax highlighting, compact diffs for Write/Edit, and batching up to 10 embeds per message.",
        "details": "- Implement shared::embeds with builders per template:\n  - tool_use Bash: title ⚡ Bash; fields: Command (```bash), Working Dir, Tool Count; color 0xF39C12.\n  - tool_use Write/Edit/Read: titles 📝 Write / 👁️ Read; fields: File, Summary, Tool Count; color 0xF39C12.\n  - assistant: title optional; description=trimmed text; fields: Tokens In/Out, Session Cost (estimated), Model; color 0x3498DB.\n  - tool_result error: title ❌ Error; description=stderr code-fenced; field Total Errors; color 0xE74C3C.\n  - completion: title ✅ Complete; fields: Cost, Duration, Tokens total, Tools Used; color 0x27AE60.\n- Limits: respect 2000 content, 4096 desc, ≤25 fields/embed, ≤10 embeds/message.\n- Truncation helpers: trim_with_note(s, max, note=\"(truncated)\"), sanitize_code_fences(lang, body) replacing backticks runs and zero-width spaces to prevent premature closure. Disable mentions in payload via allowed_mentions: none.\n- Attachments: for overflow (e.g., long stdout, full diff), create files and link using attachment://filename in embed; include summary field with name and size.\n- Diffing: for Write/Edit if patch present; else compute diff via similar::TextDiff::from_lines(old, new). Extract contextual hunk (40–60 lines). Format as ```diff with +/-. Truncate hunks if needed and attach full diff.\n- Batching: struct Batch{embeds:Vec<Embed>, files:Vec<FilePart>}. Implement Batch::try_push(embed) -> bool; flush when count==10 or time window elapses.\n- Cost estimator: map common Claude models to per-token costs (configurable) to compute Session Cost from tokens. Fallback to 0 if unknown.\n- Pseudo-code:\n  fn to_embeds(ev:&Event, stats:&mut Stats, cfg:&Cfg)->BatchItems {\n    match ev { Event::ToolUse{..}=>build_tool_use_embed(...), Event::Assistant{..}=>build_assistant_embed(...), Event::ToolResult{..}=>build_result_embed(...), Event::Completion{..}=>build_complete_embed(...), Event::Error{..}=>build_error_embed(...) }\n  }\n  fn fenced(lang,&text)->String { format!(\"```{}\\n{}\\n```\", lang, sanitize(text)) }\n",
        "testStrategy": "- Snapshot tests: generate embeds JSON for representative events and compare to stored snapshots (insta crate).\n- Boundary tests: ensure field counts ≤25; descriptions ≤4096; proper truncation markers; no unclosed fences.\n- Diff tests: small, medium, and large patches; verify inline hunk lines 40–60; attachment created for large diffs.\n- Cost tests: verify estimator math from tokens and rates.\n- Security test: ensure allowed_mentions is set to none and content sanitization prevents @mentions.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Embed builders per event with Discord limits enforced",
            "description": "Implement shared::embeds builders for all event templates, enforcing Discord limits and producing compliant embeds ready for batching.",
            "dependencies": [
              "13.2",
              "13.3",
              "13.4",
              "13.5",
              "13.7"
            ],
            "details": "Create module shared::embeds with builder functions to render events into Discord Embed objects, enforcing: description <= 4096 chars, <= 25 fields per embed, and preparing content safe for message content <= 2000 if used. Builders: 1) build_tool_use_bash_embed(cmd: &str, cwd: &Path, tool_count: usize) -> Embed; title: \"⚡ Bash\"; fields: Command (fenced as bash), Working Dir, Tool Count; color 0xF39C12. 2) build_tool_use_fs_embed(kind: {Write, Edit, Read}, file: &Path, summary: &str, tool_count: usize, diff: Option<DiffSummary>) -> Embed; titles: \"📝 Write\" / \"👁️ Read\"; fields: File, Summary (trimmed), Tool Count; include inline diff snippet in description if provided; color 0xF39C12. 3) build_assistant_embed(text: &str, tokens_in: u32, tokens_out: u32, model: &str, cost: f64) -> Embed; optional title; description: trimmed text; fields: Tokens In, Tokens Out, Session Cost (estimated), Model; color 0x3498DB. 4) build_tool_result_error_embed(stderr: &str, total_errors: usize) -> Embed; title: \"❌ Error\"; description: code-fenced stderr; field: Total Errors; color 0xE74C3C. 5) build_completion_embed(cost: f64, duration_ms: u64, tokens_total: u32, tools_used: usize) -> Embed; title: \"✅ Complete\"; fields: Cost, Duration, Tokens total, Tools Used; color 0x27AE60. Use sanitize_code_fences and trim_with_note from 13.2; use diff formatting from 13.4; for overflow, delegate to attachments (13.5) by returning an embed that links attachment:// filenames in fields where appropriate. Ensure field name/value constraints (name <= 256, value <= 1024) are respected by truncation or redirection to attachments. Message payloads created by these builders must set allowed_mentions to none via 13.3. Acceptance: builders never produce embeds that exceed limits; snapshots in 13.8 validate structure and colors.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Truncation helpers and content sanitization",
            "description": "Implement trim_with_note and code fence sanitization to prevent overflows and premature fence closures.",
            "dependencies": [],
            "details": "Provide helpers in shared::text: 1) trim_with_note(s: &str, max: usize, note: &str) -> (String, bool) that trims at valid UTF-8 boundaries, reserves space for note, and appends note if truncated. 2) sanitize_code_fences(lang: &str, body: &str) -> String that replaces backtick runs inside body by inserting a zero-width space after backticks and replaces existing zero-width spaces with a safe placeholder or removes them to avoid stealth mentions; also strips or neutralizes Discord mentions like @everyone by inserting zero-width space after '@' for non-code fields if needed. 3) fenced(lang: &str, text: &str) -> String that wraps sanitize_code_fences output with triple backticks and ensures a trailing newline and a closing fence. 4) strip_ansi(input: &str) -> String to remove ANSI escape sequences from tool outputs before embedding. All helpers must be allocation-safe and unit-tested for boundary conditions (empty, exact max, multibyte chars, long runs of backticks).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Disable mentions in payload builder",
            "description": "Ensure outgoing Discord payloads have mentions disabled to prevent accidental pings.",
            "dependencies": [],
            "details": "Implement a message payload builder shared::payloads with AllowedMentions::none configuration (no roles, users, or everyone). Provide fn make_payload(content: Option<String>, embeds: Vec<Embed>, files: Vec<FilePart>) -> DiscordMessage that always sets allowed_mentions to none. Add a guard that rejects payloads if content contains raw @everyone or @here unless inside a code fence (best-effort heuristic). Acceptance: integration test verifies DiscordMessage JSON includes allowed_mentions with empty parse arrays.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Diffing utilities with contextual hunks and diff formatting",
            "description": "Implement utilities to parse or compute diffs and format compact contextual snippets as diff code blocks.",
            "dependencies": [
              "13.2"
            ],
            "details": "Create shared::diff: 1) enum DiffInput { PatchText(String), OldNew { old: String, new_: String } }. 2) compute_diff(input: DiffInput) -> DiffSummary that uses similar::TextDiff::from_lines for OldNew; if PatchText given, parse minimally or fallback to compute. 3) Extract contextual hunks totaling approximately 40–60 lines; default target 50 lines across hunks with equal context around changes; collapse large sections with \"...\" markers. 4) Format inline snippet as a diff code fence (lang \"diff\") with lines prefixed +/-. 5) If total diff exceeds inline budget (e.g., 3 KB or would push embed description over 4096), mark is_truncated = true and return full_text in DiffSummary for attachment use. 6) Return DiffSummary { inline_snippet: String, full_text: Option<String>, is_truncated: bool, added: usize, removed: usize }. Ensure sanitization via fenced(\"diff\", ...) from 13.2 and no unclosed fences.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Attachments pipeline for overflow content",
            "description": "Create attachment handling and embed linking using attachment:// and size summaries.",
            "dependencies": [
              "13.2",
              "13.4"
            ],
            "details": "Implement shared::attachments: 1) struct FilePart { filename: String, bytes: bytes::Bytes, content_type: String }. 2) fn make_attachment(filename: &str, bytes: impl Into<bytes::Bytes>, content_type: &str) -> FilePart; sanitize filename to remove path separators. 3) Policy helpers: should_attach_text(field_limit: usize, text: &str) -> bool; should_attach_desc(desc_limit: usize, text: &str) -> bool; use 1024 for field values, 4096 for descriptions. 4) fn link_attachment_field(embed: &mut Embed, field_name: &str, attach: &FilePart) to insert a field with value like \"attachment://filename\" and an adjacent summary field with humanized size. 5) For diffs: if DiffSummary.full_text is Some or inline truncation occurs, create a .diff attachment and link it; for long stdout/stderr, create .log attachments. 6) Ensure binary safety and size accounting; expose total attachments count to the batcher. Acceptance: snapshot tests show attachment links and size summaries; large inputs move to attachments.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Batching struct and flush policy",
            "description": "Implement Batch and Batch::try_push with a time/window-based flush strategy up to 10 embeds per message.",
            "dependencies": [],
            "details": "Create shared::batch: 1) struct Batch { embeds: Vec<Embed>, files: Vec<FilePart> }. 2) impl Batch { fn try_push(&mut self, embed: Embed) -> bool that returns false if pushing would exceed 10 embeds; fn add_file(&mut self, f: FilePart); fn is_full(&self) -> bool; }. 3) Batcher with windowed flushing: struct Batcher { current: Batch, max_embeds: usize, window: std::time::Duration, last_flush_at: Instant }. Methods: new(window), push_items(embeds: Vec<Embed>, files: Vec<FilePart>) -> Option<Batch> that flushes when count reaches 10 or when now - last_flush_at >= window; fn force_flush(&mut self) -> Option<Batch>. 4) Ensure attachments carry over with the batch. Acceptance: unit tests verify not exceeding 10 embeds, correct flush on window and count.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Cost estimator mapping and safe fallbacks",
            "description": "Add configurable model-to-rate mapping and compute session cost from token counts.",
            "dependencies": [],
            "details": "Implement shared::costs: 1) struct ModelRates { input_per_token: f64, output_per_token: f64 }. 2) Configurable map: HashMap<String, ModelRates> loaded from cfg or defaults for common Claude models; allow overrides at runtime. 3) fn estimate_session_cost(model: &str, tokens_in: u32, tokens_out: u32, rates: &HashMap<String, ModelRates>) -> f64 that returns 0.0 if model unknown; round to 4 decimal places for display. 4) Integrate with assistant embed builder to show Session Cost (estimated). Acceptance: unit tests for known/unknown models and zero-safety.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Snapshot and boundary tests (insta) for builders and limits",
            "description": "Create comprehensive insta snapshots and boundary tests validating builders, truncation, fences, and diffs.",
            "dependencies": [
              "13.1",
              "13.2",
              "13.3",
              "13.4",
              "13.5",
              "13.6",
              "13.7"
            ],
            "details": "Add tests: 1) Snapshot JSON of Embed payloads for representative events (tool_use Bash; tool_use Write/Edit with and without diffs; assistant long text; tool_result error; completion). 2) Boundary tests: field count never exceeds 25; description <= 4096; field values <= 1024; message content (if used) <= 2000. 3) Truncation markers: verify presence of note \"(truncated)\" when applied. 4) Fence closure: no unclosed fences; code blocks remain intact even with backtick runs. 5) Diff tests: small, medium, large patches; inline hunks within ~50 lines; large diffs attach full version and mark is_truncated. 6) Attachment linkage: verify attachment:// references and size summary fields. 7) Batching tests: verify flush at 10 embeds and on time window. DoD: all tests green, snapshots stable.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "Security tests: mention neutralization and ANSI stripping",
            "description": "Ensure no accidental mentions or terminal escapes slip through embeds and fields.",
            "dependencies": [
              "13.1",
              "13.2",
              "13.3",
              "13.5"
            ],
            "details": "Add tests to validate: 1) allowed_mentions is none on all payloads (no users, roles, everyone). 2) Strings containing @everyone, @here, and <@id> do not trigger mentions (either blocked by allowed_mentions or neutralized in non-code fields). 3) ANSI escape sequences in tool outputs are stripped by strip_ansi before embedding and do not render styled text. 4) Zero-width character handling: input containing ZWSP or ZWNJ cannot be used to craft stealth mentions; sanitization preserves readability. 5) Long untrusted stderr/stdout is moved to attachments when exceeding limits, ensuring embeds remain within constraints.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 14,
        "title": "Discord Webhook Client with Rate Limiting, Backoff, and Attachments",
        "description": "Post batched embeds via channel-scoped webhook, respecting Discord limits and handling 429 responses with proper backoff. Support attachments for large outputs.",
        "details": "- Implement shared::webhook::Client using reqwest with connect pool, gzip enabled, and timeout sensible defaults.\n- API:\n  - send_batch(webhook_url:&str, batch:Batch) -> Result<()>\n  - queue + worker: an mpsc channel to buffer BatchItems; worker aggregates to ≤10 embeds per request, flushes on size or time (e.g., 250ms) for near-real-time.\n- Rate limit handling:\n  - On 429, parse headers: Retry-After or X-RateLimit-Reset-After; sleep accordingly; requeue with jitter. Honor route-specific rate limit for webhooks.\n  - Backoff with exponential + jitter on 5xx; cap retries.\n- Multipart for attachments: use files[] name, attach via payload_json referencing attachments with attachment://.\n- Enforce content limits pre-send to avoid rejections.\n- Disable mentions: allowed_mentions: { parse: [] }.\n- Telemetry: logs for 2xx/4xx/5xx; counters for retries; latency histograms.\n- Pseudo-code:\n  loop { batch=collect_up_to(10, 250ms); let res=post(webhook_url, batch).await; match res.status { 2xx=>ok, 429=>sleep(retry_after); requeue(batch); 5xx=>retry_with_backoff(batch); 4xx=>log and drop } }",
        "testStrategy": "- Wiremock server to emulate Discord webhook. Scenarios:\n  - 200 with embeds and attachments -> assert payload structure.\n  - 429 with Retry-After header -> assert delay and retry once.\n  - 5xx transient -> retries with backoff then success.\n  - Payload exceeding limits -> ensure truncated before send.\n- Concurrency test: multiple quick batches coalesce up to 10 embeds.\n- Measure end-to-end post latency remains < network limits.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Watcher Binary Integration and Pipeline Orchestration",
        "description": "Wire tailer, formatter, and webhook client into a low-overhead sidecar binary with config, initial header embed, final summary, health checks, and parity-mode.",
        "details": "- CLI/env via clap + env vars:\n  - DISCORD_WEBHOOK_URL (required), WORKSPACE_PATH, POLL_INTERVAL_MS (default 100), BATCH_SIZE (default 10), PARITY_MODE=false, FILTERS (JSON), COST_RATES (JSON), LOG_LEVEL.\n- Startup: resolve transcript path (block/retry until present). Post initial header embed including run metadata (taskId, attempt, shortId if available from env), model (if first event has it), timestamps.\n- Pipeline:\n  - tailer_rx -> formatter -> batch_queue -> webhook_client.\n  - Maintain Stats {tokens_in/out, tools_used, errors, start_ts} updated per event.\n- Finalization: on Completion event or tail idle timeout + EOF, post ✅ Complete embed with totals, cost, duration; revoke webhook optional by notifying bot endpoint if configured.\n- Health: expose /healthz with Axum returning 200 if tailer alive and webhook reachable lately; /metrics optional.\n- Performance: use Tokio current_thread in watcher if single-thread suffices; pin poll interval; reuse buffers; set reqwest client reuse.\n- Pseudo-code:\n  let cfg=load_cfg();\n  post_header(&cfg.webhook);\n  while let Some(ev)=rx.recv().await { let items=to_embeds(ev,&mut stats,&cfg); batch_tx.send(items).await; }\n  post_completion(stats);\n",
        "testStrategy": "- End-to-end with a temp JSONL file and wiremock webhook: simulate run with all event types; assert initial header and final summary posted.\n- Failure modes: webhook unavailable initially -> retries; file appears late -> pipeline recovers.\n- Parity-mode test: ensure no filtering and minimal truncation.\n- Resource checks: run under /usr/bin/time; verify CPU <1% average and RSS <64Mi for synthetic stream.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "CLI and environment config loader",
            "description": "Implement clap + env config loader with validation, defaults, and a central Config struct.",
            "dependencies": [],
            "details": "- Use clap with env var fallbacks to populate Config.\n- Supported env/flags: DISCORD_WEBHOOK_URL (required, URL), WORKSPACE_PATH (optional), POLL_INTERVAL_MS (default 100), BATCH_SIZE (default 10), PARITY_MODE (default false), FILTERS (JSON), COST_RATES (JSON), LOG_LEVEL (default info), HTTP_ADDR (optional, e.g., 127.0.0.1:8080), WEBHOOK_REVOKE_URL (optional URL).\n- Validate: webhook URL non-empty and valid; POLL_INTERVAL_MS > 0; BATCH_SIZE >= 1; JSON fields parse to expected types.\n- Parse FILTERS and COST_RATES into typed structs (serde). COST_RATES should map model names to per-token cost for prompt/completion.\n- Include run metadata sources: TASK_ID, ATTEMPT, SHORT_ID from env if present.\n- Initialize tracing with LOG_LEVEL.\n- Construct a single reqwest::Client with connection reuse, keepalive, pooled DNS; expose in Config or Context.\n- Unit tests for defaults, invalid/missing values, and JSON parse errors.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Transcript path resolution with blocking retry",
            "description": "Resolve transcript JSONL path and block with retries until the file appears.",
            "dependencies": [
              "15.1"
            ],
            "details": "- Determine transcript path from WORKSPACE_PATH (e.g., <workspace>/transcript.jsonl or provided pattern); allow an override via an optional env/flag if provided.\n- Implement resolve_transcript_path() that polls at POLL_INTERVAL_MS until the file exists (and is readable), handling ENOENT gracefully.\n- Log periodic status and first success; support cancellation on shutdown signals.\n- Return a PathBuf and initial file offset for tailing.\n- Unit test: file appears late; permission error; immediate presence; cancellation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Pipeline wiring: tailer → formatter → batching → webhook client",
            "description": "Wire the async pipeline with channels, backpressure, retries, and performance tuning.",
            "dependencies": [
              "15.1",
              "15.2"
            ],
            "details": "- Tailer: async JSONL tail of the resolved file; non-blocking reads; emits Event items; detects EOF and idle periods; uses pinned poll interval.\n- Formatter: convert Events to Discord embeds/messages applying filters and parity-mode toggles; reuse buffers to limit allocations.\n- Batch queue: bounded mpsc; accumulate up to BATCH_SIZE or flush on time; coalesce small batches; ensure ordering.\n- Webhook client: send batches to DISCORD_WEBHOOK_URL using shared reqwest::Client; implement retry with exponential backoff and jitter; respect Discord rate limits if present.\n- Error handling: propagate fatal errors; retry transient ones; ensure backpressure does not deadlock.\n- Tokio runtime: prefer current_thread for watcher; spawn minimal tasks; pin poll interval; avoid blocking.\n- Pseudo-flow: while let Some(ev)=tailer_rx.recv() { let items=to_embeds(ev,&mut stats,&cfg); batch_tx.send(items).await; }",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Initial header embed posting",
            "description": "Post the startup header embed with run metadata and optionally model from first event.",
            "dependencies": [
              "15.1",
              "15.3"
            ],
            "details": "- Build header embed with: taskId, attempt, shortId (if present), start timestamp, workspace hint; include host/pid if desired.\n- Model inclusion strategy: if the first observed event contains model, include it; otherwise post header immediately and send a lightweight follow-up update when model is known.\n- Ensure idempotent header posting on retries; include a run correlation id to link messages.\n- Verify via wiremock that header is posted before processing bulk events.\n- Plumb webhook client from pipeline; handle transient failures with retries.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Stats tracking and state updates per event",
            "description": "Implement Stats struct and update logic for tokens/tools/errors and timings.",
            "dependencies": [
              "15.1",
              "15.3"
            ],
            "details": "- Stats fields: tokens_in, tokens_out, tools_used (map or set with counts), errors (count and last error), start_ts, last_event_ts, batches_sent, webhook_failures.\n- Update in formatter and/or a dedicated observer per event; record tool invocations and token deltas.\n- Cost calculation: use COST_RATES to compute totals by model and overall; expose snapshot for final summary and /metrics.\n- Concurrency: wrap in Arc<Mutex> or parking_lot::Mutex; provide cheap snapshot method.\n- Unit tests: accumulation correctness, cost math by model, error increments.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Completion detection and final summary post",
            "description": "Detect completion by explicit event or idle EOF, then post ✅ summary and optional webhook revoke.",
            "dependencies": [
              "15.3",
              "15.4",
              "15.5"
            ],
            "details": "- Completion signals: explicit Completion event, or tail idle timeout after EOF, or external shutdown signal.\n- Drain pipeline: flush remaining batches; ensure no duplicates; capture final Stats snapshot and duration.\n- Build and post final summary embed with totals (tokens, tools, errors), cost, and elapsed time; mark run as complete with ✅.\n- Optional revoke: if WEBHOOK_REVOKE_URL is set, notify bot endpoint to revoke/cleanup the webhook; retry transient failures.\n- Guarantee single execution (use OnceCell/flag) and graceful shutdown ordering.\n- Tests: verify final embed and revoke call under both completion modes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Health endpoints with Axum",
            "description": "Expose /healthz (and optional /metrics) reflecting tailer liveness and webhook reachability.",
            "dependencies": [
              "15.1",
              "15.3",
              "15.5"
            ],
            "details": "- Axum server bound to HTTP_ADDR if provided; otherwise disabled.\n- /healthz: 200 if tailer task is alive and last webhook success within a recent window; include basic JSON body with timestamps and queue depth; else 503.\n- /metrics (optional): expose Prometheus metrics (counters for events, batches, failures, gauges for queue, timestamps, cost totals).\n- Share state via Arc context (stats snapshot, last_webhook_ok_at, last_event_at, tailer_alive flag).\n- Lightweight handlers suitable for current_thread runtime; avoid blocking.\n- Tests: hit /healthz during run and after completion; verify status transitions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Parity-mode behavior and minimal truncation toggles",
            "description": "Implement PARITY_MODE to bypass filters and apply only minimal truncation for Discord limits.",
            "dependencies": [
              "15.1",
              "15.3",
              "15.5"
            ],
            "details": "- When PARITY_MODE=true: disable FILTERS; avoid summarization; preserve event content ordering; only truncate to meet Discord message/embed limits; mark messages as parity for traceability.\n- When PARITY_MODE=false: apply configured FILTERS and normal formatting/truncation rules.\n- Ensure cost/stat updates are identical across modes for equal token counts.\n- Tests: parity mode produces near-pass-through embeds; non-parity mode applies filters; verify no hidden drops.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 9,
            "title": "End-to-end tests with wiremock and synthetic JSONL",
            "description": "Write E2E tests covering normal flow, late file, webhook outage, and parity-mode.",
            "dependencies": [
              "15.1",
              "15.2",
              "15.3",
              "15.4",
              "15.5",
              "15.6",
              "15.7",
              "15.8"
            ],
            "details": "- Use temp dir + synthetic transcript JSONL including all event types and a Completion event.\n- Wiremock webhook: assert header posted first, batches follow in order, final ✅ summary with correct totals/cost/duration.\n- Failure recovery: start with webhook returning 5xx for N attempts then recover; ensure retries and eventual success; start without transcript then create it; confirm pipeline recovers.\n- Parity-mode test: ensure no filtering and only minimal truncation is applied.\n- Health checks: during run /healthz is 200; during outage flips to 503; returns to 200 after recovery.\n- Resource sanity: run under load with small POLL_INTERVAL_MS and BATCH_SIZE; assert no excessive memory growth (approx via metrics) and timely processing.\n- DoD: e2e test green with header and final summary; resilience scenarios pass.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 16,
        "title": "Discord Bot Service: Channel Lifecycle and Webhook Provisioning",
        "description": "Build a serenity-based bot that creates per-run channels and channel-scoped webhooks in the “Agent Runs” category, returning {channelId, webhookUrl}. Implement retention-based archive/delete.",
        "details": "- Serenity setup: minimal intents (GUILD_MESSAGES, GUILDS) and http client. Load DISCORD_BOT_TOKEN from secret.\n- HTTP API (Axum) for orchestrator:\n  - POST /run/create {taskId, attempt, shortId, guild_id?, category_name?}\n    - Ensure/find category “Agent Runs” (or provided), create text channel name format run-{taskId}-{attempt}-{shortId}.\n    - Create channel-scoped webhook; return {channelId, webhookUrl, webhookId} JSON.\n  - POST /run/complete {channelId, webhookId} -> delete webhook; optionally delete channel or schedule retention delete.\n- Store run map in memory (DashMap) keyed by channelId: {taskId, attempt, shortId, webhookId, created_at}. For crash resilience, optionally persist to a small file (MVP optional).\n- Post initial header embed via webhook or bot message if needed.\n- Retention: config RETENTION_HOURS; spawn background task scanning map and deleting channels/webhooks past window.\n- Permissions: restrict @everyone send if input disabled; bot manage webhooks and channels.\n- Pseudo-code (serenity + axum skeleton):\n  struct BotState{ http:Arc<Http>, cache:Cache, runs:DashMap<ChannelId, RunInfo> }\n  async fn create_run(Json(req)): -> Json<Response> { let cat=find_or_create_category(...); let ch=guild.create_channel(...).await?; let wh=ch.create_webhook(&state.http, \"run\").await?; Ok(Json({channelId:ch.id, webhookUrl:wh.url(), webhookId:wh.id})) }\n",
        "testStrategy": "- Use a test guild: run bot pointing to a sandbox server.\n- Call /run/create via curl; assert channel created under category and webhook returned.\n- Call /run/complete; assert webhook deleted and channel removed after retention.\n- Negative: duplicate create for same run -> either reuse or create uniquely suffixed; ensure no panic.\n- Permission test: verify pod never receives bot token; only webhookUrl is returned.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Serenity bot skeleton and configuration",
            "description": "Initialize a serenity-based bot with minimal gateway intents and an HTTP client, load configuration from environment, and expose shared state.",
            "dependencies": [],
            "details": "Set up a crates/bot binary. Initialize serenity::Client with intents GUILDS and GUILD_MESSAGES. Load DISCORD_BOT_TOKEN from env/secret manager. Build BotState with Arc<Http>, Cache, and placeholders for runs map. Wire logging/tracing, graceful shutdown, and config struct (e.g., RETENTION_HOURS, DEFAULT_GUILD_ID, DEFAULT_CATEGORY_NAME). Ensure bot token is never logged.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Axum HTTP API: POST /run/create and /run/complete",
            "description": "Expose Axum endpoints for orchestrator to create and complete runs with JSON schemas and error handling.",
            "dependencies": [
              "16.1",
              "16.3",
              "16.4"
            ],
            "details": "Spin up an Axum server with JSON extractor/response. Define CreateRunRequest {taskId:String, attempt:u32, shortId:String, guild_id:Option<u64>, category_name:Option<String>} and CreateRunResponse {channelId:u64, webhookUrl:String, webhookId:u64}. Define CompleteRunRequest {channelId:u64, webhookId:u64}. Implement handlers calling category/channel/webhook helpers, returning typed JSON. Add structured errors (status codes for bad input, not found, Discord API errors). Bind port from env (e.g., BOT_HTTP_PORT).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Category discovery/creation and channel naming",
            "description": "Implement utilities to find or create the target category and generate sanitized per-run channel names.",
            "dependencies": [
              "16.1"
            ],
            "details": "Add ensure_category(guild_id, category_name_or_default) -> CategoryId that searches existing categories by name (case-insensitive) and creates if missing. Implement build_channel_name(taskId, attempt, shortId) -> String using format run-{taskId}-{attempt}-{shortId}, lowercased, hyphen-only, trimmed to Discord limits. Provide create_text_channel_in_category(guild_id, category_id, name, topic?). Handle missing guild_id by using DEFAULT_GUILD_ID if configured.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Channel and channel-scoped webhook provisioning",
            "description": "Create per-run text channels in the category and provision a channel-scoped webhook, returning IDs and URL.",
            "dependencies": [
              "16.3"
            ],
            "details": "Given guild and category, create the text channel with topic including taskId/attempt/shortId. Create a channel-scoped webhook named \"run\" and fetch its URL and ID. Return {channelId, webhookUrl, webhookId}. Optionally post an initial header embed via webhook. Ensure bot manages channels/webhooks and does not leak the bot token in responses or logs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "In-memory run map (DashMap) with optional persistence",
            "description": "Track run metadata keyed by channelId and optionally persist to a small file for crash resilience.",
            "dependencies": [
              "16.2",
              "16.4"
            ],
            "details": "Define RunInfo {taskId, attempt, shortId, webhookId, created_at}. Use DashMap<ChannelId, RunInfo> in BotState. Insert on successful create; remove on complete/delete. Implement lightweight JSON persistence to a file path (e.g., RUNS_STATE_PATH) on every mutation or periodically; load at startup and validate entries (dropping those whose channel no longer exists). Guard with Tokio tasks and atomic write temp-rename pattern.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Retention worker for archive/delete past RETENTION_HOURS",
            "description": "Implement a background task that scans runs and deletes webhooks/channels older than the configured retention window.",
            "dependencies": [
              "16.5"
            ],
            "details": "Read RETENTION_HOURS from config. Spawn a periodic task (e.g., every 60s) that computes cutoff and iterates DashMap entries. For each expired run: delete webhook, then delete the channel; handle partial failures with retries/backoff and skip-list to avoid hot loops. Remove from map after successful cleanup. Optionally support an ARCHIVE_CATEGORY move before deletion when an ARCHIVE_ONLY flag is set.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Permissions and safety controls",
            "description": "Apply channel permission overwrites to restrict @everyone from sending messages and implement safety practices.",
            "dependencies": [
              "16.4"
            ],
            "details": "On channel creation, set PermissionOverwrites for @everyone: deny SEND_MESSAGES, allow VIEW_CHANNEL and READ_MESSAGE_HISTORY. Optionally grant a configured role ability to send if needed. Ensure AllowedMentions are restricted for any bot posts. Validate the bot has MANAGE_CHANNELS and MANAGE_WEBHOOKS. Redact secrets in logs and never return the bot token via the API.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "Integration tests against sandbox guild",
            "description": "Write end-to-end tests for create/complete flows, idempotency, and retention using a sandbox Discord guild.",
            "dependencies": [
              "16.1",
              "16.2",
              "16.3",
              "16.4",
              "16.5",
              "16.6",
              "16.7"
            ],
            "details": "Use a test guild ID from env (TEST_GUILD_ID) and a disposable category name. Test: call /run/create and assert channel under category and a webhook URL/ID returned; call /run/complete and assert webhook deleted; set RETENTION_HOURS small (e.g., 0.001) and assert channel removed by worker. Test idempotent create for same {taskId, attempt, shortId} returns existing channel/webhook (by finding existing channel name and reusing webhook or map record). Clean up any leftovers on failure.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 17,
        "title": "Optional Operator Input Pipeline (Bot Relay → Pod-local HTTP Bridge)",
        "description": "Enable operators to send ad-hoc guidance from the run’s channel to the running agent via a local HTTP bridge that writes to agent FIFO/stdin. Enforce channel scoping per run.",
        "details": "- Bot relay (serenity EventHandler::message): accept messages or a slash command only in known run channels (validate against runs map). On command or message, POST to http://input-bridge/input with JSON {text, author, channelId, run:{taskId, attempt, shortId}}.\n- Reuse reqwest client; send acknowledgement in channel on success.\n- Bridge service (axum):\n  - POST /input -> reads JSON payload and writes a streaming line to FIFO/stdin.\n  - FIFO path from env INPUT_FIFO (default /agent/input.fifo). If FIFO not present, try to write to child process stdin file descriptor if exposed.\n  - Implement nonblocking open with retry; write newline-terminated UTF-8.\n  - Bind only 127.0.0.1; no external exposure.\n- Security: Bot validates message.channel_id belongs to a run; ignore others. Do not forward attachments (MVP). Rate limit bridge (e.g., 10 r/s) to protect agent.\n- Example bot handler:\n  if msg.channel_id == run.channel_id { if msg.content.starts_with(\"/send \") || cfg.accept_plain { let payload=...; client.post(\"http://input-bridge/input\").json(&payload).send().await?; msg.channel_id.say(&ctx.http, \"✅ forwarded\").await?; } }\n",
        "testStrategy": "- Unit: bot side - simulate Message events for in-channel and out-of-channel; assert only in-channel calls bridge.\n- Bridge: create a temp named pipe; POST JSON; confirm bytes written to FIFO.\n- End-to-end (local): run bridge and a dummy reader of FIFO; send bot command; observe message arrives.\n- Security: attempt POST from non-local interface (if forced) should be rejected; invalid JSON returns 400; bridge rate limit enforced.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Bot-side message/command handler scoped to run channels",
            "description": "Implement a serenity EventHandler::message that only accepts operator input from channels associated with active runs, parses /send commands or plain messages (if enabled), and prepares payloads for the local HTTP bridge.",
            "dependencies": [],
            "details": "Maintain a concurrent runs map keyed by channel_id with values {taskId, attempt, shortId}. In EventHandler::message, early-return unless msg.channel_id is in the runs map. Accept text when msg.content starts with \"/send \" or when config accept_plain=true; strip the command prefix and trim. Ignore attachments and non-text content for MVP. Construct payload JSON {text, author, channelId, run:{taskId, attempt, shortId}}. Reuse a single reqwest::Client across messages (e.g., in BotState). POST to http://input-bridge/input. Do not send acks/errors here; wire that in subtask 5.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "HTTP bridge service (Axum) with POST /input",
            "description": "Expose a local Axum service that accepts JSON payloads for operator input and writes newline-terminated UTF-8 to the agent input stream via the writer component.",
            "dependencies": [
              "17.3"
            ],
            "details": "Define payload schema {text: String, author: String, channelId: String, run: {taskId: String, attempt: u32|String, shortId: String}} and validate text is non-empty and UTF-8 safe. Build Axum router with POST /input; on request, call a Writer trait to perform the write and map success to 200 OK with a minimal JSON body {status:\"ok\"}. Map writer errors to appropriate HTTP status (e.g., 429 from backpressure, 503 when agent input unavailable, 400 for invalid payload). Ensure newline termination added exactly once. Use Tower layers for error handling and JSON body size limits.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "FIFO/stdin writer with nonblocking open, retry, and backpressure",
            "description": "Implement a writer that targets INPUT_FIFO when present or falls back to child process stdin, performing nonblocking open with retry and safe UTF-8 line writes.",
            "dependencies": [],
            "details": "Read FIFO path from env INPUT_FIFO (default /agent/input.fifo). Attempt O_NONBLOCK open of FIFO for write; if reader not present, retry with exponential backoff up to a bounded deadline, then return a retriable error. If FIFO not found or open fails permanently, attempt to write to an exposed child stdin file descriptor/handle if available. Ensure each write appends a single trailing newline; handle partial writes; flush if using stdin. Implement backpressure-aware writes: if pipe returns EAGAIN/EWOULDBLOCK, retry with jitter and surface a 429-style error to caller after budget is exhausted. Provide a simple Writer trait with write_line(&str) -> Result<(), WriterError> and an implementation over FIFO/stdin.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Local-only binding and basic rate limiting",
            "description": "Restrict the bridge service to loopback and enforce per-process rate limiting to protect the agent input.",
            "dependencies": [
              "17.2",
              "17.3"
            ],
            "details": "Bind Axum server only to 127.0.0.1 with an explicit non-TLS HTTP listener; avoid container port exposure. Add a Tower layer using governor or tower-ratelimit to limit to ~10 requests per second with a small burst (configurable). Return 429 with a clear JSON error body when rate limited. Ensure rate limit applies before invoking the writer. Optionally add simple request size limits to prevent large inputs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Bot acknowledgements, error feedback, and retry policy",
            "description": "Provide user-visible acks in the run channel on success, surface clear errors on failure, and implement limited retries to the bridge.",
            "dependencies": [
              "17.1",
              "17.2"
            ],
            "details": "On successful POST to the bridge, reply in the same channel with a short acknowledgement (e.g., \"✅ forwarded\"). On known errors, map to actionable messages: 429 -> \"rate limited, try again shortly\"; 503 -> \"agent unavailable\"; other -> generic failure with correlation ID. Implement retry with exponential backoff for transient errors (5xx, connect timeouts) up to a small cap (e.g., 3 attempts) and suppress duplicate acks. Ensure slash command responses or message replies do not ping everyone; respect Discord rate limits. Log errors with context (run ids, channel id, http status).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Configuration wiring and observability",
            "description": "Add configuration flags and minimal logging/metrics across bot and bridge components.",
            "dependencies": [
              "17.1",
              "17.2",
              "17.3",
              "17.4"
            ],
            "details": "Config: ACCEPT_PLAIN (bool), INPUT_BRIDGE_URL (default http://input-bridge/input) for bot; INPUT_FIFO, RATE_LIMIT_RPS (default 10), LOG_LEVEL for bridge. Use env vars or a small config struct. Observability: structured logs with run/channel context; count metrics for inputs accepted, forwarded, dropped (out-of-scope channel), rate-limited, writer errors. Expose a lightweight /health endpoint on loopback or rely on process health logs. Ensure sensitive data (tokens) not logged.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Unit and integration tests",
            "description": "Validate channel scoping, FIFO write behavior, rate limiting, and the end-to-end local flow from bot to agent input.",
            "dependencies": [
              "17.1",
              "17.2",
              "17.3",
              "17.4",
              "17.5",
              "17.6"
            ],
            "details": "Bot unit tests: simulate Message events for in-run and out-of-channel; assert only in-channel attempts to call bridge; verify command parsing and payload content. Bridge tests: create a temp named pipe, run the Axum server bound to 127.0.0.1 with the FIFO writer, POST JSON, and assert bytes received and newline termination; test fallback when FIFO absent. Rate limit tests: hammer /input and assert 429s after the configured threshold. End-to-end local: start bridge, spawn a dummy FIFO reader, simulate bot handler call, and confirm the reader receives the line. Include negative tests for empty text, oversized payloads, and writer backpressure.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 18,
        "title": "Security and Isolation Hardening",
        "description": "Ensure pods never hold bot tokens, validate run/channel mappings to prevent cross-run injection, revoke webhooks on completion, and neutralize mentions/content hazards.",
        "details": "- Token isolation: Only the bot deployment stores DISCORD_BOT_TOKEN (K8s Secret). Sidecar watcher receives only DISCORD_WEBHOOK_URL.\n- Mapping validation: bot maintains ChannelId→Run mapping; message relay checks this map before forwarding. Add signature (HMAC) option for bridge requests if desired (shared secret via env) though not strictly required for loopback.\n- Webhook lifecycle: on /run/complete or after final summary, delete webhook via bot; sidecar optionally calls bot endpoint to signal completion.\n- Allowed mentions: Set allowed_mentions:{parse:[]} in all webhook payloads to prevent @ mentions.\n- Sanitization: escape triple backticks inside code blocks; strip ANSI control sequences from stdout/stderr to avoid unwanted formatting.\n- Permissions: channel permission overwrite to disallow everyone from posting if input disabled; allow only bot and maintainers role if configured.\n- Logging hygiene: never log tokens or webhook URLs; redact in logs.\n- Pseudo-code: payload.allowed_mentions = AllowedMentions::none(); strip_ansi(s): regex \\x1B\\[[0-9;]*[A-Za-z] -> \"\".",
        "testStrategy": "- Verify watcher container envs do not include bot token (inspect pod spec).\n- Attempt cross-run injection: send message from different channel; ensure bot refuses.\n- Confirm mention suppression: post message with @everyone from agent output; ensure no actual mention in Discord.\n- Webhook revocation: after completion, trying to post should fail (410/404); ensure bot deleted webhook.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Kubernetes Sidecar and Helm Chart Packaging",
        "description": "Provide K8s manifests and Helm charts to deploy the watcher as a sidecar, the bot as a deployment, and the input-bridge in the agent pod. Include values reflecting PRD config and resource limits.",
        "details": "- Helm chart structure charts/discord-monitor with subcharts or templates for:\n  - Bot Deployment + Service + Secret (bot token), ConfigMap for retention/config.\n  - Sidecar template snippet to inject into target workloads (document how to add). Env: DISCORD_WEBHOOK_URL, WORKSPACE_PATH, POLL_INTERVAL, BATCH_SIZE, filters.\n  - Input-bridge Container (in same pod as agent) exposing 127.0.0.1 only; volumeMount for FIFO path if needed.\n- Resource requests/limits per PRD: watcher requests {cpu:10m, memory:32Mi} limits {cpu:100m, memory:64Mi}. Liveness/readiness probes: /healthz.\n- Values.yaml sketch from PRD:\n  discord.monitoring.enabled, polling.interval:100, batchSize:10, filters.includeTools, includePatterns, minStdoutLength, stats.trackCost/tokens/errors, resources.\n- RBAC minimal (bot needs none beyond network).\n- Example sidecar injection snippet for a Deployment:\n  containers:\n    - name: discord-watcher\n      image: discord-monitor:latest\n      env:\n        - name: DISCORD_WEBHOOK_URL\n          valueFrom: secretKeyRef: {...}\n        - name: WORKSPACE_PATH\n          value: /home/agent/.claude/projects/...\n      ports: [{name: health, containerPort: 8080}]\n      resources: {requests: {cpu: \"10m\", memory: \"32Mi\"}, limits: {cpu: \"100m\", memory: \"64Mi\"}}\n- Templating helpers to toggle parity-mode.\n- Documentation in NOTES.txt for how to wire bot create to inject webhook env into pod spec (e.g., via orchestrator).",
        "testStrategy": "- helm template with provided values -> validate manifests with kubeval/kubeconform.\n- kind cluster smoke test: install bot chart; deploy a sample job with sidecar; port-forward if needed; verify watcher healthz.\n- Resource verification: kubectl top shows watcher under limits during synthetic run.\n- Secret scan: ensure no bot token in sidecar env; only webhook.\n- Probe tests: kill webhook endpoint; liveness restarts watcher independently without affecting agent.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Initialize Helm chart skeleton and helper templates",
            "description": "Scaffold charts/discord-monitor with standard Helm structure and reusable helpers for naming, labels, and parity-mode toggles.",
            "dependencies": [],
            "details": "Create charts/discord-monitor with Chart.yaml, .helmignore, templates/, values.yaml (placeholder), and NOTES.txt (placeholder). Add templates/_helpers.tpl with helpers: name/fullname, labels/selectorLabels, image reference helpers, and a boolean helper for parity-mode toggles. Prepare directories or file stubs for: templates/bot/, templates/snippets/_sidecar.tpl, templates/snippets/_input_bridge.tpl, and templates/snippets/_common.tpl. Establish standard app.kubernetes.io labels and annotations across resources.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Bot Deployment, Service, Secret, and ConfigMap templates",
            "description": "Implement bot Deployment, Service, Secret (bot token), and ConfigMap for retention/config with configurable env.",
            "dependencies": [
              "19.1",
              "19.5",
              "19.6"
            ],
            "details": "Add templates/bot/deployment.yaml with env from Secret (BOT_TOKEN) and from ConfigMap for retention/config and behavior (e.g., LOG_LEVEL, STATS_*). Include liveness/readiness probes on /healthz port 8080 using common helpers. Add templates/bot/service.yaml exposing port 8080 for health/metrics. Add templates/bot/secret.yaml to create Opaque secret when .Values.bot.secret.create is true, supporting existingSecret reference. Add templates/bot/configmap.yaml to hold retention and related settings. Wire resources via common helpers and values. No RBAC beyond optional ServiceAccount; set to disabled by default.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Sidecar template snippet for target workloads",
            "description": "Provide a reusable sidecar container snippet with envs, probes, resources, and parity-mode toggles for injection into workloads.",
            "dependencies": [
              "19.1",
              "19.5",
              "19.6"
            ],
            "details": "Create templates/snippets/_sidecar.tpl exporting a named template that renders a container spec for the watcher sidecar. Include env: DISCORD_WEBHOOK_URL (from secret or value), WORKSPACE_PATH, POLL_INTERVAL (from .Values.polling.interval), BATCH_SIZE (.Values.batchSize), FILTERS (JSON), STATS booleans, PARITY_MODE, LOG_LEVEL. Add port named health on 8080 with liveness/readiness to /healthz via common helpers. Apply PRD resources: requests cpu 10m memory 32Mi; limits cpu 100m memory 64Mi. Provide values toggles to enable/disable sidecar and parity-mode. Document usage expectations (webhook secret provided externally).",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Input-bridge container snippet for agent pod",
            "description": "Add a reusable input-bridge container template that binds to 127.0.0.1 and supports FIFO volume mounts.",
            "dependencies": [
              "19.1",
              "19.5",
              "19.6"
            ],
            "details": "Create templates/snippets/_input_bridge.tpl exporting a named template for an additional container to be added to the agent pod. Configure arguments/env to bind only to 127.0.0.1, expose an internal port (configurable), and include liveness/readiness probes if applicable. Support volumeMount for FIFO path (from .Values.inputBridge.fifoPath) and document corresponding volume declaration in the host workload. Apply resources via common helpers and values.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Values.yaml reflecting PRD configuration",
            "description": "Define a comprehensive values.yaml capturing PRD defaults for polling, batching, filters, stats, resources, secrets, and toggles.",
            "dependencies": [
              "19.1"
            ],
            "details": "Populate .Values with: discord.monitoring.enabled; polling.interval: 100; batchSize: 10; filters.includeTools, filters.includePatterns (list), filters.minStdoutLength; stats.trackCost, stats.tokens, stats.errors (booleans); parityMode: false. Define images (repository, tag, pullPolicy) for watcher, bot, inputBridge. Set watcher sidecar resources: requests {cpu: 10m, memory: 32Mi}, limits {cpu: 100m, memory: 64Mi}. Add probes config (paths/port/timeouts) defaulting to /healthz on 8080. Configure bot: service.type, port, resources, config.retention fields. Secrets: bot.secret.create, bot.secret.existingName/key; webhook.existingSecret/name/key. RBAC/serviceAccount toggles default false. Input-bridge: enabled, port, fifoPath.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Common probes/resources helpers and minimal RBAC",
            "description": "Provide shared helpers for /healthz probes and resource rendering, and define minimal/optional RBAC objects.",
            "dependencies": [
              "19.1",
              "19.5"
            ],
            "details": "Add templates/snippets/_common.tpl with named templates to render liveness/readiness probes to /healthz on 8080, reading delays/timeouts from values. Add a resources helper that maps .Values resources into container resources. Provide optional ServiceAccount template controlled by .Values.serviceAccount.create; leave Roles/RoleBindings absent by default per minimal RBAC requirement. Ensure templates gate creation via values toggles and use standard labels/annotations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Documentation and NOTES.txt for webhook integration and injection",
            "description": "Write NOTES.txt and usage docs explaining how to inject the sidecar and wire webhook env/secret into workloads.",
            "dependencies": [
              "19.1",
              "19.2",
              "19.3",
              "19.4",
              "19.5",
              "19.6"
            ],
            "details": "In templates/NOTES.txt, include: helm install instructions; how to create or reference a Secret containing DISCORD_WEBHOOK_URL; example kubectl create secret command; how to inject the sidecar snippet using Helm include (e.g., include \"discord-monitor.sidecar\" . | nindent 8) into an existing Deployment; guidance for orchestrator-driven webhook creation and env injection; example of adding the input-bridge container via include; explanation of parity-mode toggles. Document expected health endpoints and resources. Specify DoD signals: helm template validates, kind install passes health checks, secrets are namespaced and not world-readable.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 8,
            "title": "CI: helm lint/template, kubeconform, and kind smoke test",
            "description": "Set up CI to lint, render, validate, and smoke-test the chart on a kind cluster including a sample workload with sidecar.",
            "dependencies": [
              "19.1",
              "19.2",
              "19.3",
              "19.4",
              "19.5",
              "19.6",
              "19.7"
            ],
            "details": "Add .github/workflows/helm-ci.yml to run: helm lint charts/discord-monitor; helm template with PRD values to a manifest file; validate with kubeconform (or kubeval) strict mode. Provision kind cluster; helm install the chart; deploy a sample Deployment that includes the sidecar snippet; wait for readiness; port-forward and curl /healthz for bot and sidecar; verify resources applied; ensure Secret scoping is correct. Provide hack/smoke.sh to reproduce locally. Define CI artifacts and caching for tool downloads. DoD: helm template produces valid manifests; kind install passes health checks; secrets are correctly scoped.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 20,
        "title": "Documentation, Acceptance Tests, and Performance Validation",
        "description": "Deliver comprehensive docs and perform acceptance and performance testing to satisfy MVP criteria: live streaming to channel, readable errors/stdout, final summary, and no agent slowdown.",
        "details": "- Docs:\n  - architecture.md (high-level architecture, data flow, security model).\n  - deploy.md (bot setup, guild permissions, creating category, Helm install, sidecar injection, envs).\n  - operations.md (retention, webhook revocation, troubleshooting rate limits, logs).\n  - formatting.md (embed templates, truncation rules, diff behavior, parity-mode).\n- Acceptance tests matrix:\n  - Basic visibility: pre-created static webhook; watcher streams tool_use, assistant, errors, final summary.\n  - Bot lifecycle: create channel+webhook per run; inject into pod; final summary + channel removal.\n  - Optional input: /send relayed to bridge; agent receives text via FIFO.\n  - Hardening: rate limit/backoff works; bot outage -> watcher continues via webhook.\n- Performance:\n  - Latency: measure append-to-post time across 1000 events; median ≤100ms.\n  - Overhead: measure CPU (<1%) and RSS (<64Mi) in realistic run.\n  - Reliability: restart watcher mid-run; ensure resume without agent impact.\n- Example scripts:\n  - scripts/gen_transcript.rs to emit synthetic JSONL at varying rates.\n  - scripts/measure_latency.rs using Instant stamps embedded into events and webhook mock timestamps.\n- Pseudo-checklist: All acceptance criteria from PRD ticked and evidenced with logs/screenshots.",
        "testStrategy": "- Execute acceptance test plan against a sandbox Discord guild with a mock agent.\n- Collect metrics/logs demonstrating latency and resource usage below thresholds.\n- Peer review of docs; run new-hire dry-run from zero to streaming in <30 minutes.\n- Record failures/regressions and file issues with repro steps.",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "architecture.md: High-level architecture, data flow, security model",
            "description": "Author architecture.md covering components, end-to-end data flow, and the security model for streaming, errors/stdout, final summary, and resilience.",
            "dependencies": [],
            "details": "- Scope: watcher, Discord bot service, webhooks, orchestrator, agent, storage/metrics, and sandbox guild.\n- Data flow: agent emits events -> watcher formats -> posts to channel via webhook; capture errors/stdout; final summary emission; resume semantics on restart.\n- Sequence diagrams for: static webhook path, per-run channel/webhook lifecycle, optional /send relay via FIFO, and backpressure/rate-limit handling.\n- Security model: Discord bot token handling, webhook secrecy and rotation, least-privilege intents, guild/category scoping, network boundaries, audit/logging, and PII handling.\n- Failure modes: Discord outages, rate limits, webhook deletion, bot unavailability; recovery behavior and idempotency.\n- Assumptions and constraints: Discord limits, embed size, message rate caps, retries with jitter/backoff.\n- Definition of Done: architecture.md merged after peer review; diagrams and flows align with acceptance/perf plans.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "deploy.md: Bot setup, permissions, Helm install, sidecar injection, envs",
            "description": "Create deploy.md describing Discord bot creation, guild permissions, category setup, Helm chart install, sidecar injection, and environment configuration.",
            "dependencies": [
              "20.1"
            ],
            "details": "- Prereqs: Discord application creation, token retrieval, sandbox guild access.\n- Guild setup: minimal intents, required permissions for channel/webhook management; create or detect \"Agent Runs\" category.\n- K8s: secrets for DISCORD_BOT_TOKEN, config maps/values, namespaces.\n- Helm: chart values (retention, category name, resources), install/upgrade/rollback commands.\n- Sidecar injection: how to attach watcher to target pods; env vars for webhook URL, FIFO paths, logging levels.\n- Validation steps: curl POST /run/create, confirm channel + webhook; smoke test event stream; /run/complete teardown.\n- Rollback and disaster recovery notes.\n- Definition of Done: deploy.md lets a new operator deploy to sandbox and verify end-to-end streaming.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "operations.md: Retention, webhook revocation, rate-limit/backoff, troubleshooting",
            "description": "Produce operations.md covering retention policies, webhook revocation/rotation, rate-limit/backoff guidance, observability, and troubleshooting playbooks.",
            "dependencies": [
              "20.2"
            ],
            "details": "- Retention: archival vs delete timelines, channel cleanup, webhook deletion on run complete; manual overrides.\n- Webhook lifecycle: rotate/revoke procedures, incident response if URL leaked.\n- Rate limits: Discord headers, local backoff with jitter, safe retry policies, queuing behavior; how to tune.\n- Monitoring: logs, metrics, alerts (latency, error rates, rate-limit hits, CPU/RSS), dashboards.\n- Troubleshooting: common failures (permission errors, missing category, hitting embed limits, bot outage), step-by-step checks and remediation.\n- Runbooks for bot outage fallback (continue via static webhook), and for restarting watcher mid-run.\n- Definition of Done: operations.md peer-reviewed; runbooks validated in sandbox.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "formatting.md: Embed templates, truncation, diff behavior, parity-mode",
            "description": "Write formatting.md detailing message/embeds templates, truncation rules, diff presentation, and parity-mode for console equivalence.",
            "dependencies": [
              "20.1"
            ],
            "details": "- Templates for: tool_use, assistant messages, errors/stdout, final summary; fields, color coding, footers, timestamps.\n- Limits: Discord message/embed constraints; truncation and overflow policy; attachment fallback when needed.\n- Diff behavior: how updates/edits are rendered; rules for collapsing/expanding; idempotent updates.\n- Parity-mode: toggles to mirror console logs; examples for each event type.\n- Examples: copy-pastable payloads for integration tests; expected render screenshots references.\n- Definition of Done: formatting.md approved; examples validated by test scripts.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Acceptance tests: matrix and scripts (static webhook, lifecycle, optional input, hardening)",
            "description": "Implement acceptance test matrix and scripts to validate visibility, bot lifecycle, optional /send input relay, and hardening scenarios.",
            "dependencies": [
              "20.2",
              "20.4"
            ],
            "details": "- Matrix definition with pass criteria:\n  - Basic visibility: pre-created static webhook; watcher streams tool_use, assistant, errors, final summary.\n  - Bot lifecycle: per-run channel+webhook creation; injection into pod; final summary; channel removal.\n  - Optional input: /send relayed to bridge; agent consumes text via FIFO.\n  - Hardening: rate-limit/backoff effectiveness; bot outage -> watcher continues via webhook.\n- Implement driver scripts to orchestrate runs, capture logs, and collect screenshots.\n- Mock agent or generator to emit representative events.\n- Artifacts: logs, screenshots, and a summary report mapping to PRD acceptance criteria.\n- Definition of Done: all acceptance tests pass with recorded evidence.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Performance harness and synthetic generators; run and capture metrics",
            "description": "Build and run performance harness with gen_transcript and latency measurement to meet thresholds: median ≤100ms, CPU <1%, RSS <64Mi; validate restart resilience.",
            "dependencies": [
              "20.2",
              "20.5"
            ],
            "details": "- Implement scripts/gen_transcript.rs to emit synthetic JSONL at configurable rates and sizes.\n- Implement scripts/measure_latency.rs using embedded Instant timestamps and webhook mock timestamps; compute median and distribution across 1000 events.\n- Load tests at realistic and peak rates; capture p50/p95 latency and error rates.\n- Resource measurement: CPU (<1%) and RSS (<64Mi) for watcher in a realistic run; document tooling (e.g., cgroup metrics, pidstat, kubectl top).\n- Reliability: restart watcher mid-run; verify resume without agent impact or slowdown.\n- Produce machine-readable metrics and a brief narrative summary.\n- Definition of Done: thresholds met with reproducible logs/metrics artifacts.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Evidence packaging and new-hire dry-run checklist",
            "description": "Assemble evidence (logs, screenshots, metrics) and create a new-hire dry-run checklist to go from zero to streaming in under 30 minutes.",
            "dependencies": [
              "20.1",
              "20.2",
              "20.3",
              "20.4",
              "20.5",
              "20.6"
            ],
            "details": "- Package artifacts: acceptance logs/screenshots, performance metrics, and reliability run outputs; index with context.\n- Create an evidence summary mapping each PRD acceptance and performance criterion to proof.\n- New-hire checklist: prerequisites, deploy steps, run acceptance basics, verify performance sampling; target completion <30 minutes.\n- Add links across docs (architecture/deploy/operations/formatting) and scripts.\n- Capture peer review sign-offs and any follow-up issues.\n- Definition of Done: evidence bundle published; checklist validated by a dry-run; all criteria satisfied.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-08-11T05:40:40.719Z",
      "updated": "2025-08-11T06:37:48.028Z",
      "description": "Tasks for master context"
    }
  }
}