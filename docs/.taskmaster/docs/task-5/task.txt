# Task ID: 5
# Title: Implementation DAG WorkflowTemplate (Rex → Clippy → QA → Deploy → Acceptance)
# Status: pending
# Dependencies: None
# Priority: medium
# Description: Create an Argo DAG for task/issue implementation including deployment and acceptance verification as per FR3.
# Details:
Implementation:
- WorkflowTemplate implementation with tasks:
  - implement (Rex): templateRef coderun-template; github-app=rex
  - clippy-format: depends implement; github-app=clippy
  - qa-testing: depends clippy-format; github-app=qa
  - deploy: depends qa-testing; template: deploy (DocsRun or container step)
  - acceptance: depends deploy; template: acceptance (DocsRun/container step)
- deploy template:
  - Uses DocsRun CR to execute deployment script (e.g., helm upgrade --install to a preview namespace) using existing infra/Argo CD patterns; avoid secret duplication; namespace per PR/task as available.
- acceptance template:
  - Runs black-box checks against the deployed service endpoints; captures logs/responses to artifacts; enforces success thresholds.
- Ensure rollback/cleanup on failure (best-effort): delete preview releases/namespace.
- Emphasize no auto-merge; success results in QA approval only.
- Output parameters include deploy URLs and evidence artifact paths.
Pseudocode (deploy via DocsRun resource template):
apiVersion: taskmaster.io/v1
kind: DocsRun
metadata:
  generateName: docsrun-deploy-
spec:
  command: ["/bin/sh","-lc","helm upgrade --install ${RELEASE} charts/app -n ${NS} --values values-preview.yaml"]

# Test Strategy:
Trigger via issue-to-task flow; confirm Rex commits code; clippy runs and cleans; QA generates tests; deploy creates a preview environment in the expected namespace; acceptance verifies endpoints and gathers proof artifacts. Failure cases: acceptance fails on 5xx and blocks approval. Ensure resources are cleaned on failure.

# Subtasks:
## 1. Define Argo WorkflowTemplate DAG (Rex → Clippy → QA → Deploy → Acceptance) [pending]
### Dependencies: None
### Description: Create the top-level WorkflowTemplate with a DAG entrypoint encoding the task order and high-level parameters.
### Details:
- Create WorkflowTemplate name: implementation-dag (or fr3-impl-dag) with entrypoint dag.
- Global parameters (with sane defaults): repo, owner, pr, commit, domain, chartPath, valuesFile, imageTag, env.
- DAG tasks in order: implement, clippy-format, qa-testing, deploy, acceptance. Use templateRef for coderun-template in first three; custom templates for deploy and acceptance.
- Wire task dependencies explicitly: clippy-format depends implement; qa-testing depends clippy-format; deploy depends qa-testing; acceptance depends deploy.
- Example skeleton (excerpt):
  apiVersion: argoproj.io/v1alpha1
  kind: WorkflowTemplate
  metadata:
    name: implementation-dag
  spec:
    entrypoint: dag
    arguments:
      parameters:
        - {name: owner}
        - {name: repo}
        - {name: pr}
        - {name: commit, value: ""}
        - {name: domain, value: preview.example.com}
        - {name: chartPath, value: charts/app}
        - {name: valuesFile, value: values-preview.yaml}
        - {name: imageTag, value: "pr-{{workflow.parameters.pr}}"}
    templates:
      - name: dag
        dag:
          tasks:
            - name: implement
              templateRef: {name: coderun-template, template: coderun-main}
              arguments: {parameters: [{name: github-app, value: rex}]}
            - name: clippy-format
              dependencies: [implement]
              templateRef: {name: coderun-template, template: coderun-main}
              arguments: {parameters: [{name: github-app, value: clippy}]}
            - name: qa-testing
              dependencies: [clippy-format]
              templateRef: {name: coderun-template, template: coderun-main}
              arguments: {parameters: [{name: github-app, value: qa}]}
            - name: deploy
              dependencies: [qa-testing]
              template: deploy
            - name: acceptance
              dependencies: [deploy]
              template: acceptance
- Add onExit template placeholder for cleanup; implemented in a later subtask.
- Acceptance criteria: WorkflowTemplate validates with kubectl; argo list templates shows implementation-dag; dry-run produces DAG with the exact dependencies.

## 2. Coderun invocations for Rex, Clippy, and QA [pending]
### Dependencies: 5.1
### Description: Invoke coderun-template for implement, clippy-format, and qa-testing with correct github-app parameterization and artifact conventions.
### Details:
- For each of the three tasks, pass parameters: owner, repo, pr, commit (if applicable), github-app in {rex, clippy, qa}.
- Mount GitHub App token via the token generator sidecar (Task 2), expose token at /var/run/github/token.
- Set consistent working directory (/workspace) and artifact root (/artifacts/{task}).
- Example task argument block:
  arguments:
    parameters:
      - {name: github-app, value: rex}
      - {name: owner, value: {{workflow.parameters.owner}}}
      - {name: repo, value: {{workflow.parameters.repo}}}
      - {name: pr, value: {{workflow.parameters.pr}}}
- Expected outputs/artifacts per task:
  - implement: /artifacts/rex/commit.txt, /artifacts/rex/logs.txt
  - clippy-format: /artifacts/clippy/report.json, /artifacts/clippy/logs.txt
  - qa-testing: /artifacts/qa/proof/index.json, /artifacts/qa/logs.txt
- Enforce that coderun steps never attempt merges; they may push commits to the PR.
- Acceptance criteria: On a test PR, Rex produces commits; Clippy runs and formats; QA produces proof artifacts in the specified paths. All three tasks succeed in sequence.

## 3. Deploy template using DocsRun to helm upgrade/install to preview namespace [pending]
### Dependencies: 5.2, 5.5
### Description: Create deploy template leveraging DocsRun CR to install/upgrade the app in a per-PR namespace following existing Argo CD/infra patterns.
### Details:
- Implement Argo template: deploy (type: resource) to create a DocsRun CR.
- Parameters consumed: NS, RELEASE, chartPath, valuesFile, imageTag, owner, repo, pr.
- Pre-deploy: ensure namespace exists and label it; avoid duplicating secrets by referencing shared ExternalSecrets/Secrets.
- Helm command example: helm upgrade --install ${RELEASE} ${CHART_PATH} -n ${NS} -f ${VALUES_FILE} --set image.tag=${IMAGE_TAG}
- Example template:
  - name: deploy
    inputs:
      parameters:
        - {name: NS}
        - {name: RELEASE}
        - {name: chartPath, value: {{workflow.parameters.chartPath}}}
        - {name: valuesFile, value: {{workflow.parameters.valuesFile}}}
        - {name: imageTag, value: {{workflow.parameters.imageTag}}}
    resource:
      action: create
      manifest: |
        apiVersion: taskmaster.io/v1
        kind: DocsRun
        metadata:
          generateName: docsrun-deploy-
        spec:
          command: ["/bin/sh","-lc","set -euo pipefail; kubectl get ns ${NS} >/dev/null 2>&1 || kubectl create ns ${NS}; kubectl label ns ${NS} preview=true pr=${PR} --overwrite; helm upgrade --install ${RELEASE} ${CHART_PATH} -n ${NS} -f ${VALUES_FILE} --set image.tag=${IMAGE_TAG}; kubectl get ing -n ${NS} -o json > /tmp/ing.json; jq -r '[.items[].spec.rules[].host] | {hosts: ., ns: "'${NS}'", release: "'${RELEASE}'"}' /tmp/ing.json > /tmp/urls.json" ]
          env:
            - {name: NS, value: "{{inputs.parameters.NS}}"}
            - {name: RELEASE, value: "{{inputs.parameters.RELEASE}}"}
            - {name: CHART_PATH, value: "{{inputs.parameters.chartPath}}"}
            - {name: VALUES_FILE, value: "{{inputs.parameters.valuesFile}}"}
            - {name: IMAGE_TAG, value: "{{inputs.parameters.imageTag}}"}
            - {name: PR, value: "{{workflow.parameters.pr}}"}
    outputs:
      artifacts:
        - name: deploy-urls
          path: /tmp/urls.json
- Output: urls.json containing hosts, ns, release.
- Acceptance criteria: Helm install succeeds in a new or existing preview namespace; docsrun outputs urls.json artifact; no new secrets created for GitHub Apps.

## 4. Acceptance template with black-box checks and artifact collection [pending]
### Dependencies: 5.3, 5.5
### Description: Implement acceptance template to probe service endpoints, enforce success thresholds, and capture artifacts (responses/logs).
### Details:
- Template type: container or DocsRun; use container for portability.
- Inputs: urls.json from deploy, thresholds (min_success_pct, max_p95_ms), NS/RELEASE for context.
- Behavior:
  - Parse urls.json to get hosts; for each host, perform N HTTP GETs with timeouts.
  - Consider a request successful if status < 500 and response within timeout.
  - Collect artifacts: acceptance/report.json (summary and per-endpoint results), acceptance/responses/*.txt, acceptance/logs.txt.
  - Fail step if success ratio below threshold or any 5xx encountered if policy requires.
- Example template:
  - name: acceptance
    inputs:
      artifacts:
        - name: deploy-urls
          path: /input/urls.json
      parameters:
        - {name: min_success_pct, value: "0.95"}
        - {name: max_p95_ms, value: "1200"}
    container:
      image: curlimages/curl:8.8.0
      command: ["/bin/sh","-lc"]
      args:
        - |
          set -euo pipefail
          mkdir -p /artifacts/acceptance/responses
          HOSTS=$(jq -r '.hosts[]' /input/urls.json)
          TOTAL=0 OK=0; TIMES=""
          for H in $HOSTS; do
            for i in $(seq 1 10); do
              START=$(date +%s%3N)
              CODE=$(curl -ksS -o /artifacts/acceptance/responses/${H//./_}-$i.txt -w "%{http_code}" https://$H/ || true)
              END=$(date +%s%3N); LAT=$((END-START))
              TOTAL=$((TOTAL+1))
              if [ "$CODE" -lt 500 ] && [ "$CODE" -ge 200 ]; then OK=$((OK+1)); TIMES="$TIMES $LAT"; fi
              echo "$H $i $CODE $LAT" >> /artifacts/acceptance/logs.txt
            done
          done
          P95=$(printf "%s\n" $TIMES | awk '{ print $1 }' | sort -n | awk 'BEGIN{c=0} {a[c++]=$1} END{ if (c==0) {print 999999} else { print a[int(0.95*c)] } }')
          SUCCESS=$(awk -v o=$OK -v t=$TOTAL 'BEGIN{ if(t==0){print 0}else{print o/t} }')
          jq -n --argjson success "$SUCCESS" --argjson p95 "$P95" --arg ns "{{tasks.deploy.outputs.parameters.NS}}" '{ns:$ns, success_pct:$success, p95_ms:$p95}' > /artifacts/acceptance/report.json
          awk -v s=$SUCCESS -v thr={{inputs.parameters.min_success_pct}} -v p=$P95 -v pthr={{inputs.parameters.max_p95_ms}} 'BEGIN{ if(s+0<thr+0 || p+0>pthr+0) exit 2 }'
    outputs:
      artifacts:
        - {name: acceptance-report, path: /artifacts/acceptance/report.json}
        - {name: acceptance-logs, path: /artifacts/acceptance/logs.txt}
- Acceptance criteria: Step fails if thresholds violated; artifacts present; report captures per-run metrics and overall success.

## 5. Environment discovery and parameterization (namespace and release naming) [pending]
### Dependencies: 5.1
### Description: Implement a setup template that computes namespace, release, and related parameters and exposes them to dependent tasks.
### Details:
- Create a script template set-params that determines:
  - NS = impl-pr-${pr}
  - RELEASE = ${repo}-pr-${pr}
  - DOMAIN derived from workflow.parameters.domain
  - IMAGE_TAG from workflow.parameters.imageTag (default pr-<pr>)
- Expose outputs as parameters for use by deploy and acceptance.
- Example template:
  - name: set-params
    script:
      image: alpine:3.20
      command: ["/bin/sh","-lc"]
      source: |
        set -euo pipefail
        NS="impl-pr-{{workflow.parameters.pr}}"
        RELEASE="{{workflow.parameters.repo}}-pr-{{workflow.parameters.pr}}"
        echo -n $NS > /tmp/ns
        echo -n $RELEASE > /tmp/release
    outputs:
      parameters:
        - name: NS
          valueFrom: {path: /tmp/ns}
        - name: RELEASE
          valueFrom: {path: /tmp/release}
- Wire DAG: add a dag task set-params at the beginning; make deploy and acceptance consume its outputs via withParam references.
- Acceptance criteria: NS and RELEASE match naming scheme; values are accessible in templates 3 and 4 via inputs.parameters.

## 6. Rollback/cleanup on failure (best-effort) [pending]
### Dependencies: 5.3, 5.4, 5.5
### Description: Implement onExit cleanup to uninstall preview release and delete namespace if the workflow fails, without impacting successful runs.
### Details:
- Add onExit: cleanup template that checks {{workflow.status}}. If Failed/Error, attempt helm uninstall and ns deletion. If Succeeded, keep for manual review with TTL.
- Prefer DocsRun CR for cleanup or a lightweight kubectl/helm container.
- Strategy:
  - Labels: all resources labeled preview=true pr=<pr> so they can be selected.
  - Commands:
    helm uninstall ${RELEASE} -n ${NS} || true
    kubectl delete ns ${NS} --ignore-not-found=true || true
  - Optional: set TTL on the namespace via annotation for cluster janitor.
- Example onExit template:
  - name: cleanup
    inputs:
      parameters:
        - {name: NS}
        - {name: RELEASE}
    script:
      image: alpine/helm:3.14.4
      command: ["/bin/sh","-lc"]
      source: |
        set -euo pipefail
        if [ "{{workflow.status}}" != "Succeeded" ]; then
          echo "Cleaning up preview ${RELEASE} in ${NS}"
          helm uninstall "{{inputs.parameters.RELEASE}}" -n "{{inputs.parameters.NS}}" || true
          kubectl delete ns "{{inputs.parameters.NS}}" --ignore-not-found=true || true
        else
          echo "Workflow succeeded; leaving preview env for QA approval only."
        fi
- Wire onExit at WorkflowTemplate spec level and pass NS/RELEASE from set-params outputs.
- Acceptance criteria: Simulate a failing acceptance; namespace/release are removed; simulate success; environment remains.

## 7. Outputs and evidence schema (deploy URLs and artifact paths) [pending]
### Dependencies: 5.3, 5.4
### Description: Define workflow-level outputs and standardized artifact schema for downstream consumers and FR3 evidence.
### Details:
- Define outputs.parameters in the main DAG wrapper to surface:
  - preview_urls (JSON from deploy-urls)
  - acceptance_report_path (artifact URI)
  - acceptance_logs_path (artifact URI)
- Artifact schema:
  - deploy-urls: {hosts: ["host1","host2"], ns: "impl-pr-123", release: "repo-pr-123"}
  - acceptance/report.json: {ns, success_pct (0..1), p95_ms (int), timestamp, endpoints: [{host, samples, successes, failures, p95_ms}]}
- Example outputs wiring in Workflow spec:
  outputs:
    parameters:
      - name: preview_urls
        valueFrom: {artifact: {name: deploy-urls, from: "{{tasks.deploy.outputs.artifacts.deploy-urls}}"}}
      - name: acceptance_report_path
        value: "{{workflow.outputs.artifacts.acceptance-report.url}}"
      - name: acceptance_logs_path
        value: "{{workflow.outputs.artifacts.acceptance-logs.url}}"
- Ensure no auto-merge behavior; these outputs are used for QA approval only (per FR3).
- Acceptance criteria: After a successful run, parameters are populated; artifacts are browseable in the artifact repository; JSON validates against the documented schema.

## 8. Security: secret reuse and access controls (no duplication) [pending]
### Dependencies: 5.1, 5.2, 5.3
### Description: Ensure the workflow reuses ExternalSecrets and token generator (Task 2), and enforce least-privilege RBAC for DocsRun and Kubernetes access.
### Details:
- Secret reuse:
  - Mount GitHub App secrets via ExternalSecret-provisioned Kubernetes Secrets; no duplication per namespace.
  - Token generator sidecar mounts APP_ID/PRIVATE_KEY and exposes /var/run/github/token (read-only) to main containers.
- RBAC:
  - ServiceAccount with permissions to create DocsRun CRs, get/list namespaces, get/list ingresses, install Helm releases in target NS.
  - ClusterRole example rules: [docsrun.taskmaster.io/* verbs create,get], ["","extensions","apps"] resources: namespaces, deployments, ingresses with get/list.
  - Bind SA to Argo WorkflowTemplate via spec.serviceAccountName.
- Pod Security:
  - Run as non-root, readOnlyRootFilesystem where possible, drop ALL capabilities, add NET_BIND_SERVICE only if required (likely not needed).
  - Do not write secrets to logs or artifacts; redact tokens.
- Example mounts (coderun steps):
  volumeMounts:
    - {name: gh-token, mountPath: /var/run/github, readOnly: true}
  volumes:
    - name: gh-token
      projected:
        sources:
          - secret: {name: github-app-$(GITHUB_APP), items: [{key: token, path: token}]}
- Acceptance criteria: Workflow runs without creating new GitHub App secrets; SA cannot list secrets cluster-wide; security scan finds no tokens in artifacts.

## 9. E2E tests and documentation [pending]
### Dependencies: 5.1, 5.2, 5.3, 5.4, 5.5, 5.6, 5.7, 5.8
### Description: Add end-to-end tests demonstrating the full DAG and produce concise documentation for operators and contributors.
### Details:
- Test plan:
  - Happy path: Open a PR; trigger workflow; verify order Rex→Clippy→QA→Deploy→Acceptance; confirm preview URLs; acceptance passes; no auto-merge occurs; environment remains for manual QA approval per policy.
  - Failure path: Force acceptance failure (return 5xx from app or set strict thresholds); check that cleanup removed Helm release and namespace.
  - Security: Rotate ExternalSecrets; ensure workflow picks new tokens; verify no secrets in logs.
- Commands:
  - argo submit --from workflowtemplate/implementation-dag -p owner=... -p repo=... -p pr=123
  - kubectl get ns impl-pr-123; kubectl get ing -n impl-pr-123
- Documentation (README.md in ops/argo/implementation-dag):
  - Overview, parameters, required prerequisites (Task 2 secrets, ArgoCD/helm), how to trigger, artifacts and outputs, cleanup behavior, acceptance criteria, troubleshooting.
- Acceptance criteria: All tests pass in CI or a staging cluster; documentation reviewed and usable by another engineer to run the flow end-to-end.

