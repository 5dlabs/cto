---
# KubeAI - AI Inference Operator for Kubernetes
# Supports LLMs, VLMs, embeddings, speech-to-text via vLLM, Ollama, FasterWhisper
# License: Apache 2.0 (safe for proprietary distribution)
# Source: https://github.com/kubeai-project/kubeai
# Docs: https://www.kubeai.org

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kubeai
  namespace: argocd
  labels:
    app.kubernetes.io/name: kubeai
    app.kubernetes.io/part-of: platform
    app.kubernetes.io/component: ai-ml
  annotations:
    # Deploy after GPU operator if present
    argocd.argoproj.io/sync-wave: "1"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: platform

  source:
    repoURL: https://www.kubeai.org
    chart: kubeai
    targetRevision: 0.23.1
    helm:
      values: |
        # Resource configuration for the KubeAI controller
        resources:
          requests:
            cpu: 100m
            memory: 256Mi
          limits:
            cpu: 500m
            memory: 512Mi

        # Pod labels for log collection
        podLabels:
          platform.5dlabs.io/log-collection: enabled

        # Pod annotations for log collection
        podAnnotations:
          logs.platform.5dlabs.io/collect: "true"
          logs.platform.5dlabs.io/service: kubeai

        # Service configuration - OpenAI-compatible API
        service:
          type: ClusterIP
          port: 80

        # Resource profiles for different GPU types
        # These define node selectors, resource requests/limits for model pods
        resourceProfiles:
          # CPU-only profile
          cpu:
            requests:
              cpu: "1"
              memory: "4Gi"
            limits:
              cpu: "4"
              memory: "8Gi"

          # NVIDIA L4 GPU (24GB VRAM) - Cost-effective inference
          nvidia-gpu-l4:
            nodeSelector:
              # Adjust based on your cluster's GPU node labels
              nvidia.com/gpu.product: "NVIDIA-L4"
            requests:
              cpu: "4"
              memory: "16Gi"
            limits:
              nvidia.com/gpu: "1"
              cpu: "8"
              memory: "32Gi"

          # NVIDIA H100 GPU (80GB VRAM) - High performance
          nvidia-gpu-h100:
            nodeSelector:
              nvidia.com/gpu.product: "NVIDIA-H100-80GB-HBM3"
            requests:
              cpu: "8"
              memory: "64Gi"
            limits:
              nvidia.com/gpu: "1"
              cpu: "16"
              memory: "128Gi"

          # NVIDIA A100 80GB - Training and large model inference
          nvidia-gpu-a100-80gb:
            nodeSelector:
              nvidia.com/gpu.product: "NVIDIA-A100-SXM4-80GB"
            requests:
              cpu: "8"
              memory: "64Gi"
            limits:
              nvidia.com/gpu: "1"
              cpu: "16"
              memory: "128Gi"

          # NVIDIA GH200 Grace Hopper (96GB unified memory)
          nvidia-gpu-gh200:
            nodeSelector:
              nvidia.com/gpu.product: "NVIDIA-GH200-120GB"
            requests:
              cpu: "16"
              memory: "128Gi"
            limits:
              nvidia.com/gpu: "1"
              cpu: "72"
              memory: "480Gi"

        # Models are deployed separately via ai-models/kubeai-models.yaml
        # See that file for the full model catalog

  destination:
    server: https://kubernetes.default.svc
    namespace: kubeai

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false

    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
      - PrunePropagationPolicy=foreground
      - RespectIgnoreDifferences=true

    retry:
      limit: 5
      backoff:
        duration: 10s
        factor: 2
        maxDuration: 3m

  revisionHistoryLimit: 5
