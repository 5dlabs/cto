---
# LlamaStack vLLM Distribution
# High-performance GPU inference using vLLM backend
# Requires HuggingFace token secret for model downloads
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-vllm
  labels:
    app.kubernetes.io/name: llamastack-vllm
    app.kubernetes.io/component: ai-stack
spec:
  replicas: 1
  server:
    distribution:
      name: vllm
    containerSpec:
      port: 8321
      env:
        - name: VLLM_MODEL
          value: "meta-llama/Llama-3.2-3B-Instruct"
        # Enable GPU support
        - name: VLLM_TARGET_DEVICE
          value: "gpu"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"
      resources:
        requests:
          memory: "16Gi"
          cpu: "4"
          nvidia.com/gpu: "1"
        limits:
          memory: "32Gi"
          cpu: "8"
          nvidia.com/gpu: "1"
    storage:
      size: "50Gi"
      mountPath: "/home/lls/.lls"
  # Requires secret with HuggingFace token
  # kubectl create secret generic hf-token-secret \
  #   --from-literal=token=<your-hf-token> -n llama-stack
