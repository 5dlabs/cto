---
# LlamaStack Ollama Distribution
# Uses Ollama as the inference backend
# Good for CPU and mixed workloads
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-ollama
  labels:
    app.kubernetes.io/name: llamastack-ollama
    app.kubernetes.io/component: ai-stack
spec:
  replicas: 1
  server:
    distribution:
      name: ollama
    containerSpec:
      port: 8321
      env:
        - name: OLLAMA_INFERENCE_MODEL
          value: "llama3.2:3b"
        # Can use internal Ollama or external
        - name: OLLAMA_URL
          value: "http://ollama-model-llama3.2-3b.ollama-operator-system.svc.cluster.local:11434"
      resources:
        requests:
          memory: "4Gi"
          cpu: "2"
        limits:
          memory: "8Gi"
          cpu: "4"
    storage:
      size: "30Gi"
      mountPath: "/home/lls/.lls"
