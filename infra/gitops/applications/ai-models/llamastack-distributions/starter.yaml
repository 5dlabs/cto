---
# LlamaStack Starter Distribution
# Quick-start distribution for development and testing
# Uses Ollama as the inference backend
apiVersion: llamastack.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastack-starter
  labels:
    app.kubernetes.io/name: llamastack-starter
    app.kubernetes.io/component: ai-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    containerSpec:
      port: 8321
      env:
        - name: OLLAMA_INFERENCE_MODEL
          value: "llama3.2:3b"
        # Point to Ollama Operator's model service
        - name: OLLAMA_URL
          value: "http://ollama-model-llama3.2-3b.ollama-operator-system.svc.cluster.local:11434"
      resources:
        requests:
          memory: "2Gi"
          cpu: "1"
        limits:
          memory: "4Gi"
          cpu: "2"
    storage:
      size: "20Gi"
      mountPath: "/home/lls/.lls"
