---
# KubeAI Models - Pre-configured AI models for inference
# Deploys via the kubeai/models Helm chart with optimized configurations
# License: Apache 2.0 (KubeAI and model serving infrastructure)
# Note: Individual model weights have their own licenses (Meta, Mistral, etc.)
#
# Supported GPU profiles:
#   - nvidia-gpu-l4:N     - NVIDIA L4 GPUs (cost-effective inference)
#   - nvidia-gpu-h100:N   - NVIDIA H100 GPUs (high performance)
#   - nvidia-gpu-a100-80gb:N - NVIDIA A100 80GB GPUs
#   - nvidia-gpu-gh200:1  - NVIDIA GH200 Grace Hopper
#   - cpu:N               - CPU-only (smaller models)
#
# Usage:
#   Models expose OpenAI-compatible API at http://kubeai/openai/v1
#   Scale from zero by default - set minReplicas: 1 for always-on

apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: kubeai-models
  namespace: argocd
  labels:
    app.kubernetes.io/name: kubeai-models
    app.kubernetes.io/part-of: platform
    app.kubernetes.io/component: ai-ml
  annotations:
    # Deploy after KubeAI operator is ready
    argocd.argoproj.io/sync-wave: "2"
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: platform

  source:
    repoURL: https://www.kubeai.org
    chart: models
    targetRevision: 0.23.1
    helm:
      values: |
        # =============================================================
        # MODEL CATALOG
        # =============================================================
        # Enable models by setting enabled: true
        # Models scale from zero by default (set minReplicas: 1 for always-on)

        catalog:
          # ---------------------------------------------------------
          # LLAMA MODELS (Meta License - free for commercial use)
          # ---------------------------------------------------------

          # Llama 3.1 8B - Great for general tasks on single L4 GPU
          llama-3.1-8b-instruct-fp8-l4:
            enabled: true
            features: [TextGeneration]
            url: "hf://neuralmagic/Meta-Llama-3.1-8B-Instruct-FP8"
            engine: VLLM
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0
            maxReplicas: 3
            targetRequests: 32
            args:
              - --max-model-len=16384
              - --max-num-batched-token=16384
              - --gpu-memory-utilization=0.9
              - --disable-log-requests

          # Llama 3.1 70B - Powerful model for complex tasks (H100)
          llama-3.1-70b-instruct-fp8-h100:
            enabled: false  # Enable when H100 GPUs available
            features: [TextGeneration]
            url: hf://neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
            engine: VLLM
            resourceProfile: nvidia-gpu-h100:2
            minReplicas: 0
            maxReplicas: 2
            targetRequests: 500
            args:
              - --max-model-len=65536
              - --max-num-batched-token=65536
              - --max-num-seqs=1024
              - --gpu-memory-utilization=0.9
              - --tensor-parallel-size=2
              - --enable-prefix-caching
              - --disable-log-requests

          # Llama 3.1 70B - Single H100 variant (smaller context)
          llama-3.1-70b-instruct-fp8-1-h100:
            enabled: false
            features: [TextGeneration]
            url: hf://neuralmagic/Meta-Llama-3.1-70B-Instruct-FP8
            engine: VLLM
            resourceProfile: nvidia-gpu-h100:1
            minReplicas: 0
            args:
              - --enable-prefix-caching
              - --max-model-len=16384
              - --max-num-batched-token=16384
              - --gpu-memory-utilization=0.95
              - --disable-log-requests
              - --kv-cache-dtype=fp8

          # Llama 3.3 70B via Ollama - Easier setup
          llama-3.3-70b-ollama-l4:
            enabled: false
            features: [TextGeneration]
            url: 'ollama://llama3.3:70b'
            engine: OLlama
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0

          # ---------------------------------------------------------
          # DEEPSEEK R1 MODELS (Reasoning-focused)
          # ---------------------------------------------------------

          # DeepSeek R1 1.5B - Small reasoning model (CPU)
          deepseek-r1-1.5b-cpu:
            enabled: true
            features: [TextGeneration]
            url: 'ollama://deepseek-r1:1.5b'
            engine: OLlama
            resourceProfile: cpu:1
            minReplicas: 1
            maxReplicas: 2

          # DeepSeek R1 Distill 8B - Good reasoning on L4
          deepseek-r1-distill-llama-8b-l4:
            enabled: true
            features: [TextGeneration]
            url: "hf://deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
            engine: VLLM
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0
            env:
              VLLM_ATTENTION_BACKEND: "FLASHINFER"
            args:
              - --max-model-len=8192
              - --max-num-batched-token=8192
              - --max-num-seqs=256
              - --gpu-memory-utilization=0.95
              - --kv-cache-dtype=fp8
              - --disable-log-requests
              - --quantization=fp8
              - --enforce-eager

          # DeepSeek R1 70B - Large reasoning model (GH200)
          deepseek-r1-70b-gh200-fp8:
            enabled: false
            features: [TextGeneration]
            url: hf://neuralmagic/DeepSeek-R1-Distill-Llama-70B-FP8-dynamic
            engine: VLLM
            resourceProfile: nvidia-gpu-gh200:1
            minReplicas: 0
            args:
              - --max-model-len=32768
              - --max-num-batched-token=32768
              - --gpu-memory-utilization=0.95
              - --kv-cache-dtype=fp8
              - --enable-prefix-caching
              - --disable-log-requests

          # ---------------------------------------------------------
          # MISTRAL MODELS
          # ---------------------------------------------------------

          # Mistral Small 24B - Excellent performance/cost ratio
          mistral-small-24b-instruct-h100:
            enabled: false
            features: [TextGeneration]
            url: hf://mistralai/Mistral-Small-24B-Instruct-2501
            engine: VLLM
            resourceProfile: nvidia-gpu-h100:1
            minReplicas: 0
            env:
              VLLM_ATTENTION_BACKEND: FLASHINFER
            args:
              - --kv-cache-dtype=fp8
              - --max-num-batched-token=65536
              - --gpu-memory-utilization=0.9
              - --enable-prefix-caching
              - --disable-log-requests

          # ---------------------------------------------------------
          # QWEN MODELS (Alibaba - Apache 2.0 License)
          # ---------------------------------------------------------

          # Qwen 2.5 7B - Good multilingual model
          qwen2.5-7b-instruct-l4:
            enabled: true
            features: [TextGeneration]
            url: "hf://Qwen/Qwen2.5-7B-Instruct"
            engine: VLLM
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0
            env:
              VLLM_ATTENTION_BACKEND: "FLASHINFER"
            args:
              - --max-model-len=8192
              - --max-num-batched-token=8192
              - --max-num-seqs=256
              - --gpu-memory-utilization=0.95
              - --kv-cache-dtype=fp8
              - --enable-prefix-caching

          # Qwen 2.5 Coder 1.5B - Code-focused, CPU friendly
          qwen2.5-coder-1.5b-cpu:
            enabled: true
            features: [TextGeneration]
            url: "ollama://qwen2.5-coder:1.5b"
            engine: OLlama
            resourceProfile: cpu:1
            minReplicas: 0

          # ---------------------------------------------------------
          # GEMMA MODELS (Google - Permissive License)
          # ---------------------------------------------------------

          # Gemma 2 9B - High quality, efficient
          gemma-2-9b-it-fp8-l4:
            enabled: false
            features: [TextGeneration]
            url: "hf://neuralmagic/gemma-2-9b-it-FP8"
            engine: VLLM
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0
            env:
              VLLM_USE_V1: "1"
            args:
              - --max-model-len=4096
              - --max-num-batched-token=4096
              - --max-num-seqs=256
              - --gpu-memory-utilization=0.95
              - --kv-cache-dtype=fp8

          # ---------------------------------------------------------
          # EMBEDDING MODELS
          # ---------------------------------------------------------

          # Nomic Embed - Popular text embeddings (CPU)
          nomic-embed-text-cpu:
            enabled: true
            features: [TextEmbedding]
            url: "ollama://nomic-embed-text"
            engine: OLlama
            resourceProfile: cpu:1
            minReplicas: 1
            maxReplicas: 3

          # BGE Small - Fast embeddings (CPU)
          bge-embed-text-cpu:
            enabled: false
            features: [TextEmbedding]
            url: "hf://BAAI/bge-small-en-v1.5"
            engine: Infinity
            resourceProfile: cpu:1
            minReplicas: 0

          # ---------------------------------------------------------
          # RERANKING MODELS
          # ---------------------------------------------------------

          # BGE Reranker - For RAG pipelines
          bge-rerank-base-cpu:
            enabled: false
            features: [Reranking]
            url: "hf://BAAI/bge-reranker-base"
            engine: Infinity
            resourceProfile: cpu:1
            minReplicas: 0

          # ---------------------------------------------------------
          # SPEECH TO TEXT
          # ---------------------------------------------------------

          # Faster Whisper Medium - Audio transcription
          faster-whisper-medium-en-cpu:
            enabled: false
            features: [SpeechToText]
            url: "hf://Systran/faster-whisper-medium.en"
            engine: FasterWhisper
            resourceProfile: cpu:1
            minReplicas: 0

          # ---------------------------------------------------------
          # VISION MODELS
          # ---------------------------------------------------------

          # Llama 3.2 11B Vision - Multimodal (image + text)
          llama-3.2-11b-vision-instruct-l4:
            enabled: false
            features: [TextGeneration]
            url: hf://neuralmagic/Llama-3.2-11B-Vision-Instruct-FP8-dynamic
            engine: VLLM
            resourceProfile: nvidia-gpu-l4:1
            minReplicas: 0
            maxReplicas: 1
            targetRequests: 32
            env:
              VLLM_WORKER_MULTIPROC_METHOD: spawn
            args:
              - --max-model-len=8192
              - --max-num-batched-token=8192
              - --gpu-memory-utilization=0.99
              - --enforce-eager
              - --disable-log-requests
              - --max-num-seqs=16

  destination:
    server: https://kubernetes.default.svc
    namespace: kubeai

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false

    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
      - PrunePropagationPolicy=foreground
      - RespectIgnoreDifferences=true

    retry:
      limit: 5
      backoff:
        duration: 10s
        factor: 2
        maxDuration: 3m

  revisionHistoryLimit: 5
