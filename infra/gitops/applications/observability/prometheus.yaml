---
# Argo CD Application for Prometheus
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: prometheus
  namespace: argocd
  labels:
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: platform
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: platform
  source:
    repoURL: https://prometheus-community.github.io/helm-charts
    targetRevision: 25.27.0
    chart: prometheus
    helm:
      values: |
        # Prometheus configuration
        server:
          persistentVolume:
            enabled: true
            storageClass: "local-path"  # Use local-path until Mayastor is configured with hugepages
            size: 50Gi
          # Pod security context
          securityContext:
            fsGroup: 65534
            runAsUser: 65534
            runAsGroup: 65534
            runAsNonRoot: true
          # Pod labels/annotations for log collection
          podLabels:
            platform.5dlabs.io/log-collection: enabled
          podAnnotations:
            logs.platform.5dlabs.io/collect: "true"
            logs.platform.5dlabs.io/service: prometheus
          # Resource allocation
          resources:
            limits:
              cpu: 2000m
              memory: 4Gi
            requests:
              cpu: 1000m
              memory: 2Gi
          # Data retention
          retention: 15d
          # Enable remote write receiver for OTel collector
          extraFlags:
            - web.enable-remote-write-receiver
          # Global scrape configuration
          global:
            scrape_interval: 15s
            scrape_timeout: 10s
            evaluation_interval: 15s

        # Scrape configurations
        serverFiles:
          prometheus.yml:
            # Alerting configuration - connect to Alertmanager
            alerting:
              alertmanagers:
                - static_configs:
                    - targets:
                        - alertmanager.observability.svc.cluster.local:9093

            # Rule files to load
            rule_files:
              - /etc/config/alerting_rules.yml

            scrape_configs:
              # Self-scrape Prometheus
              - job_name: prometheus
                static_configs:
                  - targets:
                      - localhost:9090
              # Scrape kube-state-metrics for pod/container status
              - job_name: kube-state-metrics
                static_configs:
                  - targets:
                      - kube-state-metrics.observability.svc.cluster.local:8080
                metric_relabel_configs:
                  # Keep pod and container status metrics
                  - source_labels: [__name__]
                    regex: 'kube_pod_.*|kube_container_.*|kube_job_.*|kube_namespace_.*'
                    action: keep
              # OTEL collector metrics
              - job_name: otel-collector
                static_configs:
                  - targets:
                      - otel-collector-opentelemetry-collector.observability.svc.cluster.local:8889
                    labels:
                      namespace: observability
                      service: otel-collector
              # Loki metrics (for LokiDown alert)
              - job_name: loki
                static_configs:
                  - targets:
                      - loki-gateway.observability.svc.cluster.local:80
                    labels:
                      namespace: observability
                      service: loki
              # Alertmanager metrics (for AlertmanagerDown alert)
              - job_name: alertmanager
                static_configs:
                  - targets:
                      - alertmanager.observability.svc.cluster.local:9093
                    labels:
                      namespace: observability
                      service: alertmanager
              # Grafana metrics (for GrafanaDown alert)
              - job_name: grafana
                static_configs:
                  - targets:
                      - grafana.observability.svc.cluster.local:80
                    labels:
                      namespace: observability
                      service: grafana

              # =================================================================
              # Argo Workflows Metrics
              # =================================================================
              # Scrape Argo Workflow controller for workflow execution metrics
              - job_name: argo-workflows
                static_configs:
                  - targets:
                      - argo-workflows-workflow-controller.automation.svc.cluster.local:9090
                    labels:
                      namespace: automation
                      service: argo-workflows

              # =================================================================
              # CTO Platform Metrics
              # =================================================================
              # Scrape CTO components if they expose metrics
              - job_name: cto-controller
                static_configs:
                  - targets:
                      - cto-controller.cto.svc.cluster.local:8080
                    labels:
                      namespace: cto
                      service: controller
                metrics_path: /metrics
                # Controller may not have metrics yet - ignore failures
                honor_labels: true

              - job_name: cto-healer
                static_configs:
                  - targets:
                      - cto-healer.cto.svc.cluster.local:8080
                    labels:
                      namespace: cto
                      service: healer
                metrics_path: /metrics
                honor_labels: true

              # =================================================================
              # Blackbox Exporter - External Endpoint Monitoring
              # =================================================================
              # Monitors external webhooks and APIs via Cloudflare tunnels
              # to detect issues like stale tunnel configs or SSL problems

              # External health endpoints (GET requests expecting 200)
              - job_name: blackbox-external-endpoints
                metrics_path: /probe
                params:
                  module: [http_health]
                static_configs:
                  - targets:
                      - https://pm.5dlabs.ai/health
                      - https://pm.5dlabs.ai/ready
                    labels:
                      service: pm
                      endpoint_type: health
                relabel_configs:
                  - source_labels: [__address__]
                    target_label: __param_target
                  - source_labels: [__param_target]
                    target_label: instance
                  - target_label: __address__
                    replacement: prometheus-blackbox-exporter.observability.svc:9115

              # External webhook endpoints (POST requests, expect 4xx without payload)
              - job_name: blackbox-external-webhooks
                metrics_path: /probe
                params:
                  module: [http_webhook]
                static_configs:
                  - targets:
                      - https://pm.5dlabs.ai/webhooks/linear
                    labels:
                      service: pm
                      endpoint_type: webhook
                      integration: linear
                relabel_configs:
                  - source_labels: [__address__]
                    target_label: __param_target
                  - source_labels: [__param_target]
                    target_label: instance
                  - target_label: __address__
                    replacement: prometheus-blackbox-exporter.observability.svc:9115

              # SSL certificate monitoring
              - job_name: blackbox-ssl-certificates
                metrics_path: /probe
                params:
                  module: [http_ssl]
                scrape_interval: 1h  # Check SSL less frequently
                static_configs:
                  - targets:
                      - https://pm.5dlabs.ai
                    labels:
                      service: pm
                relabel_configs:
                  - source_labels: [__address__]
                    target_label: __param_target
                  - source_labels: [__param_target]
                    target_label: instance
                  - target_label: __address__
                    replacement: prometheus-blackbox-exporter.observability.svc:9115

              # Blackbox exporter self-metrics
              - job_name: blackbox-exporter
                static_configs:
                  - targets:
                      - prometheus-blackbox-exporter.observability.svc:9115
                    labels:
                      namespace: observability
                      service: blackbox-exporter

          # =================================================================
          # Alerting Rules for External Endpoint Monitoring
          # =================================================================
          alerting_rules.yml:
            groups:
              - name: external-endpoints
                rules:
                  # Alert when any external endpoint is unreachable
                  - alert: ExternalEndpointDown
                    expr: probe_success == 0
                    for: 3m
                    labels:
                      severity: critical
                    annotations:
                      summary: "External endpoint unreachable: {{ $labels.instance }}"
                      description: |
                        The external endpoint {{ $labels.instance }} has been unreachable for more than 3 minutes.
                        Service: {{ $labels.service }}
                        Type: {{ $labels.endpoint_type }}
                        This may indicate a Cloudflare tunnel misconfiguration or service outage.

                  # Alert specifically for webhook endpoints
                  - alert: WebhookEndpointDown
                    expr: probe_success{endpoint_type="webhook"} == 0
                    for: 2m
                    labels:
                      severity: critical
                      service: "{{ $labels.service }}"
                    annotations:
                      summary: "Webhook endpoint down: {{ $labels.instance }}"
                      description: |
                        The {{ $labels.integration }} webhook endpoint at {{ $labels.instance }} is unreachable.
                        Integration events (e.g., Linear webhooks) will fail to be received.
                        Check Cloudflare tunnel configuration and PM service status.

                  # High latency warning
                  - alert: ExternalEndpointHighLatency
                    expr: probe_duration_seconds > 5
                    for: 5m
                    labels:
                      severity: warning
                    annotations:
                      summary: "High latency on external endpoint: {{ $labels.instance }}"
                      description: |
                        The endpoint {{ $labels.instance }} is responding slowly ({{ $value | printf "%.2f" }}s).
                        Service: {{ $labels.service }}

                  # SSL certificate expiring soon (within 14 days)
                  - alert: SSLCertificateExpiringSoon
                    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 14
                    for: 1h
                    labels:
                      severity: warning
                    annotations:
                      summary: "SSL certificate expiring soon: {{ $labels.instance }}"
                      description: |
                        The SSL certificate for {{ $labels.instance }} will expire in less than 14 days.
                        Days remaining: {{ $value | humanizeDuration }}

                  # SSL certificate expired or expiring very soon (within 3 days)
                  - alert: SSLCertificateExpiring
                    expr: probe_ssl_earliest_cert_expiry - time() < 86400 * 3
                    for: 10m
                    labels:
                      severity: critical
                    annotations:
                      summary: "SSL certificate expiring: {{ $labels.instance }}"
                      description: |
                        URGENT: The SSL certificate for {{ $labels.instance }} will expire in less than 3 days.
                        Renew immediately to avoid service disruption.

              - name: observability-stack
                rules:
                  # Blackbox exporter down
                  - alert: BlackboxExporterDown
                    expr: up{job="blackbox-exporter"} == 0
                    for: 5m
                    labels:
                      severity: warning
                    annotations:
                      summary: "Blackbox exporter is down"
                      description: |
                        The Blackbox exporter is not responding. External endpoint monitoring is disabled.
                        Check the blackbox-exporter deployment in the observability namespace.

              # =================================================================
              # CTO Platform Alerts
              # =================================================================
              # Monitors core CTO platform components for self-healing
              - name: cto-platform
                rules:
                  # Controller pod not running
                  - alert: CTOControllerDown
                    expr: |
                      kube_pod_status_phase{namespace="cto", pod=~"cto-controller.*", phase="Running"} == 0
                      or absent(kube_pod_status_phase{namespace="cto", pod=~"cto-controller.*", phase="Running"})
                    for: 3m
                    labels:
                      severity: critical
                      component: controller
                      platform: cto
                    annotations:
                      summary: "CTO Controller is down"
                      description: |
                        The CTO Controller pod is not running. CodeRun orchestration is disabled.
                        This will prevent new agent runs from being scheduled.
                        Namespace: cto

                  # Controller crash loop
                  - alert: CTOControllerCrashLoop
                    expr: |
                      kube_pod_container_status_restarts_total{namespace="cto", pod=~"cto-controller.*"} > 3
                    for: 5m
                    labels:
                      severity: critical
                      component: controller
                      platform: cto
                    annotations:
                      summary: "CTO Controller in crash loop"
                      description: |
                        The CTO Controller has restarted {{ $value }} times.
                        Pod: {{ $labels.pod }}
                        Check logs for panic or configuration errors.

                  # Healer pod not running
                  - alert: CTOHealerDown
                    expr: |
                      kube_pod_status_phase{namespace="cto", pod=~"cto-healer.*", phase="Running"} == 0
                      or absent(kube_pod_status_phase{namespace="cto", pod=~"cto-healer.*", phase="Running"})
                    for: 3m
                    labels:
                      severity: warning
                      component: healer
                      platform: cto
                    annotations:
                      summary: "CTO Healer is down"
                      description: |
                        The CTO Healer pod is not running. Self-healing is disabled.
                        CI failures will not be automatically remediated.

                  # PM (Project Manager) pod not running
                  - alert: CTOPMDown
                    expr: |
                      kube_pod_status_phase{namespace="cto", pod=~"cto-pm.*", phase="Running"} == 0
                      or absent(kube_pod_status_phase{namespace="cto", pod=~"cto-pm.*", phase="Running"})
                    for: 3m
                    labels:
                      severity: critical
                      component: pm
                      platform: cto
                    annotations:
                      summary: "CTO Project Manager is down"
                      description: |
                        The CTO PM pod is not running. Webhooks and intake processing are disabled.
                        Linear and GitHub webhooks will fail.

                  # BuildKit not ready
                  - alert: CTOBuildKitNotReady
                    expr: |
                      kube_pod_status_ready{namespace="cto", pod=~"cto-buildkit.*"} == 0
                    for: 5m
                    labels:
                      severity: warning
                      component: buildkit
                      platform: cto
                    annotations:
                      summary: "CTO BuildKit is not ready"
                      description: |
                        BuildKit pod {{ $labels.pod }} is not ready.
                        Container builds will be slower (no caching) or may fail.

                  # High memory usage on CTO pods
                  - alert: CTOHighMemory
                    expr: |
                      (
                        container_memory_working_set_bytes{namespace="cto", pod=~"cto-.*"}
                        / container_spec_memory_limit_bytes{namespace="cto", pod=~"cto-.*"}
                      ) > 0.9
                    for: 5m
                    labels:
                      severity: warning
                      platform: cto
                    annotations:
                      summary: "CTO pod high memory usage"
                      description: |
                        Pod {{ $labels.pod }} is using {{ $value | humanizePercentage }} of memory limit.
                        Consider increasing resource limits or investigating memory leak.

                  # CodeRun stuck in pending
                  - alert: CTOCodeRunStuck
                    expr: |
                      (
                        kube_pod_status_phase{namespace="cto", pod=~".*-coderun-.*", phase="Pending"}
                        * on(pod) group_left()
                        (time() - kube_pod_created{namespace="cto", pod=~".*-coderun-.*"}) > 600
                      )
                    for: 2m
                    labels:
                      severity: warning
                      component: coderun
                      platform: cto
                    annotations:
                      summary: "CodeRun stuck in Pending"
                      description: |
                        CodeRun pod {{ $labels.pod }} has been pending for over 10 minutes.
                        Check for resource constraints, image pull issues, or scheduling problems.

                  # CodeRun pod failure
                  - alert: CTOCodeRunFailed
                    expr: |
                      kube_pod_status_phase{namespace="cto", pod=~".*-coderun-.*", phase="Failed"} == 1
                    for: 1m
                    labels:
                      severity: warning
                      component: coderun
                      platform: cto
                    annotations:
                      summary: "CodeRun pod failed"
                      description: |
                        CodeRun pod {{ $labels.pod }} has failed.
                        Check pod logs for error details and consider remediation.

              # =================================================================
              # Argo Workflow Alerts
              # =================================================================
              # Monitors Argo Workflows for failures and stuck states
              - name: argo-workflows
                rules:
                  # Workflow controller down
                  - alert: ArgoWorkflowControllerDown
                    expr: |
                      kube_pod_status_phase{namespace="automation", pod=~"argo-workflows-workflow-controller.*", phase="Running"} == 0
                      or absent(kube_pod_status_phase{namespace="automation", pod=~"argo-workflows-workflow-controller.*", phase="Running"})
                    for: 3m
                    labels:
                      severity: critical
                      component: workflow-controller
                      platform: argo
                    annotations:
                      summary: "Argo Workflow Controller is down"
                      description: |
                        The Argo Workflow Controller is not running. No workflows can be executed.
                        Play orchestration, intake, and all automation is disabled.

                  # Workflow failed
                  - alert: ArgoWorkflowFailed
                    expr: |
                      kube_pod_status_phase{namespace=~"cto|automation", pod=~"play-.*|intake-.*|project-intake-.*", phase="Failed"} == 1
                    for: 1m
                    labels:
                      severity: warning
                      component: workflow
                      platform: argo
                    annotations:
                      summary: "Argo Workflow pod failed"
                      description: |
                        Workflow pod {{ $labels.pod }} has failed in namespace {{ $labels.namespace }}.
                        This may indicate a task failure that needs remediation.

                  # Workflow step stuck (running > 45 min)
                  - alert: ArgoWorkflowStepStuck
                    expr: |
                      (
                        kube_pod_status_phase{namespace=~"cto|automation", pod=~"play-.*|intake-.*", phase="Running"}
                        * on(pod) group_left()
                        (time() - kube_pod_start_time{namespace=~"cto|automation", pod=~"play-.*|intake-.*"}) > 2700
                      )
                    for: 5m
                    labels:
                      severity: warning
                      component: workflow
                      platform: argo
                    annotations:
                      summary: "Argo Workflow step stuck"
                      description: |
                        Workflow pod {{ $labels.pod }} has been running for over 45 minutes.
                        This may indicate an agent is stuck or unresponsive.

                  # High workflow failure rate
                  - alert: ArgoWorkflowHighFailureRate
                    expr: |
                      (
                        sum(increase(kube_pod_status_phase{namespace=~"cto|automation", pod=~"play-.*", phase="Failed"}[1h])) /
                        sum(increase(kube_pod_status_phase{namespace=~"cto|automation", pod=~"play-.*"}[1h]))
                      ) > 0.3
                    for: 10m
                    labels:
                      severity: warning
                      component: workflow
                      platform: argo
                    annotations:
                      summary: "High workflow failure rate"
                      description: |
                        Over 30% of play workflows have failed in the last hour.
                        This may indicate a systemic issue requiring investigation.

                  # Workflow pending too long
                  - alert: ArgoWorkflowPendingTooLong
                    expr: |
                      (
                        kube_pod_status_phase{namespace=~"cto|automation", pod=~"play-.*|intake-.*", phase="Pending"}
                        * on(pod) group_left()
                        (time() - kube_pod_created{namespace=~"cto|automation", pod=~"play-.*|intake-.*"}) > 300
                      )
                    for: 2m
                    labels:
                      severity: warning
                      component: workflow
                      platform: argo
                    annotations:
                      summary: "Workflow pod stuck in Pending"
                      description: |
                        Workflow pod {{ $labels.pod }} has been pending for over 5 minutes.
                        Check for resource constraints or image pull issues.

        # Disable components we don't need (using separate Alertmanager deployment)
        alertmanager:
          enabled: false

        # Disable kube-state-metrics (deployed separately)
        kube-state-metrics:
          enabled: false

        # Disable prometheus-node-exporter
        prometheus-node-exporter:
          enabled: false

        # Disable prometheus-pushgateway
        prometheus-pushgateway:
          enabled: false

  destination:
    server: https://kubernetes.default.svc
    namespace: observability
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - ServerSideApply=true
      - RespectIgnoreDifferences=true
