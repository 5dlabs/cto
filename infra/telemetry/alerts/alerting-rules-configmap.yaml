---
apiVersion: v1
kind: ConfigMap
metadata:
  name: claude-code-alerting-rules
  namespace: observability
  labels:
    app: alerting
    component: rules
data:
  claude-code-alerts.yaml: |
    groups:
      - name: claude_code_api
        interval: 30s
        rules:
          # High API error rate alert
          - alert: ClaudeCodeHighErrorRate
            expr: |
              (
                sum(rate(claude_code_api_error[5m])) by (github_user, working_service)
                /
                sum(rate(claude_code_api_request[5m])) by (github_user, working_service)
              ) * 100 > 5
            for: 5m
            labels:
              severity: warning
              component: claude-code
            annotations:
              summary: "High API error rate for {{ $labels.github_user }}"
              description: "Claude Code API error rate is {{ $value | humanize }}% for user {{ $labels.github_user }} on service {{ $labels.working_service }} (threshold: 5%)"

          # Critical API error rate alert
          - alert: ClaudeCodeCriticalErrorRate
            expr: |
              (
                sum(rate(claude_code_api_error[5m])) by (github_user, working_service)
                /
                sum(rate(claude_code_api_request[5m])) by (github_user, working_service)
              ) * 100 > 20
            for: 2m
            labels:
              severity: critical
              component: claude-code
            annotations:
              summary: "Critical API error rate for {{ $labels.github_user }}"
              description: "Claude Code API error rate is {{ $value | humanize }}% for user {{ $labels.github_user }} on service {{ $labels.working_service }} (threshold: 20%)"

      - name: claude_code_cost
        interval: 1m
        rules:
          # User spending over $100/hour
          - alert: ClaudeCodeHighUserSpend
            expr: |
              sum(increase(claude_code_cost_usage[1h])) by (github_user, working_service) > 100
            for: 5m
            labels:
              severity: warning
              component: claude-code
              cost_alert: "true"
            annotations:
              summary: "High spending detected for {{ $labels.github_user }}"
              description: "User {{ $labels.github_user }} has spent ${{ $value | humanize }} in the last hour on service {{ $labels.working_service }}"

          # Total platform spending over $500/hour
          - alert: ClaudeCodeHighPlatformSpend
            expr: |
              sum(increase(claude_code_cost_usage[1h])) > 500
            for: 5m
            labels:
              severity: critical
              component: claude-code
              cost_alert: "true"
            annotations:
              summary: "High platform-wide spending detected"
              description: "Total Claude Code spending is ${{ $value | humanize }} in the last hour (threshold: $500/hour)"

          # Daily budget approaching limit
          - alert: ClaudeCodeDailyBudgetWarning
            expr: |
              sum(increase(claude_code_cost_usage[24h])) > 800
            for: 10m
            labels:
              severity: warning
              component: claude-code
              cost_alert: "true"
            annotations:
              summary: "Daily budget approaching limit"
              description: "Claude Code has spent ${{ $value | humanize }} today (80% of $1000 daily budget)"

      - name: infrastructure_health
        interval: 30s
        rules:
          # OTLP Collector down
          - alert: OTLPCollectorDown
            expr: up{job="otel-collector",namespace="observability"} == 0
            for: 2m
            labels:
              severity: critical
              component: otel-collector
            annotations:
              summary: "OTLP Collector is down"
              description: "OpenTelemetry Collector in namespace observability has been down for more than 2 minutes"

          # Prometheus down
          - alert: PrometheusDown
            expr: up{job="prometheus"} == 0
            for: 2m
            labels:
              severity: critical
              component: prometheus
            annotations:
              summary: "Prometheus is down"
              description: "Prometheus server has been down for more than 2 minutes"

          # Loki down
          - alert: LokiDown
            expr: up{job=~"loki.*"} == 0
            for: 2m
            labels:
              severity: critical
              component: loki
            annotations:
              summary: "Loki is down"
              description: "Loki server has been down for more than 2 minutes"

          # Alertmanager down
          - alert: AlertmanagerDown
            expr: up{job="alertmanager"} == 0
            for: 2m
            labels:
              severity: critical
              component: alertmanager
            annotations:
              summary: "Alertmanager is down"
              description: "Alertmanager has been down for more than 2 minutes"

          # Grafana down
          - alert: GrafanaDown
            expr: up{job="grafana"} == 0
            for: 5m
            labels:
              severity: warning
              component: grafana
            annotations:
              summary: "Grafana is down"
              description: "Grafana dashboard has been down for more than 5 minutes"

      - name: resource_usage
        interval: 30s
        rules:
          # High memory usage
          - alert: HighMemoryUsage
            expr: |
              (
                container_memory_working_set_bytes{namespace="observability",container!=""}
                /
                container_spec_memory_limit_bytes{namespace="observability",container!=""}
              ) * 100 > 80
            for: 5m
            labels:
              severity: warning
              component: infrastructure
            annotations:
              summary: "High memory usage in {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} container {{ $labels.container }} is using {{ $value | humanize }}% of its memory limit"

          # Critical memory usage
          - alert: CriticalMemoryUsage
            expr: |
              (
                container_memory_working_set_bytes{namespace="observability",container!=""}
                /
                container_spec_memory_limit_bytes{namespace="observability",container!=""}
              ) * 100 > 95
            for: 2m
            labels:
              severity: critical
              component: infrastructure
            annotations:
              summary: "Critical memory usage in {{ $labels.pod }}"
              description: "Pod {{ $labels.pod }} container {{ $labels.container }} is using {{ $value | humanize }}% of its memory limit"

          # Disk space usage (Prometheus)
          - alert: PrometheusDiskSpaceWarning
            expr: |
              (
                1 - (node_filesystem_avail_bytes{mountpoint="/prometheus"} / node_filesystem_size_bytes{mountpoint="/prometheus"})
              ) * 100 > 80
            for: 10m
            labels:
              severity: warning
              component: prometheus
            annotations:
              summary: "Prometheus disk space running low"
              description: "Prometheus data directory is {{ $value | humanize }}% full"

          # Disk space usage (Loki)
          - alert: LokiDiskSpaceWarning
            expr: |
              (
                1 - (node_filesystem_avail_bytes{mountpoint="/var/loki"} / node_filesystem_size_bytes{mountpoint="/var/loki"})
              ) * 100 > 80
            for: 10m
            labels:
              severity: warning
              component: loki
            annotations:
              summary: "Loki disk space running low"
              description: "Loki data directory is {{ $value | humanize }}% full"

      - name: data_ingestion
        interval: 30s
        rules:
          # No metrics ingestion
          - alert: NoMetricsIngestion
            expr: |
              rate(prometheus_tsdb_samples_appended_total[5m]) == 0
            for: 10m
            labels:
              severity: warning
              component: prometheus
            annotations:
              summary: "No metrics being ingested"
              description: "Prometheus has not received any new metrics for 10 minutes"

          # No logs ingestion
          - alert: NoLogsIngestion
            expr: |
              rate(loki_ingester_streams_created_total[5m]) == 0
            for: 10m
            labels:
              severity: warning
              component: loki
            annotations:
              summary: "No logs being ingested"
              description: "Loki has not received any new logs for 10 minutes"

          # High ingestion rate warning
          - alert: HighMetricsIngestionRate
            expr: |
              rate(prometheus_tsdb_samples_appended_total[1m]) * 60 > 100000
            for: 5m
            labels:
              severity: warning
              component: prometheus
            annotations:
              summary: "High metrics ingestion rate"
              description: "Metrics ingestion rate is {{ $value | humanize }} samples/minute"
