{
  "metadata": {
    "completedCount": 0,
    "lastModified": "2025-12-06T18:39:19.024669752+00:00",
    "taskCount": 17,
    "version": "1.0.0"
  },
  "tasks": [
    {
      "agentHint": "rex",
      "dependencies": [],
      "description": "Initialize the Rust project with Axum 0.7, configure PostgreSQL 15 with sqlx, and set up Redis 7 connections. Establish the directory structure following the architecture specification.",
      "details": "1. Run `cargo init --name teamsync-api`\n2. Add dependencies to Cargo.toml:\n   - axum = \"0.7\"\n   - tokio = { version = \"1\", features = [\"full\"] }\n   - sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-rustls\", \"migrate\"] }\n   - redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }\n   - tower = \"0.4\"\n   - tower-http = { version = \"0.5\", features = [\"trace\", \"cors\"] }\n3. Create directory structure: src/{api, domain, infra}\n4. Create config.rs for environment variables (DATABASE_URL, REDIS_URL)\n5. Implement connection pools in infra/db.rs and infra/redis.rs\n6. Create main.rs with basic Axum router and health check endpoint\n7. Add .env.example with required environment variables",
      "id": "16",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:27:31.790911925Z",
          "dependencies": [],
          "description": "Create the Rust project using cargo init and configure all required dependencies in Cargo.toml including Axum 0.7, tokio, sqlx with PostgreSQL features, Redis client, tower middleware, and tower-http utilities.",
          "details": "Run `cargo init --name teamsync-api` to create the project structure. Edit Cargo.toml to add: axum = \"0.7\", tokio = { version = \"1\", features = [\"full\"] }, sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-rustls\", \"migrate\"] }, redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }, tower = \"0.4\", tower-http = { version = \"0.5\", features = [\"trace\", \"cors\"] }, serde = { version = \"1.0\", features = [\"derive\"] }, dotenvy = \"0.15\". Run `cargo check` to verify all dependencies resolve correctly.",
          "id": 1,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Execute `cargo check` and `cargo build` to ensure all dependencies are correctly specified and compile without errors. Verify Cargo.lock is generated with correct versions.",
          "title": "Initialize Cargo project and configure dependencies",
          "updatedAt": "2025-12-06T18:27:31.790911925Z"
        },
        {
          "createdAt": "2025-12-06T18:27:31.790912550Z",
          "dependencies": [
            "1"
          ],
          "description": "Establish the project directory structure following clean architecture principles with separate layers for API routes, domain logic, and infrastructure concerns. Create all necessary module files and directory hierarchy.",
          "details": "Create the following directory structure: src/api/ (for HTTP handlers and routes), src/domain/ (for business logic and entities), src/infra/ (for database, Redis, and external service integrations). Create mod.rs files in each directory to declare modules. Create placeholder files: src/api/mod.rs, src/api/health.rs, src/domain/mod.rs, src/infra/mod.rs, src/infra/db.rs, src/infra/redis.rs, src/config.rs. Update src/lib.rs to declare these modules publicly. Create .env.example with DATABASE_URL=postgresql://user:password@localhost:5432/teamsync and REDIS_URL=redis://localhost:6379.",
          "id": 2,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo build` to verify all modules are properly declared and the directory structure compiles. Check that .env.example exists with required variables.",
          "title": "Create clean architecture directory structure",
          "updatedAt": "2025-12-06T18:27:31.790912550Z"
        },
        {
          "createdAt": "2025-12-06T18:27:31.790912717Z",
          "dependencies": [
            "2"
          ],
          "description": "Create the database connection pool manager using sqlx with PostgreSQL support, including connection configuration, pool initialization, and error handling for database connectivity.",
          "details": "In src/infra/db.rs, implement a function `create_pool()` that reads DATABASE_URL from environment variables using dotenvy, creates a PgPoolOptions instance with appropriate settings (max_connections: 5, connection_timeout: 30s), and returns Result<PgPool, sqlx::Error>. Add proper error handling and logging. Implement a health check function `check_db_health(pool: &PgPool)` that executes a simple SELECT 1 query to verify connectivity. Export the pool type and functions from src/infra/mod.rs.",
          "id": 3,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Create unit tests that mock the database connection. Create integration tests that connect to a test PostgreSQL instance and verify pool creation and health check execution. Test error handling with invalid connection strings.",
          "title": "Implement PostgreSQL connection pool with sqlx",
          "updatedAt": "2025-12-06T18:27:31.790912717Z"
        },
        {
          "createdAt": "2025-12-06T18:27:31.790912800Z",
          "dependencies": [
            "2"
          ],
          "description": "Configure Redis connection manager using the redis crate with tokio compatibility, including connection pool setup, error handling, and health check functionality for Redis connectivity verification.",
          "details": "In src/infra/redis.rs, implement a function `create_redis_client()` that reads REDIS_URL from environment variables, creates a redis::Client instance, and returns Result<redis::aio::ConnectionManager, redis::RedisError>. Use ConnectionManager for automatic reconnection handling. Implement a health check function `check_redis_health(manager: &ConnectionManager)` that executes a PING command to verify connectivity. Add proper error handling with descriptive error messages. Export types and functions from src/infra/mod.rs.",
          "id": 4,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Create unit tests with mocked Redis connections. Create integration tests that connect to a test Redis instance and verify connection manager creation, PING command execution, and reconnection behavior. Test error scenarios with invalid URLs.",
          "title": "Implement Redis connection manager setup",
          "updatedAt": "2025-12-06T18:27:31.790912800Z"
        },
        {
          "createdAt": "2025-12-06T18:27:31.790913008Z",
          "dependencies": [
            "3",
            "4"
          ],
          "description": "Implement the main Axum application server with basic routing, health check endpoint that verifies database and Redis connectivity, environment configuration loading, and graceful shutdown handling.",
          "details": "In src/config.rs, create a Config struct that loads DATABASE_URL and REDIS_URL using dotenvy with validation. In src/api/health.rs, implement a health check handler that returns JSON with status of PostgreSQL and Redis connections. In src/main.rs, create the main async function that: loads configuration, initializes database pool and Redis connection manager, creates an Axum router with GET /health endpoint using tower-http tracing and CORS middleware, binds to 0.0.0.0:3000, and implements graceful shutdown on SIGTERM/SIGINT. Add structured logging setup with tracing_subscriber.",
          "id": 5,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run the server locally and test the /health endpoint returns 200 OK with proper JSON structure. Use curl or integration tests to verify CORS headers and tracing middleware. Test graceful shutdown by sending SIGTERM. Verify server starts successfully with valid .env and fails gracefully with missing configuration.",
          "title": "Create Axum server with health check endpoint and configuration",
          "updatedAt": "2025-12-06T18:27:31.790913008Z"
        }
      ],
      "testStrategy": "Verify cargo build succeeds, health check endpoint returns 200, database and Redis connections establish successfully with docker-compose up",
      "title": "Setup Rust/Axum project foundation with PostgreSQL and Redis"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "16"
      ],
      "description": "Create PostgreSQL schema for users, teams, team_members, tasks, and invite_links tables with proper indexes and constraints. Implement soft delete pattern for tasks.",
      "details": "1. Create migrations directory: migrations/\n2. Migration 001_create_users.sql:\n   - id (UUID PK), email (UNIQUE), password_hash, oauth_provider, oauth_id, created_at, updated_at\n3. Migration 002_create_teams.sql:\n   - id (UUID PK), name, description, owner_id (FK users), created_at, updated_at\n4. Migration 003_create_team_members.sql:\n   - id (UUID PK), team_id (FK teams), user_id (FK users), role (ENUM: owner, admin, member, viewer), joined_at\n   - UNIQUE constraint on (team_id, user_id)\n5. Migration 004_create_tasks.sql:\n   - id (UUID PK), team_id (FK teams), title, description, assignee_id (FK users), status (ENUM: todo, in_progress, done), due_date, deleted_at (nullable), created_at, updated_at\n   - Index on (team_id, deleted_at, status)\n6. Migration 005_create_invite_links.sql:\n   - id (UUID PK), team_id (FK teams), token (UNIQUE), expires_at, created_by (FK users), created_at\n7. Run migrations with sqlx migrate run",
      "id": "17",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:28:11.266973804Z",
          "dependencies": [],
          "description": "Design and implement the users table migration with support for both email/password and OAuth authentication methods. Include proper constraints and indexes for authentication lookups.",
          "details": "Create migration file migrations/001_create_users.sql with:\n- id column as UUID primary key with gen_random_uuid() default\n- email column as VARCHAR(255) with UNIQUE constraint and NOT NULL\n- password_hash column as VARCHAR(255) nullable (for OAuth users)\n- oauth_provider column as VARCHAR(50) nullable (e.g., 'google', 'github')\n- oauth_id column as VARCHAR(255) nullable for external provider ID\n- created_at and updated_at columns as TIMESTAMP WITH TIME ZONE with defaults\n- Create unique index on (oauth_provider, oauth_id) for OAuth lookups\n- Create index on email for fast authentication queries\n- Add CHECK constraint to ensure either password_hash OR (oauth_provider AND oauth_id) is present\n- Include rollback/down migration to drop table and indexes",
          "id": 1,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify table creation with \\d users command, test unique constraints by attempting duplicate email inserts, validate OAuth constraint logic, and test index usage with EXPLAIN ANALYZE on authentication queries",
          "title": "Create users table migration with authentication fields",
          "updatedAt": "2025-12-06T18:28:11.266973804Z"
        },
        {
          "createdAt": "2025-12-06T18:28:11.266975971Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement the teams table migration establishing the core team entity with ownership tracking and proper foreign key relationships to users.",
          "details": "Create migration file migrations/002_create_teams.sql with:\n- id column as UUID primary key with gen_random_uuid() default\n- name column as VARCHAR(255) NOT NULL\n- description column as TEXT nullable\n- owner_id column as UUID NOT NULL with foreign key reference to users(id)\n- created_at and updated_at columns as TIMESTAMP WITH TIME ZONE with defaults\n- Add foreign key constraint: FOREIGN KEY (owner_id) REFERENCES users(id) ON DELETE CASCADE\n- Create index on owner_id for efficient owner lookup queries\n- Add CHECK constraint on name to ensure length > 0\n- Include rollback/down migration to drop table, foreign keys, and indexes in correct order",
          "id": 2,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify foreign key enforcement by attempting to insert team with non-existent owner_id, test CASCADE delete behavior, validate index on owner_id with EXPLAIN, and ensure name constraint prevents empty strings",
          "title": "Create teams table with ownership relationships",
          "updatedAt": "2025-12-06T18:28:11.266975971Z"
        },
        {
          "createdAt": "2025-12-06T18:28:11.266976471Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Design and implement the team_members association table with role-based access control enum type and composite unique constraints to prevent duplicate memberships.",
          "details": "Create migration file migrations/003_create_team_members.sql with:\n- First create ENUM type: CREATE TYPE team_role AS ENUM ('owner', 'admin', 'member', 'viewer')\n- id column as UUID primary key with gen_random_uuid() default\n- team_id column as UUID NOT NULL with FOREIGN KEY to teams(id) ON DELETE CASCADE\n- user_id column as UUID NOT NULL with FOREIGN KEY to users(id) ON DELETE CASCADE\n- role column as team_role NOT NULL with default 'member'\n- joined_at column as TIMESTAMP WITH TIME ZONE with default NOW()\n- Add UNIQUE constraint on (team_id, user_id) to prevent duplicate memberships\n- Create index on team_id for team member listing queries\n- Create index on user_id for user's teams queries\n- Include rollback/down migration to drop table, indexes, and enum type in correct order",
          "id": 3,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test unique constraint by attempting duplicate (team_id, user_id) inserts, verify enum values are enforced, validate CASCADE deletes when teams or users are removed, and check index performance with EXPLAIN on member lookup queries",
          "title": "Create team_members junction table with role enum and constraints",
          "updatedAt": "2025-12-06T18:28:11.266976471Z"
        },
        {
          "createdAt": "2025-12-06T18:28:11.266977054Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement the tasks table migration with soft delete functionality using deleted_at column, status tracking enum, and proper indexing for filtered queries.",
          "details": "Create migration file migrations/004_create_tasks.sql with:\n- First create ENUM type: CREATE TYPE task_status AS ENUM ('todo', 'in_progress', 'done')\n- id column as UUID primary key with gen_random_uuid() default\n- team_id column as UUID NOT NULL with FOREIGN KEY to teams(id) ON DELETE CASCADE\n- title column as VARCHAR(500) NOT NULL\n- description column as TEXT nullable\n- assignee_id column as UUID nullable with FOREIGN KEY to users(id) ON DELETE SET NULL\n- status column as task_status NOT NULL with default 'todo'\n- due_date column as TIMESTAMP WITH TIME ZONE nullable\n- deleted_at column as TIMESTAMP WITH TIME ZONE nullable (soft delete marker)\n- created_at and updated_at columns as TIMESTAMP WITH TIME ZONE with defaults\n- Create composite index on (team_id, deleted_at, status) for efficient filtered queries\n- Create index on assignee_id for user task lookups\n- Create index on due_date for deadline queries\n- Include rollback/down migration",
          "id": 4,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify soft delete by setting deleted_at and confirming record persists, test composite index with queries filtering by team_id WHERE deleted_at IS NULL, validate status enum constraints, test assignee_id SET NULL on user deletion, and use EXPLAIN ANALYZE to verify index usage",
          "title": "Create tasks table with soft delete pattern and status enum",
          "updatedAt": "2025-12-06T18:28:11.266977054Z"
        },
        {
          "createdAt": "2025-12-06T18:28:11.266977929Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Design and implement the invite_links table for team invitation management with unique tokens, expiration tracking, and audit trail of who created invitations.",
          "details": "Create migration file migrations/005_create_invite_links.sql with:\n- id column as UUID primary key with gen_random_uuid() default\n- team_id column as UUID NOT NULL with FOREIGN KEY to teams(id) ON DELETE CASCADE\n- token column as VARCHAR(255) NOT NULL with UNIQUE constraint for secure invite URLs\n- expires_at column as TIMESTAMP WITH TIME ZONE NOT NULL\n- created_by column as UUID NOT NULL with FOREIGN KEY to users(id) ON DELETE CASCADE\n- created_at column as TIMESTAMP WITH TIME ZONE with default NOW()\n- Create unique index on token for fast invite validation lookups\n- Create index on (team_id, expires_at) for cleanup queries of expired invites\n- Create index on created_by for audit trail queries\n- Add CHECK constraint to ensure expires_at > created_at\n- Include rollback/down migration to drop table and indexes",
          "id": 5,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify token uniqueness constraint, test CHECK constraint by attempting to create invite with expires_at before created_at, validate foreign key CASCADE behavior, test index performance on token lookups, and verify cleanup queries using (team_id, expires_at) index with EXPLAIN",
          "title": "Create invite_links table with expiration logic",
          "updatedAt": "2025-12-06T18:28:11.266977929Z"
        },
        {
          "createdAt": "2025-12-06T18:28:11.266978888Z",
          "dependencies": [
            "1",
            "2",
            "3",
            "4",
            "5"
          ],
          "description": "Execute all migration files in sequence using sqlx migrate run, validate foreign key constraints, verify index creation, and test referential integrity across all tables.",
          "details": "Execute the following validation steps:\n1. Run 'sqlx migrate run' to apply all migrations in order\n2. Verify migration tracking in _sqlx_migrations table\n3. Use \\d+ on each table to confirm all columns, constraints, and indexes are created correctly\n4. Test foreign key constraints by attempting invalid inserts (non-existent references)\n5. Validate CASCADE delete behavior: create test data chain (user -> team -> task) and delete user to confirm cascade\n6. Test soft delete pattern: insert task, set deleted_at, verify it's excluded from WHERE deleted_at IS NULL queries\n7. Verify enum types with \\dT+ command\n8. Run EXPLAIN ANALYZE on common query patterns to validate index usage:\n   - User authentication by email\n   - Team members by team_id\n   - Active tasks by team_id WHERE deleted_at IS NULL\n   - Valid invite links by token WHERE expires_at > NOW()\n9. Test rollback capability with 'sqlx migrate revert' on test database\n10. Document any performance considerations or index tuning recommendations",
          "id": 6,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Create comprehensive integration test suite that: inserts data across all tables, validates all foreign key relationships, tests CASCADE and SET NULL behaviors, verifies soft delete queries exclude deleted records, confirms enum constraints, measures query performance with indexes, and validates complete migration rollback functionality",
          "title": "Validate schema integrity and run all migrations",
          "updatedAt": "2025-12-06T18:28:11.266978888Z"
        }
      ],
      "testStrategy": "Run sqlx migrate run and verify all tables exist with correct schema using psql. Test foreign key constraints by attempting invalid inserts. Verify indexes exist with EXPLAIN ANALYZE queries.",
      "title": "Design and implement database schema with migrations"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "16",
        "17"
      ],
      "description": "Build JWT-based authentication system with access tokens (15min expiry) and refresh tokens (7 days), stored in Redis. Include token validation middleware.",
      "details": "1. Add dependencies: jsonwebtoken = \"9\", argon2 = \"0.5\"\n2. Create domain/auth.rs with structs: Claims, TokenPair, RefreshToken\n3. Implement JWT generation/validation functions:\n   - generate_access_token(user_id, role) -> Result<String>\n   - generate_refresh_token(user_id) -> Result<String>\n   - validate_token(token: &str) -> Result<Claims>\n4. Store refresh tokens in Redis with 7-day TTL: SET refresh:{token_id} {user_id} EX 604800\n5. Create api/auth.rs with endpoints:\n   - POST /api/auth/register (email, password)\n   - POST /api/auth/login (email, password) -> returns TokenPair\n   - POST /api/auth/refresh (refresh_token) -> returns new TokenPair\n6. Implement Axum middleware in infra/middleware.rs:\n   - RequireAuth extractor that validates Bearer token and injects Claims\n7. Add JWT_SECRET and REFRESH_TOKEN_SECRET to config",
      "id": "18",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:28:52.656865462Z",
          "dependencies": [],
          "description": "Create the core JWT token generation and validation functions with proper error handling, including access token (15min expiry) and refresh token (7 days) generation logic using jsonwebtoken crate.",
          "details": "Add dependencies: jsonwebtoken = \"9\", argon2 = \"0.5\" to Cargo.toml. Create domain/auth.rs with structs: Claims (user_id, role, exp, iat), TokenPair (access_token, refresh_token), RefreshToken (token_id, user_id, expires_at). Implement functions: generate_access_token(user_id: &str, role: &str) -> Result<String> with 15min expiry, generate_refresh_token(user_id: &str) -> Result<(String, String)> returning token and token_id with 7-day expiry, validate_token(token: &str, secret: &str) -> Result<Claims> with signature verification. Add JWT_SECRET and REFRESH_TOKEN_SECRET to config/settings.rs. Use HS256 algorithm for signing. Include proper error types for token expiration, invalid signature, and malformed tokens.",
          "id": 1,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for token generation with various user_id/role combinations, validation of valid tokens, rejection of expired tokens (mock time), rejection of tokens with invalid signatures, and rejection of malformed token strings. Test Claims serialization/deserialization.",
          "title": "Implement JWT token generation and validation utilities",
          "updatedAt": "2025-12-06T18:28:52.656865462Z"
        },
        {
          "createdAt": "2025-12-06T18:28:52.656866795Z",
          "dependencies": [
            "1"
          ],
          "description": "Build Redis storage layer for refresh tokens with 7-day TTL management, including storage, retrieval, validation, and revocation capabilities.",
          "details": "Create infra/token_store.rs with RefreshTokenStore struct wrapping Redis connection. Implement methods: store_refresh_token(token_id: &str, user_id: &str) -> Result<()> using Redis SET command with key pattern \"refresh:{token_id}\" and value \"{user_id}\" with EX 604800 (7 days in seconds), get_user_by_refresh_token(token_id: &str) -> Result<Option<String>> using GET command, revoke_refresh_token(token_id: &str) -> Result<()> using DEL command for logout/token rotation. Add connection pooling support using redis crate's connection manager. Include error handling for Redis connection failures and key not found scenarios. Implement cleanup logic for expired tokens (handled automatically by Redis TTL).",
          "id": 2,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with Redis test container: test storing and retrieving refresh tokens, verify TTL is set correctly, test token revocation, test retrieval of non-existent tokens returns None, test connection failure handling. Mock Redis for unit tests of error scenarios.",
          "title": "Implement refresh token storage and retrieval in Redis",
          "updatedAt": "2025-12-06T18:28:52.656866795Z"
        },
        {
          "createdAt": "2025-12-06T18:28:52.656867545Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement POST /api/auth/register endpoint with email/password validation, argon2 password hashing, and user creation in database.",
          "details": "Create api/auth.rs with register handler. Define RegisterRequest struct with email (validated format) and password (minimum 8 characters) fields. Implement register(Json(payload): Json<RegisterRequest>, State(db): State<DbPool>) -> Result<Json<TokenPair>> handler. Validate email format using regex, check password strength requirements. Hash password using argon2::hash_encoded with default config (Argon2id variant, memory cost 19456 KiB, time cost 2, parallelism 1). Check for duplicate email in database before insertion. Insert new user record with hashed password into users table. Generate TokenPair using functions from subtask 1. Return 201 Created with TokenPair on success. Handle errors: 400 for validation failures, 409 for duplicate email, 500 for database/hashing errors.",
          "id": 3,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: successful registration with valid credentials, rejection of invalid email formats, rejection of weak passwords, rejection of duplicate email registration, verification that password is hashed (not stored plaintext). Test that tokens are returned and valid. Mock database for unit tests.",
          "title": "Create user registration endpoint with password hashing",
          "updatedAt": "2025-12-06T18:28:52.656867545Z"
        },
        {
          "createdAt": "2025-12-06T18:28:52.656867920Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement POST /api/auth/login endpoint that validates user credentials against hashed passwords and returns JWT token pair on successful authentication.",
          "details": "Add login handler to api/auth.rs. Define LoginRequest struct with email and password fields. Implement login(Json(payload): Json<LoginRequest>, State(db): State<DbPool>, State(token_store): State<RefreshTokenStore>) -> Result<Json<TokenPair>> handler. Query database for user by email. Return 401 Unauthorized if user not found. Verify password using argon2::verify_encoded comparing provided password with stored hash. Return 401 if password verification fails. On success: generate access token and refresh token using functions from subtask 1, store refresh token in Redis using token_store from subtask 2, return TokenPair with both tokens. Include rate limiting consideration in error responses to prevent brute force attacks. Log failed login attempts for security monitoring.",
          "id": 4,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: successful login with correct credentials returns valid tokens, failed login with incorrect password returns 401, failed login with non-existent email returns 401, verify refresh token is stored in Redis after successful login, verify returned access token contains correct user_id and role claims. Test password timing attack resistance.",
          "title": "Create login endpoint with credential validation",
          "updatedAt": "2025-12-06T18:28:52.656867920Z"
        },
        {
          "createdAt": "2025-12-06T18:28:52.656868045Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement POST /api/auth/refresh endpoint that validates refresh tokens, rotates them for security, and issues new token pairs.",
          "details": "Add refresh handler to api/auth.rs. Define RefreshRequest struct with refresh_token field. Implement refresh(Json(payload): Json<RefreshRequest>, State(db): State<DbPool>, State(token_store): State<RefreshTokenStore>) -> Result<Json<TokenPair>> handler. Validate refresh token JWT signature and expiration using validate_token from subtask 1. Extract token_id from claims. Retrieve user_id from Redis using get_user_by_refresh_token from subtask 2. Return 401 if token not found in Redis (revoked/expired). Fetch user details from database to get current role. Implement token rotation: revoke old refresh token using revoke_refresh_token, generate new TokenPair, store new refresh token in Redis. Return new TokenPair. This prevents refresh token reuse attacks. Handle errors: 401 for invalid/expired/revoked tokens, 500 for database/Redis errors.",
          "id": 5,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: successful refresh with valid token returns new token pair, old refresh token is revoked after use, attempting to reuse old refresh token fails with 401, expired refresh token returns 401, invalid refresh token signature returns 401, refresh token not in Redis returns 401. Verify new refresh token is stored in Redis with correct TTL.",
          "title": "Create refresh token endpoint with rotation logic",
          "updatedAt": "2025-12-06T18:28:52.656868045Z"
        },
        {
          "createdAt": "2025-12-06T18:28:52.656868170Z",
          "dependencies": [
            "1"
          ],
          "description": "Create Axum middleware for JWT validation that extracts and validates Bearer tokens from Authorization headers and injects Claims into request handlers.",
          "details": "Create infra/middleware.rs with RequireAuth extractor. Implement FromRequestParts trait for RequireAuth struct containing Claims. In from_request_parts: extract Authorization header, validate Bearer scheme format, extract token string, validate token using validate_token from subtask 1 with JWT_SECRET, check token expiration, return Claims on success. Implement proper error responses: 401 Unauthorized with WWW-Authenticate header for missing/invalid tokens, 403 Forbidden for expired tokens. Create optional RequireRole(role: &str) extractor that builds on RequireAuth and validates user role from Claims. Add extension method to inject Config into middleware context. Example usage: async fn protected_route(RequireAuth(claims): RequireAuth) -> Result<Json<Response>>. Ensure middleware is composable with other Axum extractors.",
          "id": 6,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: requests with valid Bearer token succeed and inject correct Claims, requests without Authorization header return 401, requests with malformed Bearer token return 401, requests with expired token return 403, requests with invalid signature return 401, verify Claims contain correct user_id and role. Test RequireRole extractor with matching and non-matching roles. Test middleware composition with other extractors.",
          "title": "Implement Axum authentication middleware with Claims extraction",
          "updatedAt": "2025-12-06T18:28:52.656868170Z"
        }
      ],
      "testStrategy": "Unit tests for token generation/validation with valid/expired/invalid tokens. Integration tests: register user, login, use access token, refresh token after expiry, verify old refresh token invalidated. Test middleware rejects requests without valid token.",
      "title": "Implement JWT authentication with refresh token mechanism"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "18"
      ],
      "description": "Add OAuth2 authentication flow supporting Google and GitHub providers with callback handling and user profile synchronization.",
      "details": "1. Add dependency: oauth2 = \"4.4\"\n2. Create domain/oauth.rs with:\n   - OAuthProvider enum (Google, GitHub)\n   - OAuthClient struct wrapping oauth2::BasicClient\n3. Implement OAuth flow in api/oauth.rs:\n   - GET /api/auth/oauth/{provider} -> redirect to provider with state parameter\n   - GET /api/auth/oauth/{provider}/callback -> exchange code for token, fetch user profile\n4. Store OAuth state in Redis with 10min TTL for CSRF protection\n5. Create or update user in database:\n   - If oauth_provider + oauth_id exists, login\n   - Else create new user with email from profile\n6. Return TokenPair after successful OAuth\n7. Add to config: GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET, OAUTH_REDIRECT_URL\n8. Handle provider-specific profile APIs:\n   - Google: https://www.googleapis.com/oauth2/v2/userinfo\n   - GitHub: https://api.github.com/user",
      "id": "19",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:29:38.702302470Z",
          "dependencies": [],
          "description": "Set up OAuth2 client configuration with environment variables and create reusable OAuthClient wrapper for both Google and GitHub providers with proper scopes and endpoints.",
          "details": "1. Add oauth2 = \"4.4\" dependency to Cargo.toml\n2. Create domain/oauth.rs with OAuthProvider enum (Google, GitHub)\n3. Implement OAuthClient struct wrapping oauth2::BasicClient\n4. Add configuration loading from environment: GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET, OAUTH_REDIRECT_URL\n5. Configure provider-specific authorization and token URLs:\n   - Google: auth_url=https://accounts.google.com/o/oauth2/v2/auth, token_url=https://oauth2.googleapis.com/token\n   - GitHub: auth_url=https://github.com/login/oauth/authorize, token_url=https://github.com/login/oauth/access_token\n6. Set appropriate scopes: Google (email, profile), GitHub (read:user, user:email)\n7. Create factory method to instantiate clients based on OAuthProvider enum\n8. Add validation for required environment variables on startup",
          "id": 1,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for client instantiation with mock environment variables, validation tests for missing configuration",
          "title": "Configure OAuth2 clients for Google and GitHub providers",
          "updatedAt": "2025-12-06T18:29:38.702302470Z"
        },
        {
          "createdAt": "2025-12-06T18:29:38.702304970Z",
          "dependencies": [
            "1"
          ],
          "description": "Create GET /api/auth/oauth/{provider} endpoint that generates secure state tokens, stores them in Redis, and redirects users to the OAuth provider's authorization page.",
          "details": "1. Create api/oauth.rs with route handler for GET /api/auth/oauth/{provider}\n2. Validate provider parameter against OAuthProvider enum\n3. Generate cryptographically secure random state token (32 bytes, base64 encoded)\n4. Store state in Redis with key format \"oauth:state:{state_token}\" and 10-minute TTL\n5. Build authorization URL using OAuthClient with state parameter and redirect_uri\n6. Return HTTP 302 redirect response to provider's authorization URL\n7. Handle errors: invalid provider (400), Redis connection failure (500), client configuration error (500)\n8. Log authorization attempts with provider and timestamp",
          "id": 2,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with Redis mock, verify state storage and expiration, test redirect URL format, validate error responses for invalid providers",
          "title": "Implement authorization redirect endpoint with CSRF protection",
          "updatedAt": "2025-12-06T18:29:38.702304970Z"
        },
        {
          "createdAt": "2025-12-06T18:29:38.702306220Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Create GET /api/auth/oauth/{provider}/callback endpoint that validates state, exchanges authorization code for access token, and handles OAuth provider responses.",
          "details": "1. Create route handler for GET /api/auth/oauth/{provider}/callback\n2. Extract query parameters: code, state, error (for error handling)\n3. Validate state parameter by checking Redis key \"oauth:state:{state_token}\" exists\n4. Delete state from Redis after validation (one-time use)\n5. Exchange authorization code for access token using oauth2::BasicClient.exchange_code()\n6. Handle OAuth errors from query parameters (user denied, invalid scope, etc.)\n7. Implement retry logic with exponential backoff for token exchange (max 3 attempts)\n8. Extract access_token from TokenResponse\n9. Return error responses: state mismatch (403), expired state (401), code exchange failure (502)\n10. Pass access_token to profile fetching layer",
          "id": 3,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with mocked OAuth provider responses, test state validation success/failure, verify CSRF protection, test error handling for various OAuth error codes",
          "title": "Implement OAuth callback handler with code exchange",
          "updatedAt": "2025-12-06T18:29:38.702306220Z"
        },
        {
          "createdAt": "2025-12-06T18:29:38.702307011Z",
          "dependencies": [
            "3"
          ],
          "description": "Create service to fetch user profile from Google's userinfo API using the access token and parse the response into a standardized user profile structure.",
          "details": "1. Create UserProfile struct with fields: provider, provider_user_id, email, name, avatar_url\n2. Implement fetch_google_profile(access_token: &str) -> Result<UserProfile>\n3. Make authenticated GET request to https://www.googleapis.com/oauth2/v2/userinfo with Bearer token\n4. Parse JSON response extracting: id, email, name, picture\n5. Map Google response to UserProfile: provider=Google, provider_user_id=id, email=email, name=name, avatar_url=picture\n6. Handle API errors: invalid token (401), rate limiting (429), network failures\n7. Validate required fields exist (email must be present and verified)\n8. Add timeout of 10 seconds for API request\n9. Log profile fetch success/failure with anonymized user info",
          "id": 4,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mocked HTTP responses for success and error cases, verify field mapping, test timeout handling, validate error propagation",
          "title": "Implement Google user profile fetching and parsing",
          "updatedAt": "2025-12-06T18:29:38.702307011Z"
        },
        {
          "createdAt": "2025-12-06T18:29:38.702307261Z",
          "dependencies": [
            "3"
          ],
          "description": "Create service to fetch user profile from GitHub's user API, handling the requirement for separate email endpoint and API-specific authentication headers.",
          "details": "1. Implement fetch_github_profile(access_token: &str) -> Result<UserProfile>\n2. Make authenticated GET request to https://api.github.com/user with headers: Authorization: token {access_token}, User-Agent: {app_name}, Accept: application/vnd.github.v3+json\n3. Parse user response extracting: id, login, name, avatar_url\n4. Make second request to https://api.github.com/user/emails to get primary verified email\n5. Filter emails array for entry with primary=true and verified=true\n6. Map GitHub response to UserProfile: provider=GitHub, provider_user_id=id, email=primary_email, name=name ?? login, avatar_url=avatar_url\n7. Handle cases where name is null (use login as fallback)\n8. Handle API errors: invalid token (401), rate limiting (403 with X-RateLimit headers), network failures\n9. Add 10-second timeout for each API request\n10. Log profile fetch with rate limit info from response headers",
          "id": 5,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mocked GitHub API responses including email endpoint, test fallback logic for missing name, verify header requirements, test rate limit handling",
          "title": "Implement GitHub user profile fetching with API differences",
          "updatedAt": "2025-12-06T18:29:38.702307261Z"
        },
        {
          "createdAt": "2025-12-06T18:29:38.702307470Z",
          "dependencies": [
            "4",
            "5"
          ],
          "description": "Create database service to handle user lookup by OAuth credentials, create new users from OAuth profiles, or link OAuth providers to existing accounts with conflict resolution.",
          "details": "1. Create database query to find user by oauth_provider and oauth_provider_id\n2. If user found, update last_login timestamp and return user record\n3. If not found, check if user exists with same email address\n4. If email exists with different OAuth provider, return error (account linking required - future feature)\n5. If email exists without OAuth, link OAuth provider to existing user: update oauth_provider, oauth_provider_id\n6. If new user, insert record with: email, name, avatar_url, oauth_provider, oauth_provider_id, email_verified=true (trusted from OAuth)\n7. Handle unique constraint violations (concurrent registration race condition)\n8. Use database transaction for user creation/update operations\n9. Add user_id and oauth_provider fields to users table migration if not present\n10. Return User model with all fields populated",
          "id": 6,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with test database, verify user creation flow, test OAuth linking to existing user, test email conflict scenarios, verify transaction rollback on errors",
          "title": "Implement user creation and OAuth provider linking logic",
          "updatedAt": "2025-12-06T18:29:38.702307470Z"
        },
        {
          "createdAt": "2025-12-06T18:29:38.702308428Z",
          "dependencies": [
            "6"
          ],
          "description": "Connect the OAuth callback handler to existing JWT token generation system, creating TokenPair response and handling the complete authentication flow end-to-end.",
          "details": "1. After successful user creation/lookup in callback handler, generate JWT TokenPair using existing token service\n2. Include user_id, email, and oauth_provider in JWT claims\n3. Store refresh token in database associated with user\n4. Return JSON response with TokenPair: {access_token, refresh_token, token_type: \"Bearer\", expires_in}\n5. Add Set-Cookie headers for tokens if cookie-based auth is enabled\n6. Implement proper error responses with user-friendly messages: authentication_failed, email_conflict, provider_error\n7. Add success redirect option: if 'redirect_url' query param present, redirect to frontend with tokens in URL fragment\n8. Log successful OAuth authentication with user_id and provider\n9. Add metrics/monitoring for OAuth success/failure rates by provider\n10. Document OAuth endpoints in API documentation with example flows",
          "id": 7,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "End-to-end integration tests covering complete OAuth flow from redirect to token generation, test both JSON response and redirect modes, verify JWT claims, test error scenarios at each stage",
          "title": "Integrate OAuth flow with JWT token generation and response",
          "updatedAt": "2025-12-06T18:29:38.702308428Z"
        }
      ],
      "testStrategy": "Manual testing with real OAuth providers in development. Mock OAuth responses for integration tests. Verify state validation prevents CSRF. Test user creation and login flows. Verify email conflicts handled gracefully.",
      "title": "Implement OAuth2 integration for Google and GitHub"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "16",
        "18"
      ],
      "description": "Create middleware for rate limiting using Redis-based token bucket: 100 req/min for authenticated users, 20 req/min for anonymous.",
      "details": "1. Create infra/rate_limit.rs with TokenBucket struct\n2. Implement token bucket algorithm:\n   - Key format: rate_limit:{user_id|ip}:{window}\n   - Use Redis INCR with EXPIRE for atomic operations\n   - Lua script for atomic check-and-decrement:\n     ```lua\n     local current = redis.call('GET', KEYS[1])\n     if not current then\n       redis.call('SET', KEYS[1], ARGV[1] - 1, 'EX', ARGV[2])\n       return 1\n     elseif tonumber(current) > 0 then\n       redis.call('DECR', KEYS[1])\n       return 1\n     else\n       return 0\n     end\n     ```\n3. Create RateLimitLayer middleware:\n   - Extract user_id from Claims or use IP address\n   - Check bucket, decrement if available\n   - Return 429 Too Many Requests with Retry-After header if exhausted\n4. Apply different limits based on authentication status\n5. Add X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset headers",
      "id": "20",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:30:27.025823423Z",
          "dependencies": [],
          "description": "Create the core token bucket algorithm in infra/rate_limit.rs using Redis Lua scripting for atomic check-and-decrement operations. Implement the TokenBucket struct with methods for initializing buckets and checking token availability.",
          "details": "Create infra/rate_limit.rs module with TokenBucket struct containing Redis connection pool. Implement the Lua script for atomic token operations: check current token count, initialize bucket if not exists with SET and EXPIRE, decrement token if available, or return 0 if exhausted. The script ensures race-condition-free token consumption. Implement helper methods: check_and_consume_token() that executes the Lua script with proper key format (rate_limit:{identifier}:{window}), refill_tokens() for bucket initialization, and get_bucket_state() for retrieving current token count. Handle Redis script loading and caching for performance. Include proper error handling for Redis connection failures with Result types.",
          "id": 1,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for token bucket logic: verify token consumption decrements correctly, bucket initialization sets proper expiry, exhausted buckets return 0, concurrent token requests maintain atomicity using multiple threads. Mock Redis responses to test edge cases including script execution failures and connection timeouts.",
          "title": "Implement Redis token bucket algorithm with Lua script",
          "updatedAt": "2025-12-06T18:30:27.025823423Z"
        },
        {
          "createdAt": "2025-12-06T18:30:27.025826339Z",
          "dependencies": [
            "1"
          ],
          "description": "Set up Redis connection pooling and implement key namespacing strategy for rate limit buckets. Create configuration for bucket parameters (capacity, refill rate, window duration) and ensure proper Redis state management.",
          "details": "Extend TokenBucket with Redis connection pool configuration using redis-rs or fred crate. Implement key namespacing: rate_limit:{user_id|ip}:{window_timestamp} where window is calculated from current time (e.g., minute-aligned timestamp). Create RateLimitConfig struct with fields: max_tokens (100 for authenticated, 20 for anonymous), window_seconds (60), and refill_strategy. Implement bucket state persistence: use Redis SET with EX for automatic expiration after window duration. Add methods for bucket cleanup and state inspection. Implement connection retry logic with exponential backoff for transient Redis failures. Create a connection health check mechanism. Handle Redis cluster vs standalone configurations. Ensure thread-safe access to Redis pool using Arc<RedisPool>.",
          "id": 2,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with real Redis instance: verify key format correctness, test bucket expiration after window duration, validate connection pool behavior under load, test failover scenarios with Redis unavailability, verify namespace isolation between different users/IPs. Use testcontainers-rs for Redis test container management.",
          "title": "Implement Redis integration with key namespacing and connection management",
          "updatedAt": "2025-12-06T18:30:27.025826339Z"
        },
        {
          "createdAt": "2025-12-06T18:30:27.025839298Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement RateLimitLayer as Axum middleware that intercepts requests, extracts user identifier (user_id or IP), checks token availability, and enforces rate limits by returning 429 status when exhausted.",
          "details": "Create RateLimitLayer struct implementing tower::Layer and tower::Service traits. In service call method: extract user identifier from request - check for JWT Claims in request extensions to get user_id, fallback to IP address from ConnectInfo or X-Forwarded-For header. Call TokenBucket::check_and_consume_token() with appropriate identifier. If token available (returns 1), pass request to inner service. If exhausted (returns 0), return 429 Too Many Requests response with Retry-After header calculated from bucket window expiration. Implement graceful degradation: if Redis is unavailable, log error and allow request through (fail-open strategy) or reject based on configuration. Add middleware configuration for enabling/disabling rate limiting per route. Handle edge cases: missing IP address, invalid JWT, Redis timeout. Use async/await properly with tokio runtime.",
          "id": 3,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Middleware integration tests: simulate authenticated requests with valid JWT and verify user_id extraction, test anonymous requests use IP address, verify 429 response when limit exceeded with proper status code and body, test Retry-After header calculation accuracy, verify requests pass through when tokens available. Test fail-open behavior when Redis is down. Use axum-test or similar for HTTP testing.",
          "title": "Create Axum middleware layer for rate limit enforcement",
          "updatedAt": "2025-12-06T18:30:27.025839298Z"
        },
        {
          "createdAt": "2025-12-06T18:30:27.025840673Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Add X-RateLimit-Limit, X-RateLimit-Remaining, and X-RateLimit-Reset headers to all responses passing through the rate limit middleware, providing clients with visibility into their rate limit status.",
          "details": "Extend RateLimitLayer middleware to inject headers into responses. After token check, retrieve current bucket state using get_bucket_state() method. Add headers: X-RateLimit-Limit (max tokens for user type: 100 or 20), X-RateLimit-Remaining (current token count from Redis), X-RateLimit-Reset (Unix timestamp when bucket resets, calculated from window expiration). For 429 responses, set X-RateLimit-Remaining to 0 and include Retry-After header in seconds. Implement header injection for both successful and rate-limited responses. Handle cases where bucket state retrieval fails gracefully by omitting headers rather than failing request. Ensure headers are added before response is sent to client. Follow RFC 6585 and draft-ietf-httpapi-ratelimit-headers standards for header format.",
          "id": 4,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test header presence and correctness: verify X-RateLimit-Limit matches user type (100 for authenticated, 20 for anonymous), confirm X-RateLimit-Remaining decrements with each request, validate X-RateLimit-Reset timestamp is accurate and in future, test Retry-After header in 429 responses shows correct wait time. Verify headers present in both success and rate-limited responses. Test header omission when Redis state unavailable.",
          "title": "Implement rate limit response headers injection",
          "updatedAt": "2025-12-06T18:30:27.025840673Z"
        },
        {
          "createdAt": "2025-12-06T18:30:27.025841923Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Create logic to apply different rate limit configurations (100 req/min for authenticated users, 20 req/min for anonymous users) based on JWT presence and validity, with proper user identification strategy.",
          "details": "Implement user type detection in RateLimitLayer: check request extensions for validated JWT Claims (from auth middleware), extract user_id if present and JWT is valid. For authenticated users: use key format rate_limit:user:{user_id}:minute with 100 token capacity. For anonymous users: extract IP from ConnectInfo<SocketAddr> or X-Forwarded-For/X-Real-IP headers, use key format rate_limit:ip:{ip_address}:minute with 20 token capacity. Create RateLimitTier enum (Authenticated, Anonymous) with associated token limits. Implement get_rate_limit_config() method that returns appropriate config based on user type. Handle edge cases: malformed JWT (treat as anonymous), missing IP (use default identifier or reject), localhost requests (apply limits or bypass based on config). Ensure user_id is properly sanitized for Redis key usage. Add configuration option to customize limits per tier via environment variables or config file.",
          "id": 5,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test tier differentiation: send requests with valid JWT and verify 100 req/min limit applied using user_id as key, send requests without JWT and verify 20 req/min limit using IP as key, test limit enforcement by exceeding both thresholds, verify different users get independent buckets, verify same IP gets consistent anonymous bucket. Test IP extraction from various headers. Mock authentication middleware to provide test Claims. Simulate 100+ requests to verify authenticated limit and 20+ for anonymous limit.",
          "title": "Implement differentiated rate limits based on authentication status",
          "updatedAt": "2025-12-06T18:30:27.025841923Z"
        }
      ],
      "testStrategy": "Unit tests for token bucket logic with mocked Redis. Integration tests: make 100 requests as authenticated user, verify 101st fails. Test anonymous limit of 20. Verify rate limit resets after window. Test concurrent requests don't exceed limit.",
      "title": "Implement rate limiting with Redis token bucket algorithm"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "17",
        "18"
      ],
      "description": "Create team management endpoints with role-based access control enforcing owner/admin/member/viewer permissions.",
      "details": "1. Create domain/team.rs with:\n   - Team, TeamMember structs\n   - TeamRole enum (Owner, Admin, Member, Viewer)\n   - Permission checks: can_update_team(role), can_invite(role)\n2. Implement infra/team_repository.rs with sqlx queries:\n   - create_team(name, description, owner_id) -> Team\n   - get_team_by_id(id) -> Option<Team> with member count\n   - update_team(id, name, description) -> Result<Team>\n   - get_user_role_in_team(user_id, team_id) -> Option<TeamRole>\n3. Create api/teams.rs with handlers:\n   - POST /api/teams (requires auth) -> creates team, adds creator as owner\n   - GET /api/teams/:id (requires member role) -> returns team with member_count\n   - PATCH /api/teams/:id (requires admin role) -> updates team\n4. Implement RequireTeamRole(role) extractor that:\n   - Validates user is team member\n   - Checks role meets minimum requirement\n   - Returns 403 Forbidden if insufficient permissions",
      "id": "21",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:31:03.193614800Z",
          "dependencies": [],
          "description": "Implement the domain layer structures including Team and TeamMember structs, TeamRole enum with four levels (Owner, Admin, Member, Viewer), and permission checking methods that define what actions each role can perform.",
          "details": "Create domain/team.rs file. Define Team struct with fields: id, name, description, created_at, updated_at. Define TeamMember struct with: id, team_id, user_id, role, joined_at. Implement TeamRole enum with Owner, Admin, Member, Viewer variants. Add permission methods: can_update_team(role) -> bool (Owner/Admin only), can_invite(role) -> bool (Owner/Admin only), can_delete_team(role) -> bool (Owner only), can_view_team(role) -> bool (all roles). Include role hierarchy comparison methods for minimum role checks.",
          "id": 1,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for permission logic covering all role combinations. Test role hierarchy comparisons. Verify can_update_team returns true only for Owner/Admin, can_invite works correctly, and can_view_team allows all roles.",
          "title": "Create domain models for Team, TeamMember, and TeamRole with permission logic",
          "updatedAt": "2025-12-06T18:31:03.193614800Z"
        },
        {
          "createdAt": "2025-12-06T18:31:03.193618300Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the infrastructure layer repository for team data access, implementing all database operations using sqlx with proper error handling and transaction support where needed.",
          "details": "Create infra/team_repository.rs file. Implement TeamRepository struct with sqlx::PgPool. Add methods: create_team(name, description, owner_id) -> Result<Team> that inserts team and adds owner as TeamMember in transaction; get_team_by_id(id) -> Result<Option<Team>> with LEFT JOIN to count members; update_team(id, name, description) -> Result<Team>; get_user_role_in_team(user_id, team_id) -> Result<Option<TeamRole>> querying team_members table; list_user_teams(user_id) -> Result<Vec<Team>> with member counts. Use prepared statements and handle sqlx errors properly.",
          "id": 2,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with test database. Verify create_team atomically creates team and owner membership. Test get_team_by_id returns correct member count. Verify update_team modifies only specified fields. Test get_user_role_in_team returns correct role or None for non-members.",
          "title": "Implement team repository with sqlx queries for CRUD operations",
          "updatedAt": "2025-12-06T18:31:03.193618300Z"
        },
        {
          "createdAt": "2025-12-06T18:31:03.193618967Z",
          "dependencies": [
            "2"
          ],
          "description": "Create POST /api/teams endpoint that authenticates users, validates input, creates a new team, and automatically assigns the creator as the team owner in a single transaction.",
          "details": "In api/teams.rs, implement create_team handler with signature: async fn create_team(State(repo): State<TeamRepository>, auth: RequireAuth, Json(payload): Json<CreateTeamRequest>) -> Result<Json<TeamResponse>>. Define CreateTeamRequest with name (required, 1-100 chars) and description (optional, max 500 chars). Validate input using validator crate. Call repo.create_team with authenticated user ID as owner. Return 201 Created with team data including member_count=1. Handle errors: 400 for validation, 500 for database errors.",
          "id": 3,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests: verify authenticated user can create team and becomes owner; test validation errors for invalid names; verify unauthenticated requests return 401; confirm team appears in database with correct owner role; test concurrent team creation by same user.",
          "title": "Implement team creation endpoint with automatic owner assignment",
          "updatedAt": "2025-12-06T18:31:03.193618967Z"
        },
        {
          "createdAt": "2025-12-06T18:31:03.193619592Z",
          "dependencies": [
            "2"
          ],
          "description": "Create GET /api/teams/:id endpoint that returns team details with aggregated member count, enforcing that only team members can view the team information.",
          "details": "In api/teams.rs, implement get_team handler: async fn get_team(State(repo): State<TeamRepository>, Path(team_id): Path<i64>, auth: RequireAuth) -> Result<Json<TeamResponse>>. First verify user is team member by calling repo.get_user_role_in_team, return 403 if None. Then call repo.get_team_by_id to fetch team with member count from JOIN query. Define TeamResponse struct with: id, name, description, member_count, created_at, updated_at. Return 404 if team not found, 403 if user not member.",
          "id": 4,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: verify team members can retrieve team data with correct member_count; test non-members receive 403 Forbidden; verify all roles (Owner/Admin/Member/Viewer) can view; test 404 for non-existent teams; confirm member_count accuracy with multiple members.",
          "title": "Implement team retrieval endpoint with member count aggregation",
          "updatedAt": "2025-12-06T18:31:03.193619592Z"
        },
        {
          "createdAt": "2025-12-06T18:31:03.193619759Z",
          "dependencies": [
            "2"
          ],
          "description": "Create PATCH /api/teams/:id endpoint that allows team admins and owners to update team name and description, with proper role-based authorization checks.",
          "details": "In api/teams.rs, implement update_team handler: async fn update_team(State(repo): State<TeamRepository>, Path(team_id): Path<i64>, auth: RequireAuth, Json(payload): Json<UpdateTeamRequest>) -> Result<Json<TeamResponse>>. Define UpdateTeamRequest with optional name and description. Check user role via repo.get_user_role_in_team, verify role is Admin or Owner using domain permission logic, return 403 otherwise. Call repo.update_team with provided fields. Validate inputs before update. Return updated team with member count.",
          "id": 5,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: verify Owner and Admin can update teams; confirm Member and Viewer receive 403; test partial updates (name only, description only); verify validation on updated fields; test non-members get 403; confirm updates persist in database correctly.",
          "title": "Implement team update endpoint with admin authorization",
          "updatedAt": "2025-12-06T18:31:03.193619759Z"
        },
        {
          "createdAt": "2025-12-06T18:31:03.193619967Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement a reusable Axum extractor that validates team membership and enforces minimum role requirements, providing declarative RBAC for team endpoints.",
          "details": "In api/extractors/team_role.rs, create RequireTeamRole struct that implements FromRequestParts. Constructor takes minimum TeamRole required. In from_request_parts: extract team_id from path parameters, get authenticated user from RequireAuth extractor, query repo.get_user_role_in_team, return 403 if user not member or role insufficient based on hierarchy comparison. Store validated role and team_id in extractor for handler use. Example: RequireTeamRole::new(TeamRole::Admin) ensures user has Admin or Owner role. Add helper methods: is_owner(), is_admin_or_higher().",
          "id": 6,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit and integration tests: verify extractor rejects non-members with 403; test role hierarchy enforcement (Member cannot access Admin-required endpoints); confirm Owner passes all role checks; test extractor with various endpoint combinations; verify proper error responses and status codes.",
          "title": "Create RequireTeamRole extractor middleware for role-based access control",
          "updatedAt": "2025-12-06T18:31:03.193619967Z"
        }
      ],
      "testStrategy": "Integration tests: create team as user A, verify user A is owner. User B cannot access team. Add user B as member, verify they can GET but not PATCH. Test admin can PATCH. Verify member counts are accurate.",
      "title": "Implement Team Management CRUD endpoints with RBAC"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "21"
      ],
      "description": "Create endpoint to generate time-limited invite links for teams with 7-day expiration and single-use token validation.",
      "details": "1. Extend domain/team.rs with InviteLink struct\n2. Implement infra/invite_repository.rs:\n   - create_invite(team_id, created_by) -> InviteLink with random token\n   - get_invite_by_token(token) -> Option<InviteLink> (only if not expired)\n   - delete_invite(token) -> Result<()>\n3. Add to api/teams.rs:\n   - POST /api/teams/:id/invite (requires admin role)\n     - Generates UUID token\n     - Stores in database with expires_at = now() + 7 days\n     - Returns full invite URL: {BASE_URL}/invite/{token}\n   - POST /api/invite/{token}/accept (requires auth)\n     - Validates token not expired\n     - Checks user not already member\n     - Adds user to team with Member role\n     - Deletes invite token (single use)\n4. Use uuid crate for token generation\n5. Add scheduled job to clean expired invites (runs daily)",
      "id": "22",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:31:43.009228597Z",
          "dependencies": [],
          "description": "Implement the InviteLink domain struct in domain/team.rs with fields for token, team_id, created_by, created_at, and expires_at. Create infra/invite_repository.rs with methods for creating invites using cryptographically secure UUID tokens, retrieving non-expired invites by token, and deleting used tokens.",
          "details": "Add InviteLink struct to domain/team.rs with: token (String/UUID), team_id (i64), created_by (i64), created_at (DateTime), expires_at (DateTime). Implement infra/invite_repository.rs with: create_invite(team_id, created_by) that generates a secure UUID v4 token using uuid crate with proper randomness, calculates expires_at as now() + 7 days, inserts into database and returns InviteLink; get_invite_by_token(token) that queries database and filters out expired invites (WHERE expires_at > NOW()); delete_invite(token) that removes the invite record. Use sqlx for database operations with proper error handling.",
          "id": 1,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for token uniqueness, expiration calculation accuracy (7 days), repository CRUD operations. Test get_invite_by_token returns None for expired invites and Some for valid ones. Verify UUID v4 token format and randomness.",
          "title": "Create invite link domain model and repository with secure token generation",
          "updatedAt": "2025-12-06T18:31:43.009228597Z"
        },
        {
          "createdAt": "2025-12-06T18:31:43.009229097Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the invite generation endpoint in api/teams.rs that requires admin role verification, generates invite links with 7-day expiration, and returns the full invite URL to authorized team administrators.",
          "details": "Add POST /api/teams/:id/invite handler in api/teams.rs. Implement middleware/guard to verify authenticated user has admin role for the specified team_id. Call invite_repository.create_invite(team_id, user_id) to generate token. Construct full URL using BASE_URL environment variable: format!(\"{}/invite/{}\", base_url, token). Return JSON response with invite_url, expires_at timestamp, and token. Handle errors: 403 if not admin, 404 if team doesn't exist, 500 for database errors. Use actix-web extractors for path params and auth context.",
          "id": 2,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: verify only team admins can generate invites (403 for non-admins), correct URL format returned, expires_at is exactly 7 days from creation. Test with missing team_id (404). Mock time to verify expiration calculation.",
          "title": "Implement POST /api/teams/:id/invite endpoint with admin authorization",
          "updatedAt": "2025-12-06T18:31:43.009229097Z"
        },
        {
          "createdAt": "2025-12-06T18:31:43.009229347Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Create the invite acceptance endpoint that validates non-expired tokens, checks user membership status, adds users to teams with Member role, and enforces single-use token consumption through atomic deletion.",
          "details": "Add POST /api/invite/:token/accept handler in api/teams.rs. Extract authenticated user_id from session/JWT. Call invite_repository.get_invite_by_token(token) and return 404 if None (expired or invalid). Query team_members table to check if user_id already exists for team_id, return 409 Conflict if already member. Within a database transaction: (1) INSERT into team_members (team_id, user_id, role='Member', joined_at=NOW()), (2) DELETE invite by token. Commit transaction to ensure atomicity. Return 200 with team details on success. Handle race conditions where token is used simultaneously by using transaction isolation.",
          "id": 3,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: successful acceptance adds user with Member role, token becomes invalid after use (404 on reuse), expired tokens return 404, existing members get 409. Concurrency test: simulate simultaneous acceptance requests to verify only one succeeds. Verify transaction rollback on failure.",
          "title": "Implement POST /api/invite/:token/accept endpoint with validation",
          "updatedAt": "2025-12-06T18:31:43.009229347Z"
        },
        {
          "createdAt": "2025-12-06T18:31:43.009229514Z",
          "dependencies": [
            "3"
          ],
          "description": "Enhance the invite acceptance logic with proper transaction isolation and optimistic locking to prevent race conditions when multiple users attempt to use the same token simultaneously, ensuring exactly-once semantics.",
          "details": "Refactor the acceptance endpoint to use database transaction with SERIALIZABLE or REPEATABLE READ isolation level. Implement the token deletion as part of the atomic transaction: SELECT invite FOR UPDATE to lock the row, verify expiration within transaction, perform membership check, insert team_member, delete invite token - all within single transaction. Add unique constraint on (team_id, user_id) in team_members table if not exists. Handle deadlock retries with exponential backoff (max 3 attempts). Return specific error codes: 404 for token not found/expired, 409 for already member, 410 Gone for already-used token.",
          "id": 4,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Stress test with concurrent acceptance attempts (10+ simultaneous requests with same token). Verify exactly one succeeds with 200, others get 404/410. Test transaction rollback scenarios. Measure deadlock occurrence and retry success rate. Verify no orphaned records on failure.",
          "title": "Add atomic single-use token enforcement with race condition handling",
          "updatedAt": "2025-12-06T18:31:43.009229514Z"
        },
        {
          "createdAt": "2025-12-06T18:31:43.009229639Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement a daily background job that automatically removes expired invite tokens from the database to prevent table bloat and maintain system hygiene, with proper logging and error handling.",
          "details": "Create jobs/cleanup_invites.rs module with async cleanup function that executes: DELETE FROM invites WHERE expires_at < NOW(). Use tokio-cron-scheduler or similar crate to schedule daily execution (e.g., 2 AM UTC). Add logging with tracing crate to record: job start time, number of deleted records, completion time, any errors. Implement graceful shutdown handling to allow running jobs to complete. Add configuration in config.rs for schedule timing (default: \"0 0 2 * * *\" cron expression). Register job in main.rs application startup. Include metrics collection for monitoring (deleted count, execution duration).",
          "id": 5,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit test cleanup function with mock database containing expired and valid invites, verify only expired ones deleted. Integration test: insert test invites with past expires_at, run job, confirm deletion. Test job scheduling triggers at correct intervals. Verify logging output completeness and error handling when database unavailable.",
          "title": "Create scheduled cleanup job for expired invite tokens",
          "updatedAt": "2025-12-06T18:31:43.009229639Z"
        }
      ],
      "testStrategy": "Integration tests: admin creates invite, verify token stored with correct expiration. Non-member accepts invite, verify added as member. Test expired token returns 404. Test already-member returns 400. Verify token deleted after acceptance. Test viewer cannot create invite.",
      "title": "Implement invite link generation with expiration"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "21"
      ],
      "description": "Create task management endpoints with status/assignee/date range filtering and soft delete with 30-day retention.",
      "details": "1. Create domain/task.rs with:\n   - Task struct, TaskStatus enum (Todo, InProgress, Done)\n   - TaskFilter struct (status, assignee_id, due_date_start, due_date_end)\n2. Implement infra/task_repository.rs:\n   - create_task(team_id, title, description, assignee_id, due_date) -> Task\n   - get_tasks(team_id, filter: TaskFilter) -> Vec<Task> (WHERE deleted_at IS NULL)\n   - get_task_by_id(id) -> Option<Task>\n   - update_task(id, updates) -> Result<Task>\n   - soft_delete_task(id) -> Result<()> (SET deleted_at = NOW())\n   - hard_delete_old_tasks() -> Result<usize> (DELETE WHERE deleted_at < NOW() - INTERVAL '30 days')\n3. Create api/tasks.rs:\n   - POST /api/teams/:team_id/tasks (requires member role)\n   - GET /api/teams/:team_id/tasks?status=todo&assignee=uuid&due_after=date&due_before=date\n   - GET /api/tasks/:id (requires team member)\n   - PATCH /api/tasks/:id (requires assignee or admin)\n   - DELETE /api/tasks/:id (soft delete, requires admin)\n4. Validate assignee is team member before creating task\n5. Add background job for hard_delete_old_tasks (runs daily)",
      "id": "23",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:32:23.895175838Z",
          "dependencies": [],
          "description": "Implement the core domain models for task management including Task struct, TaskStatus enum, and TaskFilter struct in domain/task.rs",
          "details": "Create domain/task.rs with: (1) Task struct containing fields: id (Uuid), team_id (Uuid), title (String), description (Option<String>), assignee_id (Option<Uuid>), status (TaskStatus), due_date (Option<DateTime>), created_at (DateTime), updated_at (DateTime), deleted_at (Option<DateTime>). (2) TaskStatus enum with variants: Todo, InProgress, Done. Implement Display, Serialize, Deserialize traits. (3) TaskFilter struct with optional fields: status (Option<TaskStatus>), assignee_id (Option<Uuid>), due_date_start (Option<DateTime>), due_date_end (Option<DateTime>). Add validation methods to ensure date ranges are valid (start <= end).",
          "id": 1,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for TaskStatus enum conversions, TaskFilter validation logic (valid/invalid date ranges), and struct serialization/deserialization",
          "title": "Create Task domain models with status enum and filter struct",
          "updatedAt": "2025-12-06T18:32:23.895175838Z"
        },
        {
          "createdAt": "2025-12-06T18:32:23.895191213Z",
          "dependencies": [
            "1"
          ],
          "description": "Create infra/task_repository.rs with database operations supporting dynamic filtering by status, assignee, and date ranges while excluding soft-deleted records",
          "details": "Implement infra/task_repository.rs with: (1) create_task(team_id, title, description, assignee_id, due_date) returning Task. (2) get_tasks(team_id, filter: TaskFilter) returning Vec<Task> with dynamic SQL query building: base WHERE clause includes 'deleted_at IS NULL AND team_id = $1', then conditionally add status filter, assignee_id filter, and date range filters (due_date >= due_date_start AND due_date <= due_date_end). Use parameterized queries to prevent SQL injection. (3) get_task_by_id(id) returning Option<Task> with deleted_at IS NULL check. Add database indexes on (team_id, deleted_at), (assignee_id, deleted_at), (due_date, deleted_at) for query optimization.",
          "id": 2,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with test database: verify filtering combinations (status only, assignee only, date range only, all combined), ensure soft-deleted tasks are excluded, test NULL handling for optional filters",
          "title": "Implement task repository with complex filtering queries",
          "updatedAt": "2025-12-06T18:32:23.895191213Z"
        },
        {
          "createdAt": "2025-12-06T18:32:23.895192921Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement POST /api/teams/:team_id/tasks endpoint with validation that assignee is a team member and requester has member role",
          "details": "Create api/tasks.rs with POST /api/teams/:team_id/tasks endpoint: (1) Extract authenticated user from request context. (2) Verify user is a member of the team (query team_members table). (3) Parse request body with CreateTaskRequest struct (title, description, assignee_id, due_date). (4) If assignee_id is provided, validate that assignee is also a team member before task creation. (5) Call task_repository.create_task() and return 201 Created with task JSON. Return 403 Forbidden if user is not team member, 400 Bad Request if assignee is not team member, 422 Unprocessable Entity for validation errors (empty title, invalid date).",
          "id": 3,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests: successful creation by team member, rejection when user not in team, rejection when assignee not in team, validation error handling for missing/invalid fields",
          "title": "Create task creation endpoint with assignee validation",
          "updatedAt": "2025-12-06T18:32:23.895192921Z"
        },
        {
          "createdAt": "2025-12-06T18:32:23.895197338Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Create GET /api/teams/:team_id/tasks endpoint supporting query parameters for status, assignee, and date range filtering",
          "details": "Implement GET /api/teams/:team_id/tasks endpoint: (1) Verify authenticated user is team member. (2) Parse query parameters: status (enum string), assignee (UUID string), due_after (ISO date string), due_before (ISO date string). (3) Build TaskFilter struct from query params, handling parsing errors gracefully. (4) Call task_repository.get_tasks(team_id, filter) and return 200 OK with JSON array of tasks. (5) Return empty array if no tasks match filters. Handle query parameter validation errors with 400 Bad Request and clear error messages (e.g., 'Invalid status value', 'Invalid date format', 'Invalid UUID format').",
          "id": 4,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests: list all tasks, filter by each parameter individually, filter by multiple parameters combined, verify empty results for non-matching filters, test invalid parameter formats return 400 errors",
          "title": "Implement task listing endpoint with multi-parameter filtering",
          "updatedAt": "2025-12-06T18:32:23.895197338Z"
        },
        {
          "createdAt": "2025-12-06T18:32:23.895198254Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Implement PATCH /api/tasks/:id endpoint with authorization requiring user to be either the assignee or a team admin",
          "details": "Implement PATCH /api/tasks/:id endpoint: (1) Get authenticated user. (2) Fetch task by ID using get_task_by_id(). (3) Check authorization: user must be either the task assignee OR have admin role in the task's team (query team_members for role). (4) Parse UpdateTaskRequest struct with optional fields: title, description, status, assignee_id, due_date. (5) If updating assignee_id, validate new assignee is team member. (6) Call update_task(id, updates) in repository with dynamic SQL UPDATE statement building only provided fields. (7) Return 200 OK with updated task JSON. Return 404 if task not found, 403 if unauthorized, 400 if new assignee invalid.",
          "id": 5,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests: successful update by assignee, successful update by admin, rejection by non-assignee/non-admin, partial updates (single field), validation of assignee changes, test updating non-existent task returns 404",
          "title": "Create task update endpoint with authorization checks",
          "updatedAt": "2025-12-06T18:32:23.895198254Z"
        },
        {
          "createdAt": "2025-12-06T18:32:23.895199296Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Create DELETE /api/tasks/:id endpoint that performs soft delete by setting deleted_at timestamp, restricted to team admins",
          "details": "Implement: (1) soft_delete_task(id) in task_repository.rs: execute 'UPDATE tasks SET deleted_at = NOW(), updated_at = NOW() WHERE id = $1 AND deleted_at IS NULL' returning affected rows count. (2) DELETE /api/tasks/:id endpoint in api/tasks.rs: verify authenticated user has admin role in task's team (fetch task, check team_members table for admin role). Call soft_delete_task(id) and return 204 No Content on success. Return 404 if task not found or already deleted, 403 if user is not admin. Ensure all existing repository queries (get_tasks, get_task_by_id) include 'deleted_at IS NULL' filter to exclude soft-deleted records.",
          "id": 6,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: successful soft delete by admin, rejection by non-admin, verify soft-deleted tasks excluded from get_tasks and get_task_by_id, test deleting already-deleted task returns 404, verify deleted_at timestamp is set correctly",
          "title": "Implement soft delete with deleted_at timestamp",
          "updatedAt": "2025-12-06T18:32:23.895199296Z"
        },
        {
          "createdAt": "2025-12-06T18:32:23.895200796Z",
          "dependencies": [
            "6"
          ],
          "description": "Implement scheduled background job that permanently deletes tasks soft-deleted more than 30 days ago",
          "details": "Implement: (1) hard_delete_old_tasks() in task_repository.rs: execute 'DELETE FROM tasks WHERE deleted_at IS NOT NULL AND deleted_at < NOW() - INTERVAL '30 days'' returning count of deleted rows. Log the count for monitoring. (2) Create background job scheduler (using tokio-cron-scheduler or similar): register daily job (runs at 2 AM UTC) that calls hard_delete_old_tasks(). (3) Add job initialization to application startup in main.rs. (4) Implement graceful shutdown to allow running jobs to complete. (5) Add configuration for job schedule (environment variable for cron expression, default '0 0 2 * * *'). Include error handling and retry logic for database failures.",
          "id": 7,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: manually trigger job and verify tasks older than 30 days are hard deleted, verify tasks within 30-day window are preserved, test job scheduling configuration, monitor logs for execution confirmation and deleted counts",
          "title": "Create background job for hard deletion after 30-day retention",
          "updatedAt": "2025-12-06T18:32:23.895200796Z"
        }
      ],
      "testStrategy": "Integration tests: create task, verify in list. Filter by status, assignee, date range. Update task status. Soft delete task, verify not in list but exists in DB. Test hard delete after 30 days. Verify member can create, only admin can delete. Test assignee validation.",
      "title": "Implement Task CRUD endpoints with filtering and soft delete"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "23"
      ],
      "description": "Create WebSocket connection handler that broadcasts task changes to team members using Redis pub/sub for horizontal scaling.",
      "details": "1. Add dependency: axum-extra = { version = \"0.9\", features = [\"typed-header\"] }\n2. Create domain/events.rs:\n   - TaskEvent enum (Created, Updated, Deleted, StatusChanged)\n   - Serialize to JSON for Redis pub/sub\n3. Implement infra/pubsub.rs:\n   - subscribe_to_team(team_id) -> redis::PubSub\n   - publish_task_event(team_id, event: TaskEvent) -> Result<()>\n   - Channel format: team:{team_id}:tasks\n4. Create api/ws.rs:\n   - GET /api/teams/:team_id/ws (upgrade to WebSocket)\n   - Validate JWT from query param or Sec-WebSocket-Protocol header\n   - Verify user is team member\n   - Subscribe to Redis channel for team\n   - Forward events to WebSocket client\n   - Handle ping/pong for connection health\n5. Modify task handlers to publish events after mutations\n6. Implement connection manager to track active connections per team\n7. Add graceful shutdown handling",
      "id": "24",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:33:10.737535679Z",
          "dependencies": [],
          "description": "Create the WebSocket endpoint that handles HTTP upgrade requests, extracts and validates JWT tokens from query parameters or Sec-WebSocket-Protocol header, and establishes WebSocket connections for authenticated team members.",
          "details": "Add axum-extra dependency with typed-header feature. Create api/ws.rs with GET /api/teams/:team_id/ws endpoint. Implement connection upgrade logic using axum::extract::ws::WebSocketUpgrade. Extract JWT from query string (?token=...) or Sec-WebSocket-Protocol header. Validate JWT and extract user_id. Verify user is a member of the requested team_id by querying the database. Return 401 Unauthorized if authentication fails or 403 Forbidden if not a team member. On success, upgrade to WebSocket protocol and return the WebSocket response.",
          "id": 1,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for JWT extraction from both query and header. Integration tests for authentication flow with valid/invalid tokens. Test team membership validation. Verify proper HTTP status codes for various error scenarios.",
          "title": "Implement WebSocket connection upgrade handler with JWT authentication",
          "updatedAt": "2025-12-06T18:33:10.737535679Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737537887Z",
          "dependencies": [],
          "description": "Create the event domain model that represents different types of task changes, ensuring proper serialization for Redis pub/sub message transmission.",
          "details": "Create domain/events.rs module. Define TaskEvent enum with variants: Created { task_id, task_data }, Updated { task_id, changes }, Deleted { task_id }, StatusChanged { task_id, old_status, new_status }. Derive Serialize and Deserialize traits from serde. Include timestamp and user_id fields in each variant for audit trail. Add helper methods to_json() and from_json() for converting to/from JSON strings. Include team_id in the event payload for routing validation. Add comprehensive documentation for each event type.",
          "id": 2,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for serialization/deserialization of each event variant. Test JSON format compatibility. Verify all required fields are present. Test edge cases with special characters and large payloads.",
          "title": "Design and implement TaskEvent domain model with JSON serialization",
          "updatedAt": "2025-12-06T18:33:10.737537887Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737538887Z",
          "dependencies": [
            "2"
          ],
          "description": "Implement the Redis publish/subscribe mechanism for distributing task events across multiple server instances, with proper channel naming strategy for team isolation.",
          "details": "Create infra/pubsub.rs module. Implement RedisPublisher struct with publish_task_event(team_id: i64, event: TaskEvent) -> Result<()> method. Implement RedisSubscriber struct with subscribe_to_team(team_id: i64) -> Result<redis::aio::PubSub> method. Use channel naming format: team:{team_id}:tasks for proper isolation. Configure Redis connection pool for pub/sub operations (separate from regular Redis operations). Add error handling for Redis connection failures. Implement reconnection logic with exponential backoff. Add logging for published and received events.",
          "id": 3,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with real Redis instance. Test publishing and subscribing to multiple team channels. Verify channel isolation between teams. Test reconnection logic by simulating Redis failures. Performance test with high message throughput.",
          "title": "Build Redis pub/sub infrastructure for task events",
          "updatedAt": "2025-12-06T18:33:10.737538887Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737540346Z",
          "dependencies": [
            "1"
          ],
          "description": "Create a thread-safe connection manager that tracks active WebSocket connections per team, enabling efficient message broadcasting and connection lifecycle management.",
          "details": "Create api/ws/connection_manager.rs module. Implement ConnectionManager struct using Arc<RwLock<HashMap<TeamId, HashMap<ConnectionId, WebSocketSender>>>>. Add methods: add_connection(team_id, connection_id, sender), remove_connection(team_id, connection_id), get_team_connections(team_id), broadcast_to_team(team_id, message). Use UUID for connection_id. Implement connection count tracking per team. Add method to get all active teams for monitoring. Ensure thread-safety for concurrent access. Add graceful handling when broadcasting to closed connections.",
          "id": 4,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for concurrent add/remove operations. Test broadcasting to multiple connections. Verify proper cleanup of disconnected clients. Test memory leaks with connection churn. Load test with thousands of concurrent connections.",
          "title": "Implement connection manager for tracking active WebSocket connections",
          "updatedAt": "2025-12-06T18:33:10.737540346Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737541012Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Modify existing task CRUD handlers to publish TaskEvent messages to Redis after successful database mutations, ensuring non-blocking operation.",
          "details": "Update api/tasks.rs handlers: create_task, update_task, delete_task, update_task_status. After successful database commit, publish corresponding TaskEvent (Created, Updated, Deleted, StatusChanged) using RedisPublisher. Use tokio::spawn to publish events asynchronously to avoid blocking the HTTP response. Add error logging for failed event publishing but don't fail the HTTP request. Extract team_id from the task or request context. Include full task data in Created events, only changed fields in Updated events. Add configuration flag to enable/disable event publishing for testing.",
          "id": 5,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests verifying events are published after mutations. Test that HTTP responses are not delayed by event publishing. Verify events contain correct data. Test behavior when Redis is unavailable (should log error but complete request). Test event ordering for rapid consecutive updates.",
          "title": "Integrate event publishing into task mutation handlers",
          "updatedAt": "2025-12-06T18:33:10.737541012Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737541887Z",
          "dependencies": [
            "3",
            "4"
          ],
          "description": "Create the core message forwarding logic that receives events from Redis pub/sub and broadcasts them to connected WebSocket clients for the corresponding team.",
          "details": "In api/ws.rs, implement message forwarding loop. After WebSocket upgrade, spawn async task that: 1) Subscribes to Redis channel for team using RedisSubscriber, 2) Receives messages from Redis pub/sub, 3) Deserializes TaskEvent from JSON, 4) Validates event team_id matches connection team_id, 5) Serializes event to WebSocket message format, 6) Sends to WebSocket client using sender. Handle Redis subscription errors with reconnection. Register connection with ConnectionManager. Remove connection on disconnect or error. Add structured logging for message flow. Implement message queuing if client is slow to consume.",
          "id": 6,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with mock Redis pub/sub. Test message delivery to single and multiple clients. Verify filtering by team_id. Test behavior when client disconnects during message send. Test message ordering preservation. Load test with high message rates.",
          "title": "Implement WebSocket message forwarding from Redis to clients",
          "updatedAt": "2025-12-06T18:33:10.737541887Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737542221Z",
          "dependencies": [
            "6"
          ],
          "description": "Implement connection health checks using WebSocket ping/pong frames to detect and clean up stale connections, ensuring reliable real-time communication.",
          "details": "In api/ws.rs, implement ping/pong mechanism. Send ping frames every 30 seconds using tokio::time::interval. Set pong timeout to 10 seconds. Track last pong received timestamp per connection. Close connection if pong not received within timeout. Handle incoming pong frames from client. Add connection state tracking (Connected, PingSent, PongReceived). Implement automatic reconnection hints in close frame. Add metrics for connection health (ping latency, timeout count). Log connection health events for monitoring.",
          "id": 7,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for ping/pong timing logic. Integration tests simulating slow/unresponsive clients. Test connection closure after timeout. Verify proper cleanup in ConnectionManager. Test with flaky network conditions using network simulation tools.",
          "title": "Add WebSocket connection health monitoring with ping/pong",
          "updatedAt": "2025-12-06T18:33:10.737542221Z"
        },
        {
          "createdAt": "2025-12-06T18:33:10.737554137Z",
          "dependencies": [
            "6",
            "7"
          ],
          "description": "Add proper shutdown handling for WebSocket connections and Redis subscriptions, with client reconnection guidance to ensure zero data loss during deployments.",
          "details": "Implement graceful shutdown in api/ws.rs. Listen for shutdown signal (SIGTERM/SIGINT). On shutdown: 1) Stop accepting new WebSocket connections, 2) Send close frame with reconnect code (1012 Service Restart) to all active connections, 3) Wait up to 30 seconds for connections to close gracefully, 4) Unsubscribe from all Redis channels, 5) Close Redis pub/sub connections. Add shutdown coordination with ConnectionManager. Implement drain mode where existing connections are maintained but new ones rejected. Add client reconnection logic guidance in API documentation. Log shutdown progress for monitoring. Ensure no events are lost during shutdown by buffering.",
          "id": 8,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for shutdown sequence. Test connection closure with various client behaviors. Verify Redis cleanup. Test with active message flow during shutdown. Verify no event loss during graceful shutdown. Test forced shutdown after timeout.",
          "title": "Implement graceful shutdown and reconnection handling",
          "updatedAt": "2025-12-06T18:33:10.737554137Z"
        }
      ],
      "testStrategy": "Integration tests: connect WebSocket as team member, create task via REST API, verify WebSocket receives event. Test unauthorized connection rejected. Test connection survives network interruptions with ping/pong. Load test 1000 concurrent connections. Verify events only sent to team members.",
      "title": "Implement WebSocket endpoint for real-time task updates"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "23"
      ],
      "description": "Set up email notification system for task mentions and due date reminders with user-configurable preferences using background job queue.",
      "details": "1. Add dependencies: lettre = \"0.11\", tokio-cron-scheduler = \"0.9\"\n2. Create domain/notification.rs:\n   - NotificationType enum (Mention, DueDateReminder, TaskAssigned)\n   - NotificationPreferences struct (email_enabled, mention_enabled, reminder_enabled)\n3. Add notification_preferences column to users table (JSONB)\n4. Implement infra/email.rs:\n   - EmailService using lettre SMTP\n   - send_task_mention(to, task, mentioned_by) -> Result<()>\n   - send_due_date_reminder(to, task) -> Result<()>\n   - HTML email templates in templates/ directory\n5. Create api/notifications.rs:\n   - GET /api/users/me/notification-preferences\n   - PATCH /api/users/me/notification-preferences\n6. Implement background jobs:\n   - Parse task description for @mentions on create/update\n   - Cron job (runs hourly) to check tasks due in next 24 hours\n   - Queue jobs in Redis list: LPUSH email_queue {job_json}\n   - Worker process: BRPOP email_queue, send email, retry on failure\n7. Add SMTP config: SMTP_HOST, SMTP_PORT, SMTP_USERNAME, SMTP_PASSWORD",
      "id": "25",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:34:07.844738469Z",
          "dependencies": [],
          "description": "Implement the core email service infrastructure using lettre for SMTP communication, including configuration management and connection handling.",
          "details": "Add lettre = \"0.11\" dependency to Cargo.toml. Create infra/email.rs with EmailService struct that encapsulates SMTP configuration (host, port, username, password). Implement connection pooling or reusable transport. Add environment variables: SMTP_HOST, SMTP_PORT, SMTP_USERNAME, SMTP_PASSWORD, SMTP_FROM_EMAIL. Create EmailService::new() constructor that validates configuration and tests SMTP connection. Implement send_raw_email(to: &str, subject: &str, html_body: &str) -> Result<(), EmailError> as the base sending method. Add proper error handling for connection failures, authentication errors, and send failures. Include logging for all email operations.",
          "id": 1,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mock SMTP server. Test connection failure scenarios, authentication errors, and successful email sending. Verify configuration validation and error propagation.",
          "title": "Set up email service with SMTP configuration and lettre integration",
          "updatedAt": "2025-12-06T18:34:07.844738469Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844740011Z",
          "dependencies": [
            "1"
          ],
          "description": "Design and implement responsive HTML email templates for task mention notifications and due date reminders with proper styling and dynamic content.",
          "details": "Create templates/ directory structure. Implement templates/email_mention.html with placeholders for {mentioned_by_name}, {task_title}, {task_description}, {task_url}. Implement templates/email_due_date_reminder.html with placeholders for {task_title}, {task_description}, {due_date}, {task_url}, {hours_remaining}. Use inline CSS for email client compatibility. Include responsive design for mobile devices. Add email header with logo/branding and footer with unsubscribe link. Create a template rendering function render_template(template_name: &str, context: HashMap<String, String>) -> Result<String> that replaces placeholders. Implement send_task_mention(to: &str, task: &Task, mentioned_by: &User) and send_due_date_reminder(to: &str, task: &Task) methods in EmailService that use these templates.",
          "id": 2,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Visual testing of rendered HTML in multiple email clients (Gmail, Outlook, Apple Mail). Unit tests for template rendering with various context values. Verify all placeholders are correctly replaced.",
          "title": "Create HTML email templates for mentions and due date reminders",
          "updatedAt": "2025-12-06T18:34:07.844740011Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844740511Z",
          "dependencies": [],
          "description": "Add notification preferences data model to users table and create API endpoints for users to view and update their notification settings.",
          "details": "Create domain/notification.rs with NotificationType enum (Mention, DueDateReminder, TaskAssigned). Define NotificationPreferences struct with fields: email_enabled: bool, mention_enabled: bool, reminder_enabled: bool, reminder_hours_before: u32. Add migration to create notification_preferences column in users table as JSONB type with default values {\"email_enabled\": true, \"mention_enabled\": true, \"reminder_enabled\": true, \"reminder_hours_before\": 24}. Update User model to include notification_preferences field. Create api/notifications.rs with GET /api/users/me/notification-preferences endpoint (returns current preferences) and PATCH /api/users/me/notification-preferences endpoint (updates preferences with validation). Add request/response DTOs: NotificationPreferencesResponse and UpdateNotificationPreferencesRequest. Implement validation: reminder_hours_before must be between 1 and 168 hours.",
          "id": 3,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for API endpoints with authenticated users. Test GET returns correct preferences, PATCH updates database correctly. Verify validation rules for reminder_hours_before. Test unauthorized access returns 401.",
          "title": "Implement notification preferences schema and API endpoints",
          "updatedAt": "2025-12-06T18:34:07.844740511Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844740636Z",
          "dependencies": [
            "3"
          ],
          "description": "Create a parser to detect @username mentions in task descriptions and extract mentioned users for notification triggering.",
          "details": "Create domain/mention_parser.rs with parse_mentions(text: &str) -> Vec<String> function that uses regex to find @username patterns (alphanumeric and underscores). Implement resolve_mentions(usernames: Vec<String>, db: &DbPool) -> Result<Vec<User>> to look up actual users by username. Add mention detection to task creation and update handlers in api/tasks.rs. When task is created or description updated, call parse_mentions() and resolve_mentions(). Store mentioned user IDs for job queue. Handle edge cases: invalid usernames, deleted users, self-mentions (skip), duplicate mentions. Implement get_new_mentions(old_description: Option<&str>, new_description: &str) -> Vec<String> to only notify on newly added mentions during updates.",
          "id": 4,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for mention parsing with various patterns: single mention, multiple mentions, mentions at different positions, invalid patterns. Test resolve_mentions with mock database. Verify get_new_mentions correctly identifies only new mentions.",
          "title": "Implement mention parsing logic for task descriptions",
          "updatedAt": "2025-12-06T18:34:07.844740636Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844741053Z",
          "dependencies": [],
          "description": "Set up Redis job queue infrastructure for queuing email notification jobs with proper serialization and error handling.",
          "details": "Add redis = \"0.24\" dependency. Create infra/job_queue.rs with JobQueue struct wrapping Redis connection. Define EmailJob enum with variants: SendMention { to_user_id, task_id, mentioned_by_user_id }, SendDueDateReminder { to_user_id, task_id }. Implement Serialize/Deserialize for EmailJob with job_id (UUID), job_type, payload, created_at, retry_count. Create JobQueue::push(job: EmailJob) -> Result<()> that serializes job to JSON and executes LPUSH email_queue {json}. Implement JobQueue::pop(timeout_seconds: u64) -> Result<Option<EmailJob>> using BRPOP email_queue {timeout}. Add JobQueue::push_to_dlq(job: EmailJob, error: &str) for failed jobs using LPUSH email_dlq. Include job deduplication using SET job:{job_id} EX 3600 to prevent duplicate processing within 1 hour window.",
          "id": 5,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with Redis. Test push/pop operations, job serialization/deserialization. Verify BRPOP timeout behavior. Test deduplication prevents duplicate job processing. Verify DLQ receives failed jobs.",
          "title": "Implement Redis-based job queue with LPUSH/BRPOP pattern",
          "updatedAt": "2025-12-06T18:34:07.844741053Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844741178Z",
          "dependencies": [
            "1",
            "2",
            "5"
          ],
          "description": "Implement a background worker process that consumes jobs from Redis queue, sends emails, and handles failures with retry mechanism.",
          "details": "Create bin/email_worker.rs as separate binary. Implement main loop: pop job from queue with 5-second timeout, process job, handle success/failure. Create process_email_job(job: EmailJob, email_service: &EmailService, db: &DbPool) -> Result<()> that: 1) loads user and checks notification preferences, 2) loads task details, 3) sends appropriate email via EmailService. Implement retry logic: if send fails and retry_count < 3, increment retry_count, add exponential backoff delay (2^retry_count minutes), push back to queue. If retry_count >= 3, push to DLQ with error details. Add graceful shutdown handling (SIGTERM/SIGINT). Include comprehensive logging for job processing, retries, and failures. Implement idempotency check using job_id to skip already-processed jobs. Add metrics tracking: jobs_processed, jobs_failed, jobs_retried.",
          "id": 6,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with mock EmailService and Redis. Test successful job processing, retry logic with transient failures, DLQ handling after max retries. Verify idempotency prevents duplicate sends. Test graceful shutdown.",
          "title": "Create background worker process for email sending with retry logic",
          "updatedAt": "2025-12-06T18:34:07.844741178Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844741636Z",
          "dependencies": [
            "3",
            "5"
          ],
          "description": "Create a scheduled job using tokio-cron-scheduler that runs hourly to identify tasks due soon and queue reminder notifications.",
          "details": "Add tokio-cron-scheduler = \"0.9\" dependency. Create bin/reminder_scheduler.rs or integrate into main application. Use tokio_cron_scheduler::JobScheduler to create job running every hour (\"0 0 * * * *\" cron expression). Implement scan_due_tasks() function that queries database for tasks where: due_date BETWEEN NOW() AND NOW() + INTERVAL '24 hours', status != 'completed', and task hasn't been reminded in last 23 hours (track in task_reminders table or Redis). For each task, load assignee user, check their notification preferences (reminder_enabled and reminder_hours_before), create SendDueDateReminder job and push to queue. Add task_reminders tracking table with columns: task_id, reminded_at to prevent duplicate reminders. Implement efficient database query with proper indexes on due_date and status. Add logging for scan results: tasks found, reminders queued.",
          "id": 7,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with test database. Create tasks with various due dates and verify correct tasks are selected. Test preference checking filters out users with reminders disabled. Verify deduplication prevents multiple reminders. Test cron expression triggers at correct intervals.",
          "title": "Implement cron job for due date reminder scanning",
          "updatedAt": "2025-12-06T18:34:07.844741636Z"
        },
        {
          "createdAt": "2025-12-06T18:34:07.844742178Z",
          "dependencies": [
            "4",
            "6",
            "7"
          ],
          "description": "Wire up the complete notification system by integrating preference checks before sending notifications and creating mention jobs when tasks are created or updated.",
          "details": "Update api/tasks.rs create_task and update_task handlers to: 1) after task save, call parse_mentions and resolve_mentions, 2) for each mentioned user, check their notification_preferences.mention_enabled, 3) if enabled, create SendMention job with task_id, to_user_id, mentioned_by_user_id and push to job queue. In email_worker process_email_job, add preference checking: load user's notification_preferences from database, verify email_enabled is true and specific notification type is enabled before sending. If preferences disabled, mark job as skipped (don't retry). Add integration between all components: task API -> mention parser -> job queue -> worker -> email service. Implement end-to-end logging to trace notification flow from task creation to email sent. Add configuration flag NOTIFICATIONS_ENABLED to allow disabling entire system in development/testing.",
          "id": 8,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "End-to-end integration tests: create task with mentions, verify jobs queued, worker processes jobs, emails sent. Test with preferences disabled, verify no emails sent. Test task updates only notify new mentions. Verify complete flow from API request to email delivery with all preference checks.",
          "title": "Integrate preference checking and mention job creation in task workflows",
          "updatedAt": "2025-12-06T18:34:07.844742178Z"
        }
      ],
      "testStrategy": "Unit tests for mention parsing. Integration tests: create task with @mention, verify email sent if preference enabled. Test due date reminder job finds tasks due tomorrow. Test preference updates respected. Mock SMTP in tests. Verify email queue processes jobs and retries failures.",
      "title": "Implement email notifications with user preferences"
    },
    {
      "agentHint": "tap",
      "dependencies": [
        "23"
      ],
      "description": "Integrate Firebase Cloud Messaging for mobile push notifications with device token management and notification delivery.",
      "details": "1. Add dependency: fcm = \"0.9\"\n2. Create migrations/006_device_tokens.sql:\n   - id (UUID PK), user_id (FK users), token (UNIQUE), platform (ENUM: ios, android), created_at, last_used_at\n3. Implement infra/push.rs:\n   - FcmService with API key from config\n   - send_push_notification(user_id, title, body, data) -> Result<()>\n   - Fetch active device tokens for user (last_used_at within 30 days)\n   - Handle invalid token responses, delete from DB\n4. Create api/devices.rs:\n   - POST /api/devices/register (requires auth) -> stores FCM token\n   - DELETE /api/devices/:token (requires auth) -> removes token\n5. Integrate with task events:\n   - Send push on task assignment\n   - Send push on @mention\n   - Respect user notification preferences\n6. Add FCM_API_KEY to config\n7. Include deep link data in notification payload: {task_id, team_id}",
      "id": "26",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:34:51.992689670Z",
          "dependencies": [],
          "description": "Design and implement the database schema for storing FCM device tokens with user associations, platform information, and usage tracking. Create migration file 006_device_tokens.sql with proper indexes and constraints.",
          "details": "Create migrations/006_device_tokens.sql with the following structure:\n- id: UUID primary key\n- user_id: Foreign key reference to users table with CASCADE delete\n- token: TEXT field with UNIQUE constraint for FCM device token\n- platform: ENUM type with values ('ios', 'android')\n- created_at: TIMESTAMP with default NOW()\n- last_used_at: TIMESTAMP with default NOW()\n\nAdd indexes:\n- Index on user_id for efficient user token lookups\n- Index on last_used_at for filtering active tokens\n- Unique index on token to prevent duplicates\n\nInclude proper foreign key constraints and ensure the migration is reversible with a DOWN migration.",
          "id": 1,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test migration up/down execution, verify constraints work correctly (unique token, valid platform enum values), test CASCADE delete when user is removed, verify indexes are created properly",
          "title": "Create device token database schema and migration",
          "updatedAt": "2025-12-06T18:34:51.992689670Z"
        },
        {
          "createdAt": "2025-12-06T18:34:51.992691045Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the FcmService in infra/push.rs that handles Firebase Cloud Messaging integration, including API key configuration, HTTP client setup, and basic FCM API communication structure.",
          "details": "Add fcm = \"0.9\" dependency to Cargo.toml.\n\nCreate infra/push.rs with:\n- FcmService struct containing HTTP client and API key\n- Constructor that reads FCM_API_KEY from config/environment\n- Error types for FCM-specific failures (InvalidToken, NetworkError, AuthError)\n- Helper method to construct FCM API request headers with Authorization bearer token\n- Basic request/response structures matching FCM v1 API format\n\nAdd FCM_API_KEY to config files (config.toml, .env.example) with documentation.\n\nImplement connection validation method to verify API key is valid on service initialization.",
          "id": 2,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit test FcmService initialization with valid/invalid API keys, mock HTTP client to test request header construction, verify error handling for missing configuration",
          "title": "Implement FCM service with API key configuration",
          "updatedAt": "2025-12-06T18:34:51.992691045Z"
        },
        {
          "createdAt": "2025-12-06T18:34:51.992691628Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement REST API endpoints in api/devices.rs for registering new device tokens and removing tokens when devices are logged out or uninstalled.",
          "details": "Create api/devices.rs with authenticated endpoints:\n\nPOST /api/devices/register:\n- Require authentication middleware\n- Request body: {token: String, platform: Enum('ios'|'android')}\n- Validate token format and platform value\n- Upsert device token (INSERT ON CONFLICT UPDATE last_used_at)\n- Return 201 Created with device record\n- Handle duplicate token errors gracefully\n\nDELETE /api/devices/:token:\n- Require authentication middleware\n- Verify token belongs to authenticated user\n- Delete token from database\n- Return 204 No Content on success\n- Return 404 if token not found or doesn't belong to user\n\nAdd request/response DTOs and validation logic. Include rate limiting to prevent abuse.",
          "id": 3,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for both endpoints with authentication, test duplicate token registration updates last_used_at, verify user can only delete their own tokens, test validation errors for invalid platform values, test rate limiting",
          "title": "Create device registration and deletion API endpoints",
          "updatedAt": "2025-12-06T18:34:51.992691628Z"
        },
        {
          "createdAt": "2025-12-06T18:34:51.992692337Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Create the core push notification sending logic that fetches all active device tokens for a user, constructs FCM payloads with deep link data, and sends notifications to multiple devices with proper error handling.",
          "details": "Implement in infra/push.rs:\n\nsend_push_notification(user_id, title, body, data) -> Result<()>:\n- Query database for device tokens WHERE user_id = ? AND last_used_at > NOW() - INTERVAL '30 days'\n- For each active token:\n  - Construct FCM message payload with:\n    - notification: {title, body}\n    - data: {task_id, team_id, notification_type} for deep linking\n    - platform-specific options (priority, sound, badge)\n  - Send HTTP POST to FCM API endpoint\n  - Collect results (success/failure per token)\n- Return aggregated result with success count and failures\n\nImplement retry logic for transient failures (network errors, rate limits).\nAdd logging for notification delivery tracking.\nHandle batch sending if user has many devices (chunk into groups of 100).",
          "id": 4,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mocked FCM API responses, test multi-device scenarios (0, 1, 5+ devices), verify deep link data construction, test retry logic for transient failures, verify 30-day token filtering works correctly, integration test with FCM sandbox",
          "title": "Implement push notification sending with multi-device support",
          "updatedAt": "2025-12-06T18:34:51.992692337Z"
        },
        {
          "createdAt": "2025-12-06T18:34:51.992692587Z",
          "dependencies": [
            "4"
          ],
          "description": "Add logic to process FCM API responses and automatically remove invalid or expired device tokens from the database to maintain data hygiene and prevent repeated failed delivery attempts.",
          "details": "Extend send_push_notification in infra/push.rs:\n\n- Parse FCM response for each token to identify:\n  - InvalidRegistration errors (token format invalid)\n  - NotRegistered errors (token unregistered/expired)\n  - Other permanent failures\n- For invalid/unregistered tokens:\n  - Delete from device_tokens table immediately\n  - Log deletion with reason for audit trail\n- For successful deliveries:\n  - Update last_used_at timestamp to track active tokens\n- Implement cleanup_stale_tokens() background task:\n  - Runs periodically (daily)\n  - Removes tokens with last_used_at > 90 days\n  - Logs cleanup statistics\n\nAdd metrics/monitoring for token cleanup rates to detect issues.",
          "id": 5,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for FCM error response parsing, test token deletion on InvalidRegistration/NotRegistered errors, verify last_used_at updates on successful delivery, test cleanup_stale_tokens removes old tokens, mock various FCM error scenarios",
          "title": "Implement invalid token cleanup based on FCM responses",
          "updatedAt": "2025-12-06T18:34:51.992692587Z"
        },
        {
          "createdAt": "2025-12-06T18:34:51.992693128Z",
          "dependencies": [
            "4",
            "5"
          ],
          "description": "Connect the push notification system to task lifecycle events (assignment, mentions) and implement user notification preference checking to ensure notifications are only sent when appropriate.",
          "details": "Create notification trigger integration:\n\n1. In task assignment logic:\n   - After successful task assignment, call send_push_notification\n   - Title: \"New Task Assigned\"\n   - Body: \"You've been assigned to: {task_title}\"\n   - Data: {task_id, team_id, type: 'assignment'}\n\n2. In comment/mention handling:\n   - Parse @mentions from comment body\n   - For each mentioned user, call send_push_notification\n   - Title: \"{mentioner_name} mentioned you\"\n   - Body: Preview of comment text (truncated to 100 chars)\n   - Data: {task_id, team_id, comment_id, type: 'mention'}\n\n3. Implement preference checking:\n   - Query user notification preferences before sending\n   - Check push_enabled, assignment_notifications, mention_notifications flags\n   - Skip sending if user has disabled relevant notification type\n   - Add preference management endpoints if not existing\n\nEnsure notifications are sent asynchronously (background job) to avoid blocking request handlers.",
          "id": 6,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for task assignment triggering push notification, test @mention detection and notification, verify preference checking prevents unwanted notifications, test notification is skipped when push_enabled=false, verify async/background job execution, test deep link data is correctly formatted for mobile app navigation",
          "title": "Integrate push notifications with task events and user preferences",
          "updatedAt": "2025-12-06T18:34:51.992693128Z"
        }
      ],
      "testStrategy": "Integration tests with FCM sandbox. Register device token, create task assigned to user, verify push sent. Test invalid token cleanup. Verify preferences disable push. Test multiple devices per user. Manual testing on iOS/Android devices with deep links.",
      "title": "Implement FCM push notifications for mobile"
    },
    {
      "agentHint": "bolt",
      "dependencies": [
        "16"
      ],
      "description": "Implement Prometheus metrics endpoint, structured JSON logging with trace IDs, and health check endpoints for Kubernetes.",
      "details": "1. Add dependencies:\n   - prometheus = \"0.13\"\n   - tracing = \"0.1\"\n   - tracing-subscriber = { version = \"0.3\", features = [\"json\", \"env-filter\"] }\n   - uuid = { version = \"1\", features = [\"v4\"] }\n2. Create infra/metrics.rs:\n   - Register metrics: http_requests_total (counter), http_request_duration_seconds (histogram), active_websocket_connections (gauge), task_operations_total (counter)\n   - Middleware to record metrics on each request\n3. Implement infra/tracing.rs:\n   - Initialize tracing_subscriber with JSON formatter\n   - Generate trace_id (UUID v4) per request, inject into logs\n   - Log level from RUST_LOG env var\n4. Add endpoints in api/health.rs:\n   - GET /health/live -> 200 OK (liveness probe)\n   - GET /health/ready -> checks DB and Redis connections, 200 if healthy\n   - GET /metrics -> Prometheus text format\n5. Add trace_id to response headers: X-Trace-Id\n6. Log format: {\"timestamp\", \"level\", \"trace_id\", \"message\", \"fields\"}\n7. Instrument key functions with tracing spans",
      "id": "27",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:35:41.976574096Z",
          "dependencies": [],
          "description": "Add Prometheus dependencies and create the metrics module with registration of core application metrics including HTTP request counters, duration histograms, WebSocket connection gauges, and task operation counters.",
          "details": "1. Add dependencies to Cargo.toml: prometheus = \"0.13\", lazy_static = \"1.4\" for static registry\n2. Create src/infra/metrics.rs module\n3. Initialize Prometheus registry and register metrics:\n   - http_requests_total: Counter with labels [method, path, status]\n   - http_request_duration_seconds: Histogram with labels [method, path] and buckets [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0]\n   - active_websocket_connections: Gauge (no labels)\n   - task_operations_total: Counter with labels [operation, status]\n4. Create public functions to access each metric: get_http_requests_counter(), get_http_duration_histogram(), get_websocket_gauge(), get_task_operations_counter()\n5. Export a get_registry() function to access the Prometheus registry for metrics exposition",
          "id": 1,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests to verify metrics are properly registered in the registry, test incrementing counters and observing histograms, verify metric labels are correctly applied",
          "title": "Setup Prometheus metrics infrastructure and register core metrics",
          "updatedAt": "2025-12-06T18:35:41.976574096Z"
        },
        {
          "createdAt": "2025-12-06T18:35:41.976574805Z",
          "dependencies": [
            "1"
          ],
          "description": "Create Axum middleware that automatically records HTTP request metrics including request count, duration, and status codes for all incoming requests with minimal performance overhead.",
          "details": "1. Create middleware function in src/infra/metrics.rs: metrics_middleware()\n2. Use tower/axum middleware pattern to wrap requests\n3. On request start: record timestamp using std::time::Instant\n4. On request completion:\n   - Calculate duration in seconds\n   - Increment http_requests_total counter with labels: method (GET/POST/etc), path (route pattern), status (200/404/500/etc)\n   - Observe http_request_duration_seconds histogram with method and path labels\n5. Handle errors gracefully to ensure metrics recording doesn't fail requests\n6. Extract route pattern from Axum MatchedPath extension to avoid high cardinality in path labels\n7. Add middleware to main application router before route handlers",
          "id": 2,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests making HTTP requests and verifying metrics are incremented, test various status codes (2xx, 4xx, 5xx), verify duration histogram buckets are populated correctly, load test to ensure minimal overhead (<1ms per request)",
          "title": "Implement HTTP metrics collection middleware",
          "updatedAt": "2025-12-06T18:35:41.976574805Z"
        },
        {
          "createdAt": "2025-12-06T18:35:41.976575055Z",
          "dependencies": [],
          "description": "Configure tracing-subscriber with JSON formatting for structured logs, implement log level configuration via environment variables, and establish the foundation for distributed tracing.",
          "details": "1. Add dependencies to Cargo.toml:\n   - tracing = \"0.1\"\n   - tracing-subscriber = { version = \"0.3\", features = [\"json\", \"env-filter\"] }\n2. Create src/infra/tracing.rs module\n3. Implement init_tracing() function:\n   - Create JsonLayer for structured JSON output with fields: timestamp (RFC3339), level, target, message, fields\n   - Configure EnvFilter to read RUST_LOG environment variable (default: \"info\")\n   - Set global subscriber using tracing_subscriber::registry()\n4. Format configuration:\n   - Use UTC timestamps in ISO 8601 format\n   - Include span fields in log output\n   - Flatten nested fields for easier parsing\n5. Call init_tracing() early in main() before any other initialization\n6. Add tracing macros (info!, warn!, error!, debug!) to replace println! statements",
          "id": 3,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify JSON output format with test subscriber, test different log levels via RUST_LOG, validate timestamp format, ensure all required fields are present in output, test that log filtering works correctly",
          "title": "Setup structured logging with tracing-subscriber and JSON formatting",
          "updatedAt": "2025-12-06T18:35:41.976575055Z"
        },
        {
          "createdAt": "2025-12-06T18:35:41.976575096Z",
          "dependencies": [
            "3"
          ],
          "description": "Add UUID v4 trace ID generation per request, propagate trace IDs through tracing spans, inject trace IDs into log entries, and include trace IDs in HTTP response headers for request correlation.",
          "details": "1. Add dependency: uuid = { version = \"1\", features = [\"v4\"] }\n2. Create trace ID middleware in src/infra/tracing.rs:\n   - Generate UUID v4 for each incoming request\n   - Check for existing X-Trace-Id header and use if present (for distributed tracing)\n   - Create root span for request with trace_id field\n3. Store trace_id in request extensions for access by handlers\n4. Configure tracing-subscriber to include trace_id in all log entries within the span\n5. Add response middleware to inject X-Trace-Id header in all responses\n6. Create helper function get_trace_id() to retrieve current trace ID from span context\n7. Instrument async functions with #[tracing::instrument] macro to automatically propagate trace context\n8. Add trace_id field to JSON log output format",
          "id": 4,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test trace ID generation and uniqueness, verify trace ID appears in logs for request lifecycle, test trace ID in response headers, verify trace ID propagation through async boundaries, test existing trace ID is preserved when provided in request",
          "title": "Implement trace ID generation and propagation throughout request lifecycle",
          "updatedAt": "2025-12-06T18:35:41.976575096Z"
        },
        {
          "createdAt": "2025-12-06T18:35:41.976575180Z",
          "dependencies": [],
          "description": "Create liveness and readiness probe endpoints that check application and dependency health, including database and Redis connectivity checks with proper timeout handling and failure isolation.",
          "details": "1. Create src/api/health.rs module\n2. Implement GET /health/live endpoint:\n   - Simple 200 OK response with {\"status\": \"alive\"}\n   - No dependency checks (indicates process is running)\n   - Should always succeed unless process is deadlocked\n3. Implement GET /health/ready endpoint:\n   - Check database connection: execute simple SELECT 1 query with 2-second timeout\n   - Check Redis connection: execute PING command with 2-second timeout\n   - Return 200 OK with {\"status\": \"ready\", \"checks\": {\"database\": \"ok\", \"redis\": \"ok\"}} if all healthy\n   - Return 503 Service Unavailable with details of failed checks if any dependency is down\n   - Use tokio::time::timeout to prevent hanging on slow dependencies\n4. Add error handling to prevent panic on connection failures\n5. Log health check failures at warn level with trace_id\n6. Register routes in main router",
          "id": 5,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test liveness endpoint always returns 200, test readiness with healthy dependencies returns 200, test readiness with database down returns 503, test readiness with Redis down returns 503, verify timeout behavior with slow connections, test concurrent health check requests don't interfere",
          "title": "Implement health check endpoints for Kubernetes probes",
          "updatedAt": "2025-12-06T18:35:41.976575180Z"
        },
        {
          "createdAt": "2025-12-06T18:35:41.976575263Z",
          "dependencies": [
            "1",
            "4"
          ],
          "description": "Create the /metrics endpoint that exposes metrics in Prometheus text format, and add tracing instrumentation to critical application functions for observability of business logic.",
          "details": "1. Implement GET /metrics endpoint in src/infra/metrics.rs:\n   - Use prometheus::TextEncoder to encode metrics\n   - Return metrics in Prometheus text exposition format\n   - Set Content-Type: text/plain; version=0.0.4\n   - Handle encoding errors gracefully\n2. Register /metrics route in main router (consider excluding from metrics middleware to avoid recursion)\n3. Instrument critical code paths with tracing spans:\n   - Add #[tracing::instrument] to handler functions with skip for large parameters\n   - Add spans around database queries: span!(\"db_query\", query = %query_name)\n   - Add spans around Redis operations: span!(\"redis_op\", operation = %op_type)\n   - Add spans around business logic operations: task creation, updates, deletions\n4. Record business metrics:\n   - Increment task_operations_total on task CRUD operations with operation and status labels\n   - Update active_websocket_connections gauge on connection/disconnection\n5. Add span events for significant operations: span.record(\"result\", &result)\n6. Test metrics endpoint accessibility and format",
          "id": 6,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test /metrics endpoint returns valid Prometheus format, verify metrics appear in output after operations, test with Prometheus scraper or promtool check, verify instrumented spans appear in logs with trace_id, test business metrics are incremented correctly, load test to ensure instrumentation overhead is acceptable",
          "title": "Implement Prometheus metrics exposition endpoint and instrument critical code paths",
          "updatedAt": "2025-12-06T18:35:41.976575263Z"
        }
      ],
      "testStrategy": "Unit tests for metrics recording. Integration tests: make requests, verify metrics incremented. Check /health/ready returns 503 when DB unavailable. Verify logs are valid JSON with trace_ids. Load test and verify metrics in Prometheus. Test log filtering with RUST_LOG.",
      "title": "Setup observability with Prometheus metrics and structured logging"
    },
    {
      "agentHint": "bolt",
      "dependencies": [
        "27"
      ],
      "description": "Build optimized Docker image with multi-stage build and create Kubernetes deployment with HPA, ConfigMaps, and Secrets.",
      "details": "1. Create Dockerfile:\n   ```dockerfile\n   FROM rust:1.75 as builder\n   WORKDIR /app\n   COPY Cargo.* ./\n   RUN mkdir src && echo \"fn main() {}\" > src/main.rs && cargo build --release\n   COPY src ./src\n   RUN touch src/main.rs && cargo build --release\n   \n   FROM debian:bookworm-slim\n   RUN apt-get update && apt-get install -y ca-certificates && rm -rf /var/lib/apt/lists/*\n   COPY --from=builder /app/target/release/teamsync-api /usr/local/bin/\n   EXPOSE 8080\n   CMD [\"teamsync-api\"]\n   ```\n2. Create infra/k8s/namespace.yaml\n3. Create infra/k8s/configmap.yaml (non-sensitive config)\n4. Create infra/k8s/secret.yaml (JWT_SECRET, DB credentials, etc.)\n5. Create infra/k8s/deployment.yaml:\n   - 3 replicas, resource limits (500m CPU, 512Mi memory)\n   - livenessProbe: /health/live, readinessProbe: /health/ready\n   - Environment variables from ConfigMap and Secret\n6. Create infra/k8s/service.yaml (ClusterIP, port 8080)\n7. Create infra/k8s/hpa.yaml:\n   - Min 3, max 10 replicas\n   - Target CPU 70%, memory 80%\n8. Create infra/k8s/ingress.yaml (HTTPS with cert-manager)",
      "id": "28",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:36:29.741727590Z",
          "dependencies": [],
          "description": "Implement a multi-stage Dockerfile that optimizes build time through dependency caching and minimizes final image size using debian:bookworm-slim base.",
          "details": "Create Dockerfile with two stages: (1) Builder stage using rust:1.75 that first copies only Cargo.toml/Cargo.lock, creates dummy main.rs, builds dependencies to cache them, then copies real source and builds release binary. (2) Runtime stage using debian:bookworm-slim with only ca-certificates installed, copying the compiled binary from builder. This approach caches dependencies separately from application code, significantly reducing rebuild times. Add .dockerignore to exclude target/, .git/, and other unnecessary files. Test build with 'docker build -t teamsync-api:latest .' and verify image size is under 100MB.",
          "id": 1,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Build Docker image locally and verify: (1) Image builds successfully, (2) Image size is optimized (under 100MB), (3) Rebuild with code changes only rebuilds application layer, not dependencies, (4) Container runs and responds to health checks",
          "title": "Create optimized multi-stage Dockerfile with dependency caching",
          "updatedAt": "2025-12-06T18:36:29.741727590Z"
        },
        {
          "createdAt": "2025-12-06T18:36:29.741730757Z",
          "dependencies": [],
          "description": "Define Kubernetes namespace for the application with resource quotas to ensure proper resource allocation and isolation.",
          "details": "Create infra/k8s/namespace.yaml defining namespace 'teamsync' with ResourceQuota limiting total CPU (10 cores), memory (20Gi), and pod count (50). Add LimitRange to set default container resource requests (100m CPU, 128Mi memory) and limits (1 CPU, 1Gi memory). Include labels for environment (production/staging) and team ownership. This ensures the application has dedicated resources and prevents resource exhaustion. Apply with 'kubectl apply -f infra/k8s/namespace.yaml'.",
          "id": 2,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Apply namespace manifest and verify: (1) Namespace is created successfully, (2) ResourceQuota is enforced, (3) LimitRange defaults are applied to new pods, (4) Labels are correctly set",
          "title": "Create Kubernetes namespace with resource quotas",
          "updatedAt": "2025-12-06T18:36:29.741730757Z"
        },
        {
          "createdAt": "2025-12-06T18:36:29.741731215Z",
          "dependencies": [
            "2"
          ],
          "description": "Define ConfigMap containing all non-sensitive configuration parameters for the application including logging levels, feature flags, and service endpoints.",
          "details": "Create infra/k8s/configmap.yaml in 'teamsync' namespace with key-value pairs for: LOG_LEVEL (info), RUST_LOG (teamsync_api=debug), PORT (8080), ENVIRONMENT (production), METRICS_ENABLED (true), API_TIMEOUT_SECONDS (30), MAX_CONNECTIONS (1000). Use data section for simple values and consider using a config file if configuration becomes complex. Include annotations for documentation and version tracking. This separates configuration from code, enabling environment-specific settings without rebuilding images.",
          "id": 3,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Apply ConfigMap and verify: (1) ConfigMap is created in correct namespace, (2) All key-value pairs are present, (3) ConfigMap can be referenced in pod specs, (4) Values can be updated without pod restart if needed",
          "title": "Create ConfigMap for non-sensitive application configuration",
          "updatedAt": "2025-12-06T18:36:29.741731215Z"
        },
        {
          "createdAt": "2025-12-06T18:36:29.741731965Z",
          "dependencies": [
            "2"
          ],
          "description": "Define Kubernetes Secret containing all sensitive credentials including JWT secrets, database credentials, and API keys using base64 encoding.",
          "details": "Create infra/k8s/secret.yaml in 'teamsync' namespace with type 'Opaque' containing base64-encoded values for: JWT_SECRET (random 64-char string), DATABASE_URL (postgres connection string), DATABASE_PASSWORD, REDIS_URL, API_KEY. Use 'echo -n \"value\" | base64' for encoding. Add stringData section with example values for documentation (to be replaced in actual deployment). Include annotations warning against committing real secrets to version control. Document that production secrets should be managed via sealed-secrets, external-secrets, or vault integration. Add RBAC rules limiting secret access to service account only.",
          "id": 4,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Apply Secret and verify: (1) Secret is created with correct type, (2) Values are properly base64 encoded, (3) Secret can be mounted as environment variables, (4) Secret data is not visible in kubectl describe output, (5) Only authorized service accounts can access",
          "title": "Create Secret for sensitive credentials and tokens",
          "updatedAt": "2025-12-06T18:36:29.741731965Z"
        },
        {
          "createdAt": "2025-12-06T18:36:29.741732465Z",
          "dependencies": [
            "1",
            "3",
            "4"
          ],
          "description": "Define Kubernetes Deployment with 3 replicas, health probes, resource limits, rolling update strategy, and environment variable injection from ConfigMap and Secret.",
          "details": "Create infra/k8s/deployment.yaml in 'teamsync' namespace with: 3 replicas, selector matching app=teamsync-api label, pod template with container using teamsync-api:latest image. Set resources requests (250m CPU, 256Mi memory) and limits (500m CPU, 512Mi memory). Configure livenessProbe on /health/live (initialDelaySeconds: 30, periodSeconds: 10, failureThreshold: 3) and readinessProbe on /health/ready (initialDelaySeconds: 10, periodSeconds: 5). Inject environment variables using envFrom referencing ConfigMap and Secret. Set rolling update strategy (maxSurge: 1, maxUnavailable: 0) for zero-downtime deployments. Add pod disruption budget allowing max 1 unavailable pod.",
          "id": 5,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Deploy and verify: (1) All 3 replicas start successfully, (2) Liveness and readiness probes pass, (3) Environment variables are correctly injected, (4) Resource limits are enforced, (5) Rolling update works without downtime, (6) Pods restart on failed health checks",
          "title": "Create Deployment manifest with probes and resource limits",
          "updatedAt": "2025-12-06T18:36:29.741732465Z"
        },
        {
          "createdAt": "2025-12-06T18:36:29.741733049Z",
          "dependencies": [
            "5"
          ],
          "description": "Define Kubernetes Service for internal routing and Ingress with cert-manager integration for external HTTPS access with automatic TLS certificate management.",
          "details": "Create infra/k8s/service.yaml defining ClusterIP service named 'teamsync-api-service' in 'teamsync' namespace, selecting pods with app=teamsync-api label, exposing port 80 targeting container port 8080. Create infra/k8s/ingress.yaml using networking.k8s.io/v1 with annotations for cert-manager (cert-manager.io/cluster-issuer: letsencrypt-prod) and nginx ingress class. Define host rule (api.teamsync.example.com) routing to teamsync-api-service on port 80. Configure TLS section referencing secret 'teamsync-tls' for certificate storage. Add annotations for rate limiting (nginx.ingress.kubernetes.io/limit-rps: \"100\") and CORS if needed. Include redirect from HTTP to HTTPS.",
          "id": 6,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Apply manifests and verify: (1) Service is created and endpoints are populated, (2) Ingress is created with correct rules, (3) TLS certificate is automatically provisioned by cert-manager, (4) HTTPS access works with valid certificate, (5) HTTP redirects to HTTPS, (6) Service discovery works within cluster",
          "title": "Create Service and Ingress with TLS configuration",
          "updatedAt": "2025-12-06T18:36:29.741733049Z"
        },
        {
          "createdAt": "2025-12-06T18:36:29.741733590Z",
          "dependencies": [
            "5"
          ],
          "description": "Define HorizontalPodAutoscaler that automatically scales the deployment between 3-10 replicas based on CPU (70%) and memory (80%) utilization metrics.",
          "details": "Create infra/k8s/hpa.yaml in 'teamsync' namespace targeting Deployment 'teamsync-api' with scaleTargetRef. Set minReplicas: 3, maxReplicas: 10. Configure metrics for both CPU (type: Resource, target.type: Utilization, target.averageUtilization: 70) and memory (target.averageUtilization: 80). Set behavior for scaling: scaleDown stabilizationWindowSeconds: 300, policies limiting scale down to 1 pod per 60 seconds; scaleUp policies allowing 2 pods per 30 seconds for faster response to load spikes. Requires metrics-server installed in cluster. Add annotations documenting scaling behavior and thresholds. Test with load testing tool (hey, k6) to verify autoscaling triggers correctly.",
          "id": 7,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Deploy HPA and verify: (1) HPA is created and targets correct deployment, (2) Current metrics are visible in 'kubectl get hpa', (3) Under load test, pods scale up when CPU/memory exceeds thresholds, (4) After load decreases, pods scale down gradually respecting stabilization window, (5) Scaling stays within min/max bounds, (6) Events show scaling decisions",
          "title": "Create HorizontalPodAutoscaler with CPU and memory targets",
          "updatedAt": "2025-12-06T18:36:29.741733590Z"
        }
      ],
      "testStrategy": "Build Docker image, verify size < 100MB. Run container locally, test health endpoints. Deploy to local k8s (minikube/kind), verify pods start. Test HPA by generating load. Verify rolling updates work. Test ConfigMap/Secret changes trigger restart.",
      "title": "Create Docker multi-stage build and Kubernetes manifests"
    },
    {
      "agentHint": "tap",
      "dependencies": [
        "23"
      ],
      "description": "Create endpoints for user data export and account deletion to comply with GDPR right to access and right to be forgotten.",
      "details": "1. Create api/gdpr.rs:\n   - GET /api/users/me/export (requires auth)\n     - Collect all user data: profile, teams, tasks, notifications\n     - Generate JSON export file\n     - Include metadata: export_date, data_version\n   - DELETE /api/users/me (requires auth + password confirmation)\n     - Soft delete user (set deleted_at)\n     - Anonymize user data (replace email with deleted_{uuid}@example.com)\n     - Remove from all teams\n     - Reassign owned tasks to team admins\n     - Delete device tokens, OAuth tokens, refresh tokens\n     - Schedule hard delete after 30 days\n2. Implement infra/gdpr.rs:\n   - export_user_data(user_id) -> Result<UserDataExport>\n   - anonymize_user(user_id) -> Result<()>\n3. Create background job for hard deletion:\n   - Runs daily, deletes users where deleted_at < NOW() - 30 days\n   - Cascade delete user's created tasks, comments, etc.\n4. Add audit log for GDPR operations\n5. Ensure all user queries filter out deleted_at IS NULL",
      "id": "29",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:37:11.526548596Z",
          "dependencies": [],
          "description": "Create GET /api/users/me/export endpoint that collects all user-related data from across the database including profile information, team memberships, tasks, notifications, comments, and any other user-associated data.",
          "details": "Implement the endpoint in api/gdpr.rs that requires authentication. Query all relevant tables (users, teams, tasks, notifications, comments, etc.) using proper joins to collect complete user data. Handle pagination for large datasets. Ensure efficient queries with appropriate indexes. Include error handling for database failures and return appropriate HTTP status codes (200 for success, 401 for unauthorized, 500 for server errors).",
          "id": 1,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for data collection from each table, integration tests verifying complete data retrieval, test with users having varying amounts of data (empty, small, large datasets), verify authentication requirements, test error handling for database failures",
          "title": "Implement user data export endpoint with comprehensive data collection",
          "updatedAt": "2025-12-06T18:37:11.526548596Z"
        },
        {
          "createdAt": "2025-12-06T18:37:11.526550304Z",
          "dependencies": [
            "1"
          ],
          "description": "Create a structured JSON format for the data export that includes all user data organized logically with metadata fields such as export_date, data_version, and data integrity checksums.",
          "details": "Define UserDataExport struct in infra/gdpr.rs with nested structures for different data categories (profile, teams, tasks, notifications). Include metadata fields: export_date (ISO 8601 timestamp), data_version (schema version for future compatibility), record_counts (summary of data exported). Implement serialization to JSON with proper formatting. Add compression option (gzip) for large exports. Set appropriate Content-Type and Content-Disposition headers for file download.",
          "id": 2,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Validate JSON schema structure, test serialization with various data sizes, verify metadata accuracy, test deserialization to ensure format is machine-readable, validate compression works correctly, test download headers are set properly",
          "title": "Design and implement JSON export format with metadata",
          "updatedAt": "2025-12-06T18:37:11.526550304Z"
        },
        {
          "createdAt": "2025-12-06T18:37:11.526550888Z",
          "dependencies": [],
          "description": "Implement DELETE /api/users/me endpoint that requires authentication and password re-confirmation before initiating the account deletion process.",
          "details": "Create endpoint in api/gdpr.rs requiring auth middleware. Accept password in request body and verify against stored hash using secure comparison. Implement rate limiting (max 5 attempts per hour) to prevent brute force. Return clear error messages for invalid passwords. On successful verification, initiate soft delete process by setting deleted_at timestamp to current time. Log the deletion request in audit log with timestamp and IP address. Return 200 with confirmation message, 401 for auth failure, 403 for wrong password, 429 for rate limit.",
          "id": 3,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test password verification logic, verify rate limiting works correctly, test authentication requirements, verify audit logging occurs, test error responses for various failure scenarios, integration test full deletion flow",
          "title": "Create account deletion endpoint with password confirmation",
          "updatedAt": "2025-12-06T18:37:11.526550888Z"
        },
        {
          "createdAt": "2025-12-06T18:37:11.526551013Z",
          "dependencies": [
            "3"
          ],
          "description": "Create anonymize_user function that replaces all personally identifiable information with anonymized values while maintaining referential integrity in the database.",
          "details": "Implement anonymize_user(user_id) in infra/gdpr.rs. Replace email with 'deleted_{uuid}@example.com' format. Clear or anonymize: first_name, last_name, phone, profile_picture, bio, address fields. Preserve user_id for referential integrity. Update all related records that display user info (comments, audit logs) to show 'Deleted User'. Use database transaction to ensure atomicity. Verify no PII remains in any table. Handle foreign key constraints properly.",
          "id": 4,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests verifying each field is anonymized correctly, test transaction rollback on failure, verify referential integrity maintained, query all tables to ensure no PII leakage, test with users having various data combinations, verify foreign key constraints still valid",
          "title": "Implement user anonymization logic with PII scrubbing",
          "updatedAt": "2025-12-06T18:37:11.526551013Z"
        },
        {
          "createdAt": "2025-12-06T18:37:11.526551596Z",
          "dependencies": [
            "4"
          ],
          "description": "Create logic to reassign all tasks owned by the deleted user to appropriate team administrators, ensuring no tasks become orphaned during account deletion.",
          "details": "Query all tasks where owner_id matches deleted user. For each task, identify the team it belongs to and find team admins (users with admin role in that team). Implement selection logic: prefer most active admin, fallback to oldest admin account. Update task owner_id to selected admin. Create notification to new owner about reassigned tasks. Handle tasks without teams by reassigning to system admin or marking as unassigned. Log all reassignments in audit trail. Execute within transaction with anonymization.",
          "id": 5,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test reassignment logic with various team structures, verify admin selection algorithm, test tasks in multiple teams, test tasks without teams, verify notifications sent correctly, test transaction integrity with reassignment, verify audit logs created",
          "title": "Implement task reassignment to team admins on user deletion",
          "updatedAt": "2025-12-06T18:37:11.526551596Z"
        },
        {
          "createdAt": "2025-12-06T18:37:11.526552263Z",
          "dependencies": [
            "4"
          ],
          "description": "Delete all authentication-related data including device tokens, OAuth tokens, refresh tokens, and active sessions when a user account is deleted.",
          "details": "Implement cleanup in infra/gdpr.rs to delete from tables: device_tokens (push notification tokens), oauth_tokens (third-party auth), refresh_tokens (JWT refresh), user_sessions (active sessions), api_keys (if applicable). Use CASCADE DELETE where appropriate or explicit DELETE queries. Ensure all tokens are invalidated immediately to prevent post-deletion access. Execute within same transaction as anonymization. Add verification step to confirm all tokens removed. Log cleanup operations in audit trail.",
          "id": 6,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test each token type is deleted, verify cascade deletes work correctly, test transaction rollback includes token cleanup, attempt to use tokens after deletion (should fail), verify audit logs for cleanup operations, test with users having multiple tokens of each type",
          "title": "Implement cascading cleanup of tokens and sessions",
          "updatedAt": "2025-12-06T18:37:11.526552263Z"
        },
        {
          "createdAt": "2025-12-06T18:37:11.526552638Z",
          "dependencies": [
            "5",
            "6"
          ],
          "description": "Implement a scheduled background job that runs daily to permanently delete user accounts and all associated data after the 30-day soft delete retention period has elapsed.",
          "details": "Create background job (using cron or task scheduler) that runs daily at off-peak hours. Query users WHERE deleted_at < NOW() - INTERVAL '30 days' AND deleted_at IS NOT NULL. For each user, perform hard delete: CASCADE DELETE user record (will cascade to remaining associated data based on foreign key constraints). Delete any remaining orphaned records. Log each hard deletion in audit log with user_id, deletion_date, and data_purged summary. Implement job monitoring and alerting for failures. Add configuration for retention period (default 30 days). Ensure job is idempotent and can safely retry.",
          "id": 7,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test job scheduling works correctly, verify 30-day calculation accurate, test cascade deletes remove all data, query database after hard delete to confirm no remnants, test job handles failures gracefully, verify audit logging for hard deletes, test with various retention periods, simulate job restart/retry scenarios",
          "title": "Create background job for hard deletion after 30-day retention",
          "updatedAt": "2025-12-06T18:37:11.526552638Z"
        }
      ],
      "testStrategy": "Integration tests: export user data, verify completeness. Delete account, verify anonymization and team removal. Test task reassignment. Verify hard delete after 30 days. Test deleted user cannot login. Verify GDPR operations logged.",
      "title": "Implement GDPR compliance with data export and deletion"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "23",
        "24"
      ],
      "description": "Create React 18 + TypeScript frontend with Tailwind CSS featuring Kanban board with drag-and-drop task management.",
      "details": "1. Initialize React app in frontend/:\n   ```bash\n   npx create-react-app frontend --template typescript\n   cd frontend && npm install -D tailwindcss postcss autoprefixer\n   npm install @dnd-kit/core @dnd-kit/sortable axios react-router-dom\n   ```\n2. Configure Tailwind in tailwind.config.js with dark mode support\n3. Create components:\n   - src/components/KanbanBoard.tsx (columns: Todo, In Progress, Done)\n   - src/components/TaskCard.tsx (draggable task)\n   - src/components/TaskModal.tsx (create/edit task)\n   - src/components/TeamHeader.tsx (team info, member count)\n4. Implement drag-and-drop with @dnd-kit:\n   - DndContext wraps board\n   - Droppable columns\n   - Draggable task cards\n   - onDragEnd updates task status via API\n5. Setup WebSocket connection:\n   - Connect on board mount\n   - Listen for TaskEvent, update local state\n   - Reconnect on disconnect\n6. Create API client in src/api/client.ts:\n   - Axios instance with JWT interceptor\n   - Methods for all task/team endpoints\n7. Add routing: /teams/:id/board\n8. Implement optimistic updates for drag-and-drop",
      "id": "30",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:38:03.463972592Z",
          "dependencies": [],
          "description": "Set up React 18 application with TypeScript template, install and configure Tailwind CSS with dark mode support, and install all required dependencies including @dnd-kit, axios, and react-router-dom.",
          "details": "Run 'npx create-react-app frontend --template typescript' to create the project. Install Tailwind CSS with 'npm install -D tailwindcss postcss autoprefixer' and run 'npx tailwindcss init -p'. Configure tailwind.config.js with content paths ['./src/**/*.{js,jsx,ts,tsx}'] and darkMode: 'class'. Update src/index.css to include Tailwind directives (@tailwind base, components, utilities). Install dependencies: 'npm install @dnd-kit/core @dnd-kit/sortable @dnd-kit/utilities axios react-router-dom'. Create basic folder structure: src/components/, src/api/, src/types/, src/hooks/. Verify the app runs with 'npm start'.",
          "id": 1,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify that the React app starts successfully, Tailwind classes are applied correctly, and all dependencies are installed without errors. Test dark mode toggle functionality.",
          "title": "Initialize React TypeScript project with Tailwind CSS",
          "updatedAt": "2025-12-06T18:38:03.463972592Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463975301Z",
          "dependencies": [
            "1"
          ],
          "description": "Define comprehensive TypeScript interfaces for Task, Team, User, TaskEvent, and API response types to ensure type safety throughout the application.",
          "details": "Create src/types/index.ts with interfaces: Task (id: string, title: string, description: string, status: 'todo' | 'in-progress' | 'done', assignee?: User, createdAt: Date, updatedAt: Date), Team (id: string, name: string, members: User[], createdAt: Date), User (id: string, name: string, email: string, avatar?: string), TaskEvent (type: 'created' | 'updated' | 'deleted', task: Task, teamId: string, timestamp: Date), ApiResponse<T> (data: T, message?: string, error?: string). Export all types for use across components.",
          "id": 2,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify TypeScript compilation succeeds with no type errors. Ensure all types are properly exported and can be imported in other files.",
          "title": "Create TypeScript types and interfaces for API models",
          "updatedAt": "2025-12-06T18:38:03.463975301Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463976551Z",
          "dependencies": [
            "2"
          ],
          "description": "Create a centralized API client using Axios with JWT authentication interceptor, error handling, and methods for all task and team endpoints.",
          "details": "Create src/api/client.ts with an Axios instance configured with baseURL (process.env.REACT_APP_API_URL or 'http://localhost:3000/api'). Implement request interceptor to add JWT token from localStorage to Authorization header. Implement response interceptor for error handling (401 redirects to login, network errors show notifications). Create methods: getTasks(teamId: string), createTask(teamId: string, task: Partial<Task>), updateTask(taskId: string, updates: Partial<Task>), deleteTask(taskId: string), getTeam(teamId: string), getTeamMembers(teamId: string). All methods should return typed promises using the defined interfaces. Add retry logic for failed requests (max 3 retries with exponential backoff).",
          "id": 3,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit test each API method with mocked Axios responses. Test JWT interceptor adds token correctly. Test error handling for 401, 404, and 500 status codes. Verify retry logic works for network failures.",
          "title": "Build API client with Axios and JWT interceptor",
          "updatedAt": "2025-12-06T18:38:03.463976551Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463976884Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Build the main KanbanBoard component with three columns (Todo, In Progress, Done) using Tailwind CSS for responsive layout and styling.",
          "details": "Create src/components/KanbanBoard.tsx as a functional component. Implement state management using useState for tasks grouped by status. Use useEffect to fetch initial tasks on mount via API client. Create column layout using Tailwind grid (grid-cols-1 md:grid-cols-3 gap-4). Each column should have a header with title and task count, a scrollable container for task cards (min-h-[500px] max-h-[calc(100vh-200px)] overflow-y-auto), and styling with bg-gray-100 dark:bg-gray-800 rounded-lg p-4. Implement loading state with skeleton placeholders. Add error boundary for graceful error handling. Include TeamHeader component at the top showing team name and member count.",
          "id": 4,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test component renders three columns correctly. Verify tasks are fetched and displayed in correct columns based on status. Test loading and error states. Verify responsive layout works on mobile and desktop.",
          "title": "Create Kanban board layout with three columns",
          "updatedAt": "2025-12-06T18:38:03.463976884Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463977176Z",
          "dependencies": [
            "2"
          ],
          "description": "Create a reusable TaskCard component that displays task information with proper styling, assignee avatar, and interactive hover states.",
          "details": "Create src/components/TaskCard.tsx accepting task prop of type Task. Display task title (font-semibold text-gray-900 dark:text-white), truncated description (text-sm text-gray-600 dark:text-gray-400, line-clamp-2), assignee info with avatar (rounded-full w-8 h-8) or initials fallback, and timestamp (text-xs text-gray-500). Style card with bg-white dark:bg-gray-700 rounded-lg shadow-sm p-4 border border-gray-200 dark:border-gray-600. Add hover effects (hover:shadow-md transition-shadow cursor-pointer). Include edit and delete action buttons (visible on hover) in top-right corner. Add priority indicator with colored left border (blue for low, yellow for medium, red for high priority).",
          "id": 5,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test card renders all task properties correctly. Verify hover states and transitions work smoothly. Test with tasks having and missing assignees. Verify dark mode styling. Test action buttons trigger correct callbacks.",
          "title": "Implement TaskCard component with visual design",
          "updatedAt": "2025-12-06T18:38:03.463977176Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463978009Z",
          "dependencies": [
            "4",
            "5"
          ],
          "description": "Integrate @dnd-kit library to enable drag-and-drop task movement between columns with smooth animations and accessibility support.",
          "details": "Wrap KanbanBoard with DndContext from @dnd-kit/core. Create droppable column containers using useDroppable hook with unique ids ('todo', 'in-progress', 'done'). Make TaskCard draggable using useDraggable hook with task.id as id. Implement onDragEnd handler that: (1) determines source and destination columns from event.active and event.over, (2) updates local state immediately (optimistic update), (3) calls API to update task status, (4) handles API errors by reverting state. Use SortableContext from @dnd-kit/sortable for each column to enable reordering within columns. Add DragOverlay for visual feedback during drag. Configure sensors (PointerSensor, KeyboardSensor) for mouse and keyboard accessibility. Style dragging state with opacity-50 and transform transitions.",
          "id": 6,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test dragging tasks between all column combinations. Verify optimistic updates occur immediately. Test API failure rollback restores original state. Verify keyboard navigation works (Tab to select, Space/Enter to drag, Arrow keys to move). Test touch device compatibility.",
          "title": "Implement drag-and-drop functionality with @dnd-kit",
          "updatedAt": "2025-12-06T18:38:03.463978009Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463978259Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Create a modal component for creating new tasks and editing existing tasks with form validation and API integration.",
          "details": "Create src/components/TaskModal.tsx with props: isOpen (boolean), onClose (function), task (optional Task for edit mode), teamId (string). Use controlled form inputs for title (required, max 200 chars), description (textarea, max 2000 chars), status (select dropdown), assignee (searchable dropdown of team members). Implement form validation using useState for errors. Style modal with fixed overlay (bg-black bg-opacity-50), centered content (bg-white dark:bg-gray-800 rounded-lg max-w-2xl w-full p-6), close button, and action buttons (Cancel, Save). On save: validate inputs, call createTask or updateTask API method, show loading spinner on submit button, display success/error toast notifications, close modal on success. Include keyboard shortcuts (Esc to close, Cmd/Ctrl+Enter to submit). Add focus trap to keep focus within modal.",
          "id": 7,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test modal opens and closes correctly. Verify form validation shows errors for invalid inputs. Test create mode saves new tasks via API. Test edit mode loads existing task data and updates correctly. Verify keyboard shortcuts work. Test focus trap and accessibility with screen readers.",
          "title": "Build TaskModal for create and edit operations",
          "updatedAt": "2025-12-06T18:38:03.463978259Z"
        },
        {
          "createdAt": "2025-12-06T18:38:03.463978759Z",
          "dependencies": [
            "4",
            "6"
          ],
          "description": "Set up WebSocket connection for real-time task updates across clients with automatic reconnection logic and state synchronization.",
          "details": "Create src/hooks/useWebSocket.ts custom hook accepting teamId parameter. Establish WebSocket connection to ws://localhost:3000/ws/teams/:teamId on mount. Store connection in useRef to persist across renders. Listen for 'message' events, parse TaskEvent JSON, and update local tasks state based on event type: 'created' adds task to appropriate column, 'updated' modifies existing task (merge with current state), 'deleted' removes task. Implement reconnection logic: on 'close' or 'error' events, attempt reconnect with exponential backoff (1s, 2s, 4s, 8s, max 30s). Clean up connection on unmount. Add connection status indicator (connected/disconnected/reconnecting) displayed in UI. Handle race conditions between WebSocket updates and optimistic UI updates by using task.updatedAt timestamp to determine which version is newer. Send heartbeat pings every 30s to keep connection alive.",
          "id": 8,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test WebSocket connects successfully on component mount. Verify real-time updates from other clients appear correctly. Test reconnection logic by simulating connection drops. Verify optimistic updates don't conflict with WebSocket updates. Test multiple tabs stay synchronized. Verify connection cleans up properly on unmount. Test heartbeat keeps connection alive during idle periods.",
          "title": "Implement WebSocket connection with real-time updates",
          "updatedAt": "2025-12-06T18:38:03.463978759Z"
        }
      ],
      "testStrategy": "Unit tests for components with React Testing Library. Integration tests: render board, verify tasks display. Test drag-and-drop updates task status. Verify WebSocket updates reflected in UI. Test create/edit task modal. Manual testing for responsive design and accessibility.",
      "title": "Build React dashboard with Kanban board and drag-and-drop"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "30"
      ],
      "description": "Create React components for team activity timeline and member management interface with role assignment.",
      "details": "1. Create backend endpoint in api/teams.rs:\n   - GET /api/teams/:id/activity -> recent events (task created, member joined, etc.)\n   - GET /api/teams/:id/members -> list members with roles\n   - PATCH /api/teams/:id/members/:user_id -> update member role (admin only)\n   - DELETE /api/teams/:id/members/:user_id -> remove member (admin only)\n2. Store activity events in new table: team_events\n   - Columns: id, team_id, event_type, actor_id, metadata (JSONB), created_at\n   - Index on (team_id, created_at DESC)\n3. Create React components:\n   - src/components/ActivityFeed.tsx (timeline of events)\n   - src/components/MemberList.tsx (table with avatars, roles)\n   - src/components/MemberRoleSelect.tsx (dropdown for role change)\n   - src/components/InviteModal.tsx (generate invite link)\n4. Implement pagination for activity feed (infinite scroll)\n5. Add confirmation dialog for member removal\n6. Display member roles with badges (Owner, Admin, Member, Viewer)\n7. Disable role changes for owners and self\n8. Add route: /teams/:id/members",
      "id": "31",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:38:45.915067001Z",
          "dependencies": [],
          "description": "Design and implement the database schema for storing team activity events including table creation, indexes, and migration scripts.",
          "details": "Create a new migration file to add the team_events table with columns: id (primary key), team_id (foreign key to teams), event_type (varchar for event classification like 'member_joined', 'task_created', 'role_changed'), actor_id (foreign key to users), metadata (JSONB for flexible event data storage), and created_at (timestamp). Add an index on (team_id, created_at DESC) for efficient querying of recent events. Include proper foreign key constraints and ensure the JSONB column can store event-specific details like changed role values or task references.",
          "id": 1,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write migration tests to verify table creation, index creation, and foreign key constraints. Test JSONB column accepts valid JSON data and rejects invalid formats.",
          "title": "Create team_events table schema and migration",
          "updatedAt": "2025-12-06T18:38:45.915067001Z"
        },
        {
          "createdAt": "2025-12-06T18:38:45.915069501Z",
          "dependencies": [
            "1"
          ],
          "description": "Create GET /api/teams/:id/activity endpoint in api/teams.rs that retrieves paginated team events with proper authorization checks.",
          "details": "Implement the activity feed endpoint that accepts query parameters for pagination (limit, offset or cursor-based). Verify the requesting user is a member of the team before returning events. Query the team_events table ordered by created_at DESC, joining with users table to include actor information (name, avatar). Return formatted event objects with human-readable descriptions. Support filtering by event_type if needed. Implement cursor-based pagination for infinite scroll support, returning a next_cursor value in the response. Limit results to 20-50 events per request to optimize performance.",
          "id": 2,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write integration tests for authenticated and unauthorized access, pagination boundary conditions, and event ordering. Test with empty result sets and large event histories.",
          "title": "Implement activity feed API endpoint with pagination",
          "updatedAt": "2025-12-06T18:38:45.915069501Z"
        },
        {
          "createdAt": "2025-12-06T18:38:45.915070001Z",
          "dependencies": [
            "1"
          ],
          "description": "Create API endpoints for listing team members, updating member roles, and removing members with proper authorization checks in api/teams.rs.",
          "details": "Implement three endpoints: (1) GET /api/teams/:id/members - returns list of team members with user details (id, name, email, avatar) and their roles, sorted by join date or role hierarchy. (2) PATCH /api/teams/:id/members/:user_id - allows admins/owners to update a member's role, with validation preventing self-role changes and owner role modifications. (3) DELETE /api/teams/:id/members/:user_id - allows admins/owners to remove members, preventing owner removal and self-removal. All endpoints must verify the requesting user has appropriate permissions (admin or owner role). Log role changes and member removals as events in team_events table. Return appropriate error codes (403 for unauthorized, 404 for not found, 400 for invalid operations).",
          "id": 3,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write comprehensive authorization tests covering admin vs member permissions, self-modification attempts, owner protection rules. Test edge cases like removing the last admin or non-existent users.",
          "title": "Implement member management API endpoints",
          "updatedAt": "2025-12-06T18:38:45.915070001Z"
        },
        {
          "createdAt": "2025-12-06T18:38:45.915070417Z",
          "dependencies": [
            "2"
          ],
          "description": "Build the ActivityFeed.tsx component that displays a timeline of team events with infinite scroll pagination.",
          "details": "Create a React component that fetches and displays team activity events using the activity feed API. Implement infinite scroll using Intersection Observer API or a library like react-infinite-scroll-component. Display events in a timeline format with event icons, actor avatars, event descriptions, and timestamps (using relative time like '2 hours ago'). Format different event types appropriately (member joined, task created, role changed, etc.). Show loading indicators while fetching more events. Handle empty states with appropriate messaging. Use React Query or similar for data fetching, caching, and pagination state management. Ensure the component re-fetches on team context changes.",
          "id": 4,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write unit tests for event rendering, infinite scroll trigger, and loading states. Test with mock data for various event types. Verify pagination cursor management and duplicate event prevention.",
          "title": "Create ActivityFeed React component with infinite scroll",
          "updatedAt": "2025-12-06T18:38:45.915070417Z"
        },
        {
          "createdAt": "2025-12-06T18:38:45.915070751Z",
          "dependencies": [
            "3"
          ],
          "description": "Build the MemberList.tsx component displaying team members in a table format with avatars, role badges, and action buttons based on permissions.",
          "details": "Create a React component that fetches and displays team members using the members list API. Display members in a table or card layout with user avatars, names, email addresses, and role badges (Owner, Admin, Member, Viewer) styled with different colors. Show action buttons (change role, remove member) conditionally based on the current user's permissions. Disable actions for owners and prevent users from modifying their own roles. Include a search/filter functionality for large teams. Add an 'Invite Member' button that opens the invite modal. Use proper loading states and error handling. Implement optimistic UI updates when roles are changed or members are removed.",
          "id": 5,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write tests for role badge rendering, conditional button visibility based on user permissions, and search/filter functionality. Mock API responses to test various permission scenarios.",
          "title": "Create MemberList component with role badges and actions",
          "updatedAt": "2025-12-06T18:38:45.915070751Z"
        },
        {
          "createdAt": "2025-12-06T18:38:45.915071084Z",
          "dependencies": [
            "3",
            "5"
          ],
          "description": "Create MemberRoleSelect.tsx component and confirmation dialogs for role changes and member removal with proper authorization handling.",
          "details": "Build a MemberRoleSelect dropdown component that displays available roles and triggers role change API calls. Implement a confirmation dialog component that appears before executing role changes, clearly showing the old and new roles. Create a separate confirmation dialog for member removal with warning text about permanent action. Both dialogs should have 'Cancel' and 'Confirm' buttons. After confirmation, call the appropriate API endpoint (PATCH for role change, DELETE for removal). Handle API responses, showing success messages and updating the UI optimistically. Display error messages if operations fail (e.g., insufficient permissions). Integrate these components into MemberList, ensuring dialogs appear in the correct context.",
          "id": 6,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write tests for dialog opening/closing, confirmation flow, API call triggering, and error handling. Test that unauthorized actions are prevented at the UI level and that API errors are displayed properly.",
          "title": "Implement role change functionality with confirmation dialogs",
          "updatedAt": "2025-12-06T18:38:45.915071084Z"
        },
        {
          "createdAt": "2025-12-06T18:38:45.915071292Z",
          "dependencies": [
            "5"
          ],
          "description": "Build the InviteModal.tsx component that generates team invite links with role selection and copy-to-clipboard functionality.",
          "details": "Create a modal component that allows team admins/owners to generate invite links for new members. Include a dropdown to select the default role for invited members (Member, Viewer, etc., but not Owner or Admin unless current user is owner). Call a backend API endpoint (POST /api/teams/:id/invites) to generate a unique invite token with expiration. Display the generated invite URL in a read-only input field. Implement a 'Copy to Clipboard' button using the Clipboard API with visual feedback (button text changes to 'Copied!' temporarily). Show the invite link expiration time. Optionally include email invite functionality. Add a close button and handle modal open/close state. Style the modal to match the application's design system.",
          "id": 7,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write tests for modal open/close behavior, invite link generation API call, role selection, and clipboard copy functionality. Mock the Clipboard API and verify success/error states are handled correctly.",
          "title": "Create invite link generation modal with copy functionality",
          "updatedAt": "2025-12-06T18:38:45.915071292Z"
        }
      ],
      "testStrategy": "Integration tests: fetch activity feed, verify events ordered by date. Test member list displays correct roles. Admin updates member role, verify API call and UI update. Test remove member with confirmation. Verify non-admin cannot change roles. Test invite link generation and copy to clipboard.",
      "title": "Implement team activity feed and member management UI"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "30"
      ],
      "description": "Add theme switching functionality with system preference detection and localStorage persistence across the React dashboard.",
      "details": "1. Create src/contexts/ThemeContext.tsx:\n   - ThemeProvider with state: 'light' | 'dark' | 'system'\n   - Detect system preference with window.matchMedia('(prefers-color-scheme: dark)')\n   - Listen for system theme changes\n   - Persist preference in localStorage\n2. Create src/components/ThemeToggle.tsx:\n   - Button with sun/moon/auto icons\n   - Cycles through light -> dark -> system\n3. Update tailwind.config.js:\n   - Enable dark mode: 'class'\n4. Add dark: variants to all components:\n   - Dark background colors\n   - Dark text colors\n   - Dark borders and shadows\n5. Apply theme class to document.documentElement:\n   - <html className={theme === 'dark' ? 'dark' : ''}>\n6. Create CSS variables for theme colors in src/index.css:\n   - --color-background, --color-text, etc.\n7. Add ThemeToggle to app header",
      "id": "32",
      "priority": "low",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T18:39:19.019290627Z",
          "dependencies": [],
          "description": "Implement a React Context provider for theme state management that supports three modes (light, dark, system) with system preference detection using the matchMedia API and dynamic listener for system theme changes.",
          "details": "Create src/contexts/ThemeContext.tsx with: (1) ThemeProvider component managing state with useState for 'light' | 'dark' | 'system', (2) useEffect to detect initial system preference using window.matchMedia('(prefers-color-scheme: dark)'), (3) Event listener for system theme changes that updates when user changes OS theme, (4) Computed effective theme that resolves 'system' to actual 'light' or 'dark' based on system preference, (5) Context value exposing theme state and setTheme function, (6) useTheme custom hook for consuming context. Include proper TypeScript types for all props and context values.",
          "id": 1,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test system preference detection by changing OS theme settings, verify event listener responds to system changes, test all three theme modes, ensure context provides correct values to consumers",
          "title": "Create ThemeContext with state management and system preference detection",
          "updatedAt": "2025-12-06T18:39:19.019290627Z"
        },
        {
          "createdAt": "2025-12-06T18:39:19.019293335Z",
          "dependencies": [
            "1"
          ],
          "description": "Add localStorage integration to persist user theme preference and implement initialization logic to prevent flash of unstyled content on page load by reading preference before React renders.",
          "details": "Extend ThemeContext with: (1) useEffect to save theme preference to localStorage whenever it changes using key 'theme-preference', (2) Initialize state by reading from localStorage on mount with fallback to 'system', (3) Create inline script in index.html or App.tsx that reads localStorage and applies theme class to document.documentElement before React hydration, (4) Ensure the initialization script runs synchronously in <head> or before root render to prevent FOUC (flash of unstyled content), (5) Add error handling for localStorage access failures (private browsing mode). Apply 'dark' class to html element when effective theme is dark.",
          "id": 2,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify theme persists across page refreshes, test with localStorage disabled, check no flash occurs on initial load by testing in incognito mode, verify theme applies before content renders",
          "title": "Implement localStorage persistence with flash prevention",
          "updatedAt": "2025-12-06T18:39:19.019293335Z"
        },
        {
          "createdAt": "2025-12-06T18:39:19.019294127Z",
          "dependencies": [
            "1"
          ],
          "description": "Build an interactive toggle button component that cycles through light, dark, and system modes with appropriate icons and visual feedback for the current theme state.",
          "details": "Create src/components/ThemeToggle.tsx with: (1) useTheme hook to access theme context, (2) Button that cycles through states: light -> dark -> system -> light on click, (3) Icon rendering logic showing sun icon for light, moon for dark, and monitor/auto icon for system mode, (4) Tooltip or aria-label indicating current mode and next mode on hover, (5) Smooth transition animations between icon states, (6) Proper accessibility attributes (role, aria-label, keyboard support), (7) Styling that works in both light and dark modes. Use heroicons or similar icon library for consistent visuals.",
          "id": 3,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test clicking cycles through all three modes in correct order, verify correct icon displays for each mode, test keyboard navigation and screen reader compatibility, ensure button is visible in both themes",
          "title": "Create ThemeToggle component with three-state cycle",
          "updatedAt": "2025-12-06T18:39:19.019294127Z"
        },
        {
          "createdAt": "2025-12-06T18:39:19.019294335Z",
          "dependencies": [
            "2"
          ],
          "description": "Update Tailwind configuration to enable class-based dark mode and establish a CSS custom property system for consistent theming across the application.",
          "details": "Update configuration files: (1) Modify tailwind.config.js to set darkMode: 'class' to enable dark: variant classes, (2) Create CSS variables in src/index.css defining theme colors: --color-background, --color-text, --color-border, --color-primary, --color-secondary, --color-accent, --color-muted, (3) Define separate values for light theme (default) and dark theme using .dark selector, (4) Map Tailwind theme colors to CSS variables in tailwind.config.js theme.extend.colors, (5) Ensure variables cover backgrounds, text, borders, shadows, and interactive states, (6) Test that variables cascade properly and override correctly in dark mode.",
          "id": 4,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify Tailwind dark: classes work when html has dark class, test CSS variables resolve correctly in both modes, inspect computed styles in DevTools, ensure no color conflicts or specificity issues",
          "title": "Configure Tailwind dark mode and create CSS variable system",
          "updatedAt": "2025-12-06T18:39:19.019294335Z"
        },
        {
          "createdAt": "2025-12-06T18:39:19.019294544Z",
          "dependencies": [
            "3",
            "4"
          ],
          "description": "Systematically add dark mode variants to all existing React components in the dashboard using Tailwind dark: classes and CSS variables for comprehensive theme coverage.",
          "details": "Update all component files: (1) Add dark: variants to className props for backgrounds (dark:bg-gray-800, dark:bg-gray-900), (2) Update text colors with dark:text-gray-100, dark:text-gray-300 variants, (3) Adjust borders with dark:border-gray-700 variants, (4) Update shadows and hover states for dark mode, (5) Test forms, buttons, cards, navigation, tables, and modals for visual consistency, (6) Ensure proper contrast ratios meet WCAG AA standards in both themes, (7) Update any hardcoded color values to use CSS variables or Tailwind classes, (8) Add ThemeToggle component to application header/navbar. Document any components requiring special dark mode treatment.",
          "id": 5,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Visual regression testing in both light and dark modes, verify all interactive elements are visible and accessible, test color contrast with automated tools, ensure no components are missed by navigating entire application in both themes",
          "title": "Apply dark mode styling across all dashboard components",
          "updatedAt": "2025-12-06T18:39:19.019294544Z"
        }
      ],
      "testStrategy": "Unit tests for ThemeContext. Test system preference detection. Verify localStorage persistence across sessions. Test theme toggle cycles correctly. Manual testing: verify all components readable in both themes. Test automatic theme switch when system preference changes. Verify no flash of unstyled content on load.",
      "title": "Implement dark/light theme toggle with persistence"
    }
  ]
}