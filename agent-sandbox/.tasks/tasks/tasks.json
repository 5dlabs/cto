{
  "metadata": {
    "completedCount": 0,
    "lastModified": "2025-12-06T12:01:15.225569919+00:00",
    "taskCount": 15,
    "version": "1.0.0"
  },
  "tasks": [
    {
      "agentHint": "rex",
      "dependencies": [],
      "description": "Set up the Rust project structure with Axum 0.7, sqlx for PostgreSQL, Redis client, and configure the workspace with proper directory layout",
      "details": "1. Run `cargo init --name teamsync-api`\n2. Add dependencies to Cargo.toml:\n   - axum = \"0.7\"\n   - tokio = { version = \"1\", features = [\"full\"] }\n   - sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-rustls\", \"migrate\"] }\n   - redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }\n   - tower = \"0.4\"\n   - tower-http = { version = \"0.5\", features = [\"trace\", \"cors\"] }\n   - serde = { version = \"1.0\", features = [\"derive\"] }\n   - serde_json = \"1.0\"\n3. Create directory structure: src/{api, domain, infra}/\n4. Set up main.rs with basic Axum server skeleton\n5. Create .env.example with DATABASE_URL, REDIS_URL placeholders",
      "id": "1",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:52:31.647513677Z",
          "dependencies": [],
          "description": "Create a new Rust project named 'teamsync-api' using cargo init command and verify the basic project structure is created correctly.",
          "details": "Run `cargo init --name teamsync-api` in the target directory. Verify that Cargo.toml and src/main.rs are created. Check that the project compiles with `cargo check` to ensure the basic setup is correct.",
          "id": 1,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo check` to verify the project initializes and compiles successfully",
          "title": "Initialize Rust project with cargo",
          "updatedAt": "2025-12-06T11:52:31.647513677Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647515927Z",
          "dependencies": [
            "1"
          ],
          "description": "Configure Cargo.toml with Axum 0.7 web framework and Tokio async runtime with full features enabled for the API server foundation.",
          "details": "Add to [dependencies] section: axum = \"0.7\" and tokio = { version = \"1\", features = [\"full\"] }. The tokio full feature set includes macros, io, net, time, rt-multi-thread, and sync primitives needed for an async web server.",
          "id": 2,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo check` to ensure dependencies resolve and compile without errors",
          "title": "Add Axum and Tokio dependencies to Cargo.toml",
          "updatedAt": "2025-12-06T11:52:31.647515927Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647517885Z",
          "dependencies": [
            "1"
          ],
          "description": "Configure SQLx 0.7 with PostgreSQL driver, Tokio runtime integration, and migration support for database operations.",
          "details": "Add to [dependencies]: sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-rustls\", \"migrate\"] }. The postgres feature enables PostgreSQL support, runtime-tokio-rustls provides async runtime with TLS, and migrate enables database migration functionality.",
          "id": 3,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo check` to verify SQLx compiles correctly with selected features",
          "title": "Add SQLx PostgreSQL dependencies",
          "updatedAt": "2025-12-06T11:52:31.647517885Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647518052Z",
          "dependencies": [
            "1"
          ],
          "description": "Configure Redis client 0.24 with Tokio compatibility and connection manager for caching and session management capabilities.",
          "details": "Add to [dependencies]: redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }. The tokio-comp feature enables Tokio async support, and connection-manager provides connection pooling for efficient Redis operations.",
          "id": 4,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo check` to ensure Redis client dependency resolves correctly",
          "title": "Add Redis client dependencies",
          "updatedAt": "2025-12-06T11:52:31.647518052Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647518219Z",
          "dependencies": [
            "1"
          ],
          "description": "Configure Tower 0.4 and tower-http 0.5 with tracing and CORS features for middleware support, logging, and cross-origin request handling.",
          "details": "Add to [dependencies]: tower = \"0.4\" and tower-http = { version = \"0.5\", features = [\"trace\", \"cors\"] }. Tower provides the middleware abstraction layer, while tower-http adds HTTP-specific middleware including request tracing and CORS policy enforcement.",
          "id": 5,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo check` to verify Tower dependencies compile successfully",
          "title": "Add Tower middleware dependencies",
          "updatedAt": "2025-12-06T11:52:31.647518219Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647518344Z",
          "dependencies": [
            "1"
          ],
          "description": "Configure serde 1.0 with derive macros and serde_json 1.0 for JSON serialization and deserialization of API requests and responses.",
          "details": "Add to [dependencies]: serde = { version = \"1.0\", features = [\"derive\"] } and serde_json = \"1.0\". The derive feature enables automatic Serialize/Deserialize trait derivation using macros, simplifying struct serialization.",
          "id": 6,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run `cargo check` and verify all dependencies compile together without conflicts",
          "title": "Add serialization dependencies",
          "updatedAt": "2025-12-06T11:52:31.647518344Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647520094Z",
          "dependencies": [
            "1"
          ],
          "description": "Establish the modular directory layout with api, domain, and infra modules to separate concerns and organize code by architectural layers.",
          "details": "Create directories: src/api/ (for HTTP handlers and routes), src/domain/ (for business logic and entities), and src/infra/ (for infrastructure like database and cache clients). Create mod.rs files in each directory to declare them as modules.",
          "id": 7,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify directories exist and contain mod.rs files; ensure `cargo check` recognizes the module structure",
          "title": "Create project directory structure",
          "updatedAt": "2025-12-06T11:52:31.647520094Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647520219Z",
          "dependencies": [
            "2",
            "5",
            "6",
            "7"
          ],
          "description": "Create a minimal Axum HTTP server with a health check endpoint and proper async runtime setup to verify the framework integration works correctly.",
          "details": "In src/main.rs: (1) Add tokio::main macro to main function, (2) Create an Axum Router with a GET /health endpoint returning JSON {\"status\": \"ok\"}, (3) Use axum::serve with TcpListener on 0.0.0.0:3000, (4) Add basic error handling. Include use statements for axum::Router, axum::routing::get, and axum::Json.",
          "id": 8,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run the server with `cargo run` and test the health endpoint with curl or browser at http://localhost:3000/health",
          "title": "Implement basic Axum server skeleton in main.rs",
          "updatedAt": "2025-12-06T11:52:31.647520219Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647520635Z",
          "dependencies": [
            "1"
          ],
          "description": "Generate an example environment configuration file with placeholder values for database and Redis connections to guide deployment setup.",
          "details": "Create .env.example file in project root with the following content:\nDATABASE_URL=postgresql://username:password@localhost:5432/teamsync\nREDIS_URL=redis://localhost:6379\nSERVER_HOST=0.0.0.0\nSERVER_PORT=3000\nRUST_LOG=info\nInclude comments explaining each variable's purpose.",
          "id": 9,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify .env.example file exists and contains all required configuration placeholders with clear documentation",
          "title": "Create .env.example configuration template",
          "updatedAt": "2025-12-06T11:52:31.647520635Z"
        },
        {
          "createdAt": "2025-12-06T11:52:31.647521052Z",
          "dependencies": [
            "2",
            "3",
            "4",
            "5",
            "6",
            "7",
            "8",
            "9"
          ],
          "description": "Run comprehensive checks to ensure all dependencies are correctly configured, the project structure is sound, and the basic server runs successfully.",
          "details": "Execute the following verification steps: (1) Run `cargo check` to verify compilation, (2) Run `cargo build` to create debug binary, (3) Run `cargo test` to ensure test framework works, (4) Start server with `cargo run` and verify it listens on port 3000, (5) Test health endpoint returns expected JSON response, (6) Review Cargo.lock to ensure dependency resolution is stable.",
          "id": 10,
          "parentId": "1",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Complete integration test: build succeeds, server starts without errors, health endpoint responds correctly, and all cargo commands execute successfully",
          "title": "Verify complete project setup and compilation",
          "updatedAt": "2025-12-06T11:52:31.647521052Z"
        }
      ],
      "testStrategy": "Run `cargo build` and `cargo test` to ensure compilation succeeds. Start server with `cargo run` and verify it listens on port 3000",
      "title": "Initialize Rust project with Axum and core dependencies"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "1"
      ],
      "description": "Design and implement the database schema for teams, users, tasks, and invites with proper indexes and constraints using sqlx migrations",
      "details": "1. Create migrations directory: `sqlx migrate add initial_schema`\n2. Define tables:\n   - users (id uuid PRIMARY KEY, email varchar UNIQUE, password_hash varchar, created_at timestamptz)\n   - teams (id uuid PRIMARY KEY, name varchar, description text, created_at timestamptz, deleted_at timestamptz)\n   - team_members (team_id uuid, user_id uuid, role varchar, PRIMARY KEY(team_id, user_id))\n   - tasks (id uuid PRIMARY KEY, team_id uuid, title varchar, description text, assignee_id uuid, status varchar, due_date timestamptz, created_at timestamptz, deleted_at timestamptz)\n   - invites (id uuid PRIMARY KEY, team_id uuid, token varchar UNIQUE, expires_at timestamptz)\n3. Add indexes on foreign keys, deleted_at, status, due_date\n4. Create sqlx connection pool in src/infra/database.rs\n5. Implement health check query",
      "id": "2",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:53:05.145418595Z",
          "dependencies": [],
          "description": "Design and implement the foundational database schema for users, teams, and team_members tables with proper constraints, foreign keys, and initial indexes. This establishes the core identity and team management structure.",
          "details": "1. Create initial migration: `sqlx migrate add create_users_teams_tables`\n2. Define users table with: id (uuid PRIMARY KEY DEFAULT gen_random_uuid()), email (varchar(255) UNIQUE NOT NULL), password_hash (varchar(255) NOT NULL), created_at (timestamptz DEFAULT NOW())\n3. Define teams table with: id (uuid PRIMARY KEY DEFAULT gen_random_uuid()), name (varchar(255) NOT NULL), description (text), created_at (timestamptz DEFAULT NOW()), deleted_at (timestamptz)\n4. Define team_members junction table with: team_id (uuid REFERENCES teams(id) ON DELETE CASCADE), user_id (uuid REFERENCES users(id) ON DELETE CASCADE), role (varchar(50) NOT NULL CHECK (role IN ('owner', 'admin', 'member'))), joined_at (timestamptz DEFAULT NOW()), PRIMARY KEY(team_id, user_id)\n5. Create indexes: CREATE INDEX idx_teams_deleted_at ON teams(deleted_at) WHERE deleted_at IS NULL; CREATE INDEX idx_team_members_user_id ON team_members(user_id)\n6. Test migration: sqlx migrate run and verify tables exist with \\d commands",
          "id": 1,
          "parentId": "2",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run migration in test database, verify table creation with psql \\d commands, test foreign key constraints by attempting invalid inserts, verify unique constraint on email, test soft delete pattern on teams table",
          "title": "Create core user and team tables with relationships",
          "updatedAt": "2025-12-06T11:53:05.145418595Z"
        },
        {
          "createdAt": "2025-12-06T11:53:05.145423637Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the tasks table with all necessary fields, foreign key relationships to teams and users, status management, soft deletes, and optimized indexes for common query patterns including filtering by status, assignee, and due dates.",
          "details": "1. Create migration: `sqlx migrate add create_tasks_table`\n2. Define tasks table with: id (uuid PRIMARY KEY DEFAULT gen_random_uuid()), team_id (uuid NOT NULL REFERENCES teams(id) ON DELETE CASCADE), title (varchar(500) NOT NULL), description (text), assignee_id (uuid REFERENCES users(id) ON DELETE SET NULL), status (varchar(50) NOT NULL DEFAULT 'todo' CHECK (status IN ('todo', 'in_progress', 'done', 'archived'))), due_date (timestamptz), created_at (timestamptz DEFAULT NOW()), updated_at (timestamptz DEFAULT NOW()), deleted_at (timestamptz)\n3. Create performance indexes: CREATE INDEX idx_tasks_team_id ON tasks(team_id) WHERE deleted_at IS NULL; CREATE INDEX idx_tasks_assignee_id ON tasks(assignee_id) WHERE deleted_at IS NULL; CREATE INDEX idx_tasks_status ON tasks(status) WHERE deleted_at IS NULL; CREATE INDEX idx_tasks_due_date ON tasks(due_date) WHERE deleted_at IS NULL AND due_date IS NOT NULL; CREATE INDEX idx_tasks_deleted_at ON tasks(deleted_at) WHERE deleted_at IS NULL\n4. Add trigger for updated_at: CREATE TRIGGER update_tasks_updated_at BEFORE UPDATE ON tasks FOR EACH ROW EXECUTE FUNCTION update_updated_at_column()\n5. Test migration and verify all indexes are created",
          "id": 2,
          "parentId": "2",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run migration, verify table structure and all indexes with \\d tasks, test foreign key cascades, insert sample tasks and verify index usage with EXPLAIN ANALYZE, test status constraint validation, verify soft delete filtering works correctly",
          "title": "Implement tasks table with comprehensive indexing strategy",
          "updatedAt": "2025-12-06T11:53:05.145423637Z"
        },
        {
          "createdAt": "2025-12-06T11:53:05.145424595Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement the team invites table with secure token generation support, expiration timestamps, and proper indexing for token lookups and cleanup of expired invites. Include constraints to ensure data integrity.",
          "details": "1. Create migration: `sqlx migrate add create_invites_table`\n2. Define invites table with: id (uuid PRIMARY KEY DEFAULT gen_random_uuid()), team_id (uuid NOT NULL REFERENCES teams(id) ON DELETE CASCADE), email (varchar(255) NOT NULL), token (varchar(255) UNIQUE NOT NULL), invited_by (uuid NOT NULL REFERENCES users(id) ON DELETE CASCADE), expires_at (timestamptz NOT NULL), created_at (timestamptz DEFAULT NOW()), used_at (timestamptz)\n3. Add constraint: CONSTRAINT valid_expiration CHECK (expires_at > created_at)\n4. Create indexes: CREATE UNIQUE INDEX idx_invites_token ON invites(token); CREATE INDEX idx_invites_team_id ON invites(team_id); CREATE INDEX idx_invites_expires_at ON invites(expires_at) WHERE used_at IS NULL; CREATE INDEX idx_invites_email_team ON invites(email, team_id) WHERE used_at IS NULL\n5. Document token generation strategy in migration comments (tokens should be cryptographically secure, 32+ bytes)\n6. Test migration and verify unique token constraint",
          "id": 3,
          "parentId": "2",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run migration, verify table structure with \\d invites, test unique token constraint by inserting duplicate tokens, verify foreign key relationships, test expiration constraint, insert test invites and query by token to verify index usage, test cleanup queries for expired invites",
          "title": "Create invites table with token management and expiration",
          "updatedAt": "2025-12-06T11:53:05.145424595Z"
        },
        {
          "createdAt": "2025-12-06T11:53:05.145424720Z",
          "dependencies": [
            "1",
            "2",
            "3"
          ],
          "description": "Implement the database connection pool configuration using sqlx, create a health check mechanism to verify database connectivity, and set up the infrastructure code in src/infra/database.rs for application-wide database access.",
          "details": "1. Create src/infra/database.rs module\n2. Implement DatabaseConfig struct with fields: database_url (String), max_connections (u32, default 5), min_connections (u32, default 1), connection_timeout (Duration, default 30s), idle_timeout (Option<Duration>, default 10min)\n3. Implement async fn create_pool(config: &DatabaseConfig) -> Result<PgPool> using sqlx::postgres::PgPoolOptions with configured limits and timeouts\n4. Implement async fn health_check(pool: &PgPool) -> Result<bool> that executes SELECT 1 query with timeout\n5. Add pool.acquire() wrapper with error handling and logging\n6. Implement graceful shutdown: async fn close_pool(pool: PgPool) that calls pool.close().await\n7. Add documentation comments explaining pool configuration trade-offs\n8. Create integration test that verifies pool creation, health check, and query execution\n9. Add DATABASE_URL environment variable validation on startup",
          "id": 4,
          "parentId": "2",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit test DatabaseConfig default values, integration test pool creation with test database, verify health check returns true with valid connection and false/error with invalid connection, test connection timeout behavior, verify max_connections limit is enforced, test graceful shutdown doesn't leave hanging connections, run all previous migrations and verify pool can query all tables",
          "title": "Set up database connection pool and health check infrastructure",
          "updatedAt": "2025-12-06T11:53:05.145424720Z"
        }
      ],
      "testStrategy": "Run `sqlx migrate run` against test database. Verify all tables created with `\\dt` in psql. Test connection pool with simple SELECT query",
      "title": "Configure PostgreSQL database schema and migrations"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "1",
        "2"
      ],
      "description": "Build JWT-based authentication with access and refresh token generation, validation middleware, and secure token storage in Redis",
      "details": "1. Add dependencies: jsonwebtoken = \"9\", argon2 = \"0.5\", uuid = { version = \"1\", features = [\"v4\", \"serde\"] }\n2. Create src/domain/auth.rs with:\n   - struct Claims { sub: Uuid, exp: i64, role: String }\n   - fn generate_access_token(user_id: Uuid, role: String) -> Result<String> (15min expiry)\n   - fn generate_refresh_token() -> String (7 days, store in Redis with user_id)\n3. Implement password hashing with argon2\n4. Create POST /api/auth/register and /api/auth/login endpoints\n5. Build Axum middleware in src/api/middleware/auth.rs:\n   - Extract JWT from Authorization header\n   - Validate signature and expiry\n   - Inject user context into request extensions\n6. Store refresh tokens in Redis with key pattern: refresh_token:{token} -> user_id",
      "id": "3",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:54:00.073553135Z",
          "dependencies": [],
          "description": "Set up Argon2 password hashing utilities and create the user registration endpoint that accepts email/password, hashes the password securely, and stores the user in the database with proper validation and error handling.",
          "details": "1. Add dependencies to Cargo.toml: argon2 = \"0.5\", uuid = { version = \"1\", features = [\"v4\", \"serde\"] }\n2. Create src/domain/auth.rs with password hashing functions:\n   - fn hash_password(password: &str) -> Result<String> using Argon2id with recommended parameters\n   - fn verify_password(password: &str, hash: &str) -> Result<bool>\n3. Create User model with fields: id (Uuid), email (String), password_hash (String), role (String), created_at (DateTime)\n4. Implement POST /api/auth/register endpoint in src/api/routes/auth.rs:\n   - Accept JSON payload { email, password }\n   - Validate email format and password strength (min 8 chars, complexity rules)\n   - Check if email already exists\n   - Hash password with Argon2\n   - Insert user into database\n   - Return 201 with user info (excluding password)\n5. Add comprehensive error handling for duplicate emails, validation failures, and database errors",
          "id": 1,
          "parentId": "3",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: Test password hashing with various inputs, verify hash verification works correctly, test password strength validation. Integration tests: Test registration with valid data returns 201, test duplicate email returns 409, test weak passwords are rejected, test SQL injection attempts are prevented, test invalid email formats are rejected.",
          "title": "Implement password hashing and user registration endpoint",
          "updatedAt": "2025-12-06T11:54:00.073553135Z"
        },
        {
          "createdAt": "2025-12-06T11:54:00.073566593Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement JWT access and refresh token generation functions with proper claims structure, signing, and validation logic including expiry checks and signature verification.",
          "details": "1. Add jsonwebtoken = \"9\" to Cargo.toml\n2. Extend src/domain/auth.rs with JWT structures:\n   - struct Claims { sub: Uuid, exp: i64, iat: i64, role: String } with Serialize/Deserialize\n   - struct TokenPair { access_token: String, refresh_token: String }\n3. Implement token generation functions:\n   - fn generate_access_token(user_id: Uuid, role: String, secret: &str) -> Result<String>\n     * Set expiry to 15 minutes from now\n     * Include user_id as 'sub' claim, role, iat (issued at), exp (expiry)\n     * Sign with HS256 algorithm using secret key\n   - fn generate_refresh_token() -> String\n     * Generate cryptographically secure random token using uuid::Uuid::new_v4()\n     * Return as string (will be stored in Redis with 7-day TTL)\n4. Implement validation function:\n   - fn validate_access_token(token: &str, secret: &str) -> Result<Claims>\n     * Decode and verify signature\n     * Check expiry timestamp\n     * Return Claims if valid, error otherwise\n5. Create configuration struct for JWT settings (secret key, access token expiry, refresh token expiry)\n6. Load JWT secret from environment variable with fallback for development",
          "id": 2,
          "parentId": "3",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: Test token generation produces valid JWT format, verify claims are correctly embedded, test token validation accepts valid tokens, test expired tokens are rejected, test tokens with invalid signatures fail validation, test malformed tokens return appropriate errors. Property-based tests: Generate random user IDs and roles, verify round-trip encoding/decoding.",
          "title": "Create JWT token generation and validation logic",
          "updatedAt": "2025-12-06T11:54:00.073566593Z"
        },
        {
          "createdAt": "2025-12-06T11:54:00.073567051Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Create the login endpoint that authenticates users via email/password, generates access and refresh tokens, stores refresh tokens in Redis with proper TTL, and returns the token pair to the client.",
          "details": "1. Add redis = { version = \"0.23\", features = [\"tokio-comp\", \"connection-manager\"] } to Cargo.toml\n2. Set up Redis connection pool in application state:\n   - Create RedisClient wrapper in src/infrastructure/redis.rs\n   - Initialize connection manager from REDIS_URL environment variable\n   - Add to Axum application state\n3. Implement POST /api/auth/login endpoint:\n   - Accept JSON payload { email, password }\n   - Query database for user by email\n   - Return 401 if user not found (use constant-time comparison to prevent timing attacks)\n   - Verify password hash using verify_password function\n   - Return 401 if password incorrect\n   - Generate access token with user_id and role\n   - Generate refresh token (UUID v4)\n   - Store refresh token in Redis:\n     * Key: \"refresh_token:{token}\"\n     * Value: user_id as string\n     * TTL: 7 days (604800 seconds)\n   - Return 200 with JSON { access_token, refresh_token, expires_in: 900 }\n4. Add POST /api/auth/refresh endpoint:\n   - Accept { refresh_token } in body\n   - Look up token in Redis\n   - Return 401 if not found or expired\n   - Retrieve user_id and role from database\n   - Generate new access token\n   - Optionally rotate refresh token (generate new one, delete old one)\n   - Return new token pair\n5. Implement logout endpoint POST /api/auth/logout to delete refresh token from Redis",
          "id": 3,
          "parentId": "3",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: Test successful login returns valid tokens, test invalid credentials return 401, test login with non-existent email returns 401, verify refresh token is stored in Redis with correct TTL, test refresh endpoint with valid token returns new access token, test refresh with invalid token returns 401, test logout removes token from Redis. Security tests: Verify timing attack resistance, test rate limiting on login attempts, test token rotation on refresh.",
          "title": "Build login endpoint with token issuance and Redis integration",
          "updatedAt": "2025-12-06T11:54:00.073567051Z"
        },
        {
          "createdAt": "2025-12-06T11:54:00.073567176Z",
          "dependencies": [
            "2"
          ],
          "description": "Build Axum middleware that extracts JWT from Authorization header, validates the token, and injects authenticated user context into request extensions for protected routes to access.",
          "details": "1. Create src/api/middleware/auth.rs with authentication middleware\n2. Define AuthUser struct to hold authenticated user info:\n   - struct AuthUser { id: Uuid, role: String }\n   - Implement FromRequestParts trait to extract from extensions\n3. Implement auth middleware function:\n   - async fn auth_middleware(State(app_state): State<AppState>, mut req: Request, next: Next) -> Result<Response>\n   - Extract \"Authorization\" header from request\n   - Return 401 if header missing\n   - Parse \"Bearer {token}\" format, return 401 if malformed\n   - Call validate_access_token with extracted token\n   - Return 401 with appropriate error message if validation fails (expired, invalid signature, etc.)\n   - If valid, create AuthUser from claims\n   - Insert AuthUser into request extensions: req.extensions_mut().insert(auth_user)\n   - Call next.run(req).await to proceed to handler\n4. Create optional auth middleware variant for routes that optionally use auth\n5. Add helper function to extract AuthUser in handlers:\n   - Implement extractor pattern so handlers can use Extension(auth_user): Extension<AuthUser>\n6. Create role-based authorization middleware:\n   - fn require_role(required_role: &str) -> middleware that checks AuthUser.role\n7. Add proper error responses with WWW-Authenticate header for 401s",
          "id": 4,
          "parentId": "3",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: Test middleware extracts valid tokens correctly, test missing Authorization header returns 401, test malformed Bearer token returns 401, test expired token returns 401, test invalid signature returns 401. Integration tests: Test protected endpoint without token returns 401, test protected endpoint with valid token returns 200, test AuthUser is accessible in handler, test role-based middleware blocks unauthorized roles, test WWW-Authenticate header is present in 401 responses.",
          "title": "Implement authentication middleware for route protection",
          "updatedAt": "2025-12-06T11:54:00.073567176Z"
        },
        {
          "createdAt": "2025-12-06T11:54:00.073567260Z",
          "dependencies": [
            "3",
            "4"
          ],
          "description": "Wire up all authentication components into the main application, apply middleware to protected routes, and implement comprehensive security testing including penetration testing scenarios, token lifecycle validation, and security best practices verification.",
          "details": "1. Update main application setup in src/main.rs or src/api/mod.rs:\n   - Initialize Redis connection pool and add to app state\n   - Register auth routes (/api/auth/register, /api/auth/login, /api/auth/refresh, /api/auth/logout)\n   - Apply auth middleware to protected routes using axum::middleware::from_fn_with_state\n   - Create public and protected route groups\n2. Configure CORS headers appropriately for authentication:\n   - Allow credentials\n   - Restrict origins in production\n   - Handle preflight requests\n3. Add security headers middleware:\n   - X-Content-Type-Options: nosniff\n   - X-Frame-Options: DENY\n   - Strict-Transport-Security for HTTPS\n4. Implement rate limiting on auth endpoints (using tower-governor or similar):\n   - Login: 5 attempts per 15 minutes per IP\n   - Register: 3 attempts per hour per IP\n   - Refresh: 10 attempts per minute per user\n5. Add audit logging for authentication events:\n   - Log successful/failed login attempts with IP and timestamp\n   - Log token refresh events\n   - Log logout events\n6. Create comprehensive integration test suite:\n   - Full authentication flow: register -> login -> access protected route -> refresh -> logout\n   - Test token expiry and refresh flow\n   - Test concurrent requests with same token\n   - Test token reuse after logout\n7. Security testing scenarios:\n   - SQL injection attempts in login/register\n   - XSS attempts in input fields\n   - CSRF protection verification\n   - Test JWT algorithm confusion attacks (ensure only HS256 accepted)\n   - Test token tampering detection\n   - Verify password hashing parameters meet OWASP recommendations\n   - Test session fixation prevention\n8. Document authentication API endpoints with examples",
          "id": 5,
          "parentId": "3",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "End-to-end tests: Complete user journey from registration to accessing protected resources. Security tests: Run OWASP ZAP or similar tool against auth endpoints, test with deliberately malformed JWTs, attempt replay attacks with expired tokens, verify rate limiting kicks in, test brute force protection. Load tests: Verify Redis connection pool handles concurrent token validations, test performance under 1000 concurrent auth requests. Compliance tests: Verify GDPR considerations for storing user data, ensure passwords are never logged, verify secure token transmission (HTTPS only in production).",
          "title": "Integrate authentication system and implement comprehensive security testing",
          "updatedAt": "2025-12-06T11:54:00.073567260Z"
        }
      ],
      "testStrategy": "Unit test token generation/validation. Integration test: register user, login, access protected endpoint with token, refresh token flow. Test expired token rejection",
      "title": "Implement JWT authentication system with refresh tokens"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "1"
      ],
      "description": "Configure Redis connection pool and implement token bucket rate limiting for authenticated and anonymous users",
      "details": "1. Create src/infra/redis.rs with connection manager setup\n2. Implement rate limiter in src/api/middleware/rate_limit.rs:\n   - Use Redis INCR with TTL for sliding window counter\n   - Key pattern: rate_limit:{ip_or_user_id}:{minute_window}\n   - Limits: 100 req/min for authenticated, 20 req/min for anonymous\n   - Return 429 Too Many Requests when exceeded\n3. Create middleware layer that:\n   - Extracts user_id from auth context or falls back to IP\n   - Checks current count in Redis\n   - Increments counter with EXPIRE 60\n   - Adds X-RateLimit-* headers to response\n4. Apply middleware globally in router configuration",
      "id": "4",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:54:31.426010802Z",
          "dependencies": [],
          "description": "Create the Redis infrastructure module with connection pooling, configuration management, and comprehensive error handling for distributed rate limiting operations.",
          "details": "1. Create src/infra/redis.rs module with Redis connection manager using redis-rs crate\n2. Implement connection pool configuration with:\n   - Configurable pool size (min 5, max 20 connections)\n   - Connection timeout (5 seconds)\n   - Retry logic with exponential backoff\n   - Health check mechanism\n3. Add environment variable configuration for:\n   - REDIS_URL (default: redis://localhost:6379)\n   - REDIS_POOL_SIZE\n   - REDIS_CONNECTION_TIMEOUT\n4. Implement error types for Redis operations (ConnectionError, CommandError, TimeoutError)\n5. Create helper functions for common operations (get, set, incr, expire)\n6. Add connection validation on startup\n7. Implement graceful shutdown for connection pool\n8. Performance testing: Verify connection pool handles 1000+ concurrent requests without exhaustion, measure connection acquisition time (<10ms p95)",
          "id": 1,
          "parentId": "4",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for connection pool configuration and error handling. Integration tests with actual Redis instance to verify connection lifecycle, pool exhaustion scenarios, and timeout handling. Load test with 1000 concurrent connections to validate pool performance.",
          "title": "Set up Redis connection pool with configuration and error handling",
          "updatedAt": "2025-12-06T11:54:31.426010802Z"
        },
        {
          "createdAt": "2025-12-06T11:54:31.426012469Z",
          "dependencies": [
            "1"
          ],
          "description": "Build the core rate limiting logic using Redis atomic operations with sliding window counter algorithm to track and enforce request limits per user/IP.",
          "details": "1. Create src/api/middleware/rate_limit.rs module\n2. Implement RateLimiter struct with methods:\n   - check_rate_limit(key: &str, limit: u32, window_secs: u64) -> Result<RateLimitStatus>\n   - Uses Redis INCR command with key pattern: rate_limit:{identifier}:{window_timestamp}\n3. Implement sliding window logic:\n   - Calculate current minute window: timestamp / 60\n   - Use Redis pipeline for atomic operations: INCR + EXPIRE\n   - Set EXPIRE to 60 seconds on first increment\n   - Return current count and remaining quota\n4. Define rate limit configurations:\n   - Authenticated users: 100 requests/minute\n   - Anonymous users (by IP): 20 requests/minute\n5. Implement RateLimitStatus struct with fields: allowed (bool), current_count, limit, reset_at (timestamp)\n6. Add Lua script option for atomic check-and-increment to prevent race conditions\n7. Handle edge cases: Redis unavailable (fail open vs fail closed), clock skew\n8. Performance testing: Benchmark rate limiter handles 10,000 req/sec, verify atomic operations prevent race conditions under concurrent load",
          "id": 2,
          "parentId": "4",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for sliding window calculation and key generation. Integration tests with Redis to verify INCR/EXPIRE atomicity and correct counting across window boundaries. Concurrency tests with 100+ parallel requests to same key verifying no count leakage. Performance benchmarks measuring latency (target <5ms p99).",
          "title": "Implement token bucket rate limiting algorithm with Redis sliding window",
          "updatedAt": "2025-12-06T11:54:31.426012469Z"
        },
        {
          "createdAt": "2025-12-06T11:54:31.426013635Z",
          "dependencies": [
            "2"
          ],
          "description": "Develop the Axum middleware layer that integrates rate limiting into the request pipeline, extracting user context, enforcing limits, and adding standard rate limit headers to responses.",
          "details": "1. Implement Axum middleware function rate_limit_middleware in src/api/middleware/rate_limit.rs\n2. Extract identifier from request:\n   - Check for authenticated user_id from auth context (Extension<AuthUser>)\n   - Fallback to client IP address from ConnectInfo or X-Forwarded-For header\n   - Handle proxy scenarios with X-Real-IP header priority\n3. Call rate limiter with appropriate limit based on auth status\n4. On rate limit exceeded:\n   - Return 429 Too Many Requests status\n   - Include JSON body: {\"error\": \"Rate limit exceeded\", \"retry_after\": <seconds>}\n5. Add response headers for all requests:\n   - X-RateLimit-Limit: maximum requests allowed\n   - X-RateLimit-Remaining: requests remaining in window\n   - X-RateLimit-Reset: Unix timestamp when limit resets\n6. Integrate middleware into router configuration:\n   - Apply globally using .layer() in main router setup\n   - Ensure middleware runs after auth but before route handlers\n7. Add configuration for bypass patterns (e.g., health check endpoints)\n8. Implement metrics collection for rate limit hits/misses\n9. Performance testing: Verify middleware adds <2ms latency p95, test with 5000 req/sec sustained load, validate header accuracy",
          "id": 3,
          "parentId": "4",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for identifier extraction logic and header generation. Integration tests verifying 429 responses when limits exceeded and correct header values. End-to-end tests with authenticated and anonymous requests. Load testing with mixed traffic (80% under limit, 20% over limit) at 5000 req/sec to validate performance and correctness under production-like conditions.",
          "title": "Create rate limiting middleware with header injection and router integration",
          "updatedAt": "2025-12-06T11:54:31.426013635Z"
        }
      ],
      "testStrategy": "Unit test rate limit logic with mock Redis. Integration test: make 21 anonymous requests rapidly, verify 429 on 21st. Test authenticated user gets 100 req/min limit",
      "title": "Set up Redis connection and rate limiting infrastructure"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "2",
        "3"
      ],
      "description": "Create CRUD operations for teams including creation, retrieval with member count, updates, and invite link generation with 7-day expiry",
      "details": "1. Create src/api/teams.rs with handlers:\n   - POST /api/teams: create_team(Json<CreateTeamDto>) -> Json<TeamResponse>\n     * Insert into teams table\n     * Add creator as owner in team_members\n   - GET /api/teams/:id: get_team(Path<Uuid>) -> Json<TeamWithMemberCount>\n     * JOIN with team_members to count members\n     * Check user has access (is member)\n   - PATCH /api/teams/:id: update_team(Path<Uuid>, Json<UpdateTeamDto>)\n     * Verify user is owner/admin\n   - POST /api/teams/:id/invite: generate_invite(Path<Uuid>)\n     * Create UUID token, store in invites table with expires_at = now() + 7 days\n     * Return invite URL\n2. Define DTOs in src/domain/team.rs\n3. Implement authorization checks using role from team_members",
      "id": "5",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:55:02.786364844Z",
          "dependencies": [],
          "description": "Create POST /api/teams endpoint that handles team creation, inserts the team into the database, and automatically assigns the creator as the team owner in team_members table within a transaction.",
          "details": "In src/api/teams.rs, implement create_team handler that: 1) Accepts Json<CreateTeamDto> with team name and optional description, 2) Extracts authenticated user from request context, 3) Begins database transaction, 4) Inserts new team record into teams table with generated UUID, 5) Inserts creator into team_members table with role='owner', 6) Commits transaction, 7) Returns Json<TeamResponse> with created team details. Define CreateTeamDto and TeamResponse in src/domain/team.rs with proper validation (name 1-100 chars, description max 500 chars). Handle transaction rollback on errors and return appropriate HTTP status codes (201 Created on success, 400 for validation errors, 500 for database errors).",
          "id": 1,
          "parentId": "5",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: validate DTO deserialization and validation rules. Integration tests: test successful team creation with owner assignment, verify transaction rollback on database errors, test duplicate team name handling, verify creator is assigned owner role, test with missing/invalid authentication token.",
          "title": "Implement team creation endpoint with owner assignment",
          "updatedAt": "2025-12-06T11:55:02.786364844Z"
        },
        {
          "createdAt": "2025-12-06T11:55:02.786365594Z",
          "dependencies": [
            "1"
          ],
          "description": "Create GET /api/teams/:id endpoint that retrieves team details with member count using JOIN query and enforces access control to ensure only team members can view team information.",
          "details": "In src/api/teams.rs, implement get_team handler that: 1) Accepts Path<Uuid> for team_id, 2) Extracts authenticated user_id from request context, 3) Executes SQL query with LEFT JOIN between teams and team_members tables to get team details and COUNT of members, 4) Verifies requesting user is a member by checking team_members table (WHERE team_id = ? AND user_id = ?), 5) Returns 403 Forbidden if user is not a member, 404 if team doesn't exist, 6) Returns Json<TeamWithMemberCount> containing team fields plus member_count. Define TeamWithMemberCount struct in src/domain/team.rs. Optimize query to perform authorization check and data retrieval in single database round-trip where possible.",
          "id": 2,
          "parentId": "5",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: test successful retrieval by team member with correct member count, verify non-members receive 403 Forbidden, test with non-existent team ID returns 404, verify member count accuracy with varying team sizes (0, 1, multiple members), test with invalid UUID format returns 400, test unauthorized access without authentication token.",
          "title": "Build team retrieval with member count aggregation and access control",
          "updatedAt": "2025-12-06T11:55:02.786365594Z"
        },
        {
          "createdAt": "2025-12-06T11:55:02.786366011Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement PATCH /api/teams/:id endpoint that allows team owners and admins to update team information with proper role-based authorization checks before applying changes.",
          "details": "In src/api/teams.rs, implement update_team handler that: 1) Accepts Path<Uuid> for team_id and Json<UpdateTeamDto> with optional name and description fields, 2) Extracts authenticated user_id from request context, 3) Queries team_members table to get user's role for the team, 4) Verifies user has 'owner' or 'admin' role, returns 403 Forbidden if insufficient permissions or not a member, 5) Validates UpdateTeamDto fields (name 1-100 chars if provided, description max 500 chars if provided), 6) Updates only provided fields in teams table WHERE id = team_id, 7) Returns updated Json<TeamResponse> with 200 OK. Define UpdateTeamDto in src/domain/team.rs with Option<String> fields. Handle cases where no fields are provided (return 400 Bad Request).",
          "id": 3,
          "parentId": "5",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: test successful update by owner with name only, description only, and both fields, verify admin can update team, test member with regular role receives 403 Forbidden, verify non-members receive 403, test empty update payload returns 400, test validation errors for invalid field lengths, verify partial updates don't affect non-provided fields, test with non-existent team returns 404.",
          "title": "Create team update endpoint with role-based authorization",
          "updatedAt": "2025-12-06T11:55:02.786366011Z"
        },
        {
          "createdAt": "2025-12-06T11:55:02.786366136Z",
          "dependencies": [
            "1"
          ],
          "description": "Create POST /api/teams/:id/invite endpoint that generates unique invite tokens with 7-day expiration, stores them in the invites table, and returns shareable invite URLs for team recruitment.",
          "details": "In src/api/teams.rs, implement generate_invite handler that: 1) Accepts Path<Uuid> for team_id, 2) Extracts authenticated user_id from request context, 3) Verifies user is team member with 'owner' or 'admin' role (query team_members table), returns 403 if unauthorized, 4) Generates cryptographically secure UUID token using uuid::Uuid::new_v4(), 5) Calculates expires_at timestamp as current UTC time + 7 days (using chrono), 6) Inserts record into invites table with (token, team_id, created_by=user_id, expires_at, used=false), 7) Constructs invite URL string (e.g., '/teams/invite/{token}'), 8) Returns Json with invite_url and expires_at fields with 201 Created status. Define InviteResponse struct in src/domain/team.rs. Consider adding index on invites.token for future lookup performance.",
          "id": 4,
          "parentId": "5",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: test successful invite generation by owner with valid expiry date (7 days from now), verify admin can generate invites, test member without admin/owner role receives 403, verify each generated token is unique across multiple invitations, test non-members receive 403, verify invite record is correctly stored in database with all fields, test token format is valid UUID, verify expires_at calculation is accurate, test with non-existent team returns 404.",
          "title": "Implement invite link generation with expiry management",
          "updatedAt": "2025-12-06T11:55:02.786366136Z"
        }
      ],
      "testStrategy": "Integration tests: create team as user A, verify user A can access, user B cannot. Test invite generation and expiry. Test PATCH requires admin role. Verify member count accuracy",
      "title": "Implement team management API endpoints"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "2",
        "3",
        "5"
      ],
      "description": "Implement task management endpoints with create, read, update, delete operations, status/assignee/date filtering, and 30-day soft delete retention",
      "details": "1. Create src/api/tasks.rs with handlers:\n   - POST /api/teams/:team_id/tasks: create_task(Path<Uuid>, Json<CreateTaskDto>)\n   - GET /api/teams/:team_id/tasks: list_tasks(Path<Uuid>, Query<TaskFilters>)\n     * WHERE deleted_at IS NULL AND team_id = $1\n     * Apply filters: status IN (...), assignee_id = $2, due_date BETWEEN $3 AND $4\n   - GET /api/tasks/:id: get_task(Path<Uuid>)\n   - PATCH /api/tasks/:id: update_task(Path<Uuid>, Json<UpdateTaskDto>)\n   - DELETE /api/tasks/:id: soft_delete_task(Path<Uuid>)\n     * SET deleted_at = NOW() WHERE id = $1\n2. Define TaskFilters struct with Optional<Vec<Status>>, Optional<Uuid>, Optional<DateRange>\n3. Add background job (separate task) to hard delete tasks where deleted_at < NOW() - 30 days",
      "id": "6",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:55:44.483343752Z",
          "dependencies": [],
          "description": "Create the POST /api/teams/:team_id/tasks endpoint that validates team membership before creating tasks. Implement CreateTaskDto validation, database insertion with proper foreign key constraints, and return created task with 201 status.",
          "details": "1. Create src/api/tasks.rs file with Actix-web handler structure\n2. Define CreateTaskDto struct with fields: title (String, required), description (Option<String>), status (TaskStatus enum), assignee_id (Option<Uuid>), due_date (Option<DateTime>), priority (Option<Priority>)\n3. Implement create_task handler: extract Path<Uuid> team_id and Json<CreateTaskDto>\n4. Validate requester is member of team (query team_members table)\n5. If assignee_id provided, validate assignee is also team member\n6. Insert into tasks table: INSERT INTO tasks (id, team_id, title, description, status, assignee_id, due_date, priority, created_at, updated_at) VALUES (...)\n7. Return created task as JSON with 201 Created status\n8. Handle errors: 403 Forbidden if not team member, 400 Bad Request for validation failures, 404 if team not found",
          "id": 1,
          "parentId": "6",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: valid task creation, team membership validation rejection, invalid assignee rejection, missing required fields. Integration tests: end-to-end task creation with database verification, concurrent task creation for same team.",
          "title": "Implement task creation endpoint with team membership validation",
          "updatedAt": "2025-12-06T11:55:44.483343752Z"
        },
        {
          "createdAt": "2025-12-06T11:55:44.483349086Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement GET /api/teams/:team_id/tasks with TaskFilters query parameters supporting status array, assignee_id, and date range filtering. Optimize query performance with proper indexing and exclude soft-deleted tasks.",
          "details": "1. Define TaskFilters struct with serde deserialize: status (Option<Vec<TaskStatus>>), assignee_id (Option<Uuid>), due_date_start (Option<DateTime>), due_date_end (Option<DateTime>)\n2. Implement list_tasks handler: extract Path<Uuid> team_id and Query<TaskFilters>\n3. Build dynamic SQL query starting with: SELECT * FROM tasks WHERE deleted_at IS NULL AND team_id = $1\n4. Append filter clauses conditionally: if status provided add AND status = ANY($n), if assignee_id add AND assignee_id = $n, if date range add AND due_date BETWEEN $n AND $m\n5. Use sqlx query builder or dynamic parameter binding\n6. Add ORDER BY created_at DESC with pagination support (limit/offset)\n7. Execute query and return Vec<Task> as JSON\n8. Add database indexes: CREATE INDEX idx_tasks_team_deleted ON tasks(team_id, deleted_at); CREATE INDEX idx_tasks_filters ON tasks(status, assignee_id, due_date) WHERE deleted_at IS NULL",
          "id": 2,
          "parentId": "6",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: each filter combination (status only, assignee only, date range only, all combined, none). Performance tests: query execution time with 10k+ tasks, verify index usage with EXPLAIN ANALYZE. Integration tests: verify soft-deleted tasks excluded, pagination works correctly.",
          "title": "Build task listing endpoint with multi-parameter filtering",
          "updatedAt": "2025-12-06T11:55:44.483349086Z"
        },
        {
          "createdAt": "2025-12-06T11:55:44.483349794Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement GET /api/tasks/:id for single task retrieval and PATCH /api/tasks/:id for partial updates. Enforce authorization ensuring only team members can access/modify tasks, exclude soft-deleted tasks from retrieval.",
          "details": "1. Implement get_task handler: extract Path<Uuid> task_id\n2. Query: SELECT t.* FROM tasks t WHERE t.id = $1 AND t.deleted_at IS NULL\n3. Verify requester is member of task's team (JOIN team_members)\n4. Return 404 if not found or soft-deleted, 403 if not team member\n5. Define UpdateTaskDto with all optional fields: title, description, status, assignee_id, due_date, priority\n6. Implement update_task handler: extract Path<Uuid> and Json<UpdateTaskDto>\n7. Verify task exists, not soft-deleted, and requester is team member\n8. Build dynamic UPDATE query: UPDATE tasks SET updated_at = NOW() with only provided fields\n9. If assignee_id changing, validate new assignee is team member\n10. Execute update and return updated task\n11. Handle optimistic locking with updated_at comparison if needed",
          "id": 3,
          "parentId": "6",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: successful retrieval, update with each field individually, update multiple fields, authorization rejection, soft-deleted task returns 404. Integration tests: concurrent updates, assignee validation during update, partial update preserves unchanged fields.",
          "title": "Create task retrieval and update endpoints with authorization",
          "updatedAt": "2025-12-06T11:55:44.483349794Z"
        },
        {
          "createdAt": "2025-12-06T11:55:44.483349919Z",
          "dependencies": [
            "3"
          ],
          "description": "Create DELETE /api/tasks/:id endpoint that performs soft deletion by setting deleted_at timestamp instead of removing records. Ensure soft-deleted tasks are excluded from all query operations and can be restored within retention period.",
          "details": "1. Implement soft_delete_task handler: extract Path<Uuid> task_id\n2. Verify task exists and requester is team member (same authorization as update)\n3. Check task is not already soft-deleted (deleted_at IS NULL)\n4. Execute: UPDATE tasks SET deleted_at = NOW(), updated_at = NOW() WHERE id = $1 AND deleted_at IS NULL\n5. Return 204 No Content on success\n6. Return 404 if task not found or already deleted, 403 if unauthorized\n7. Optional: implement restore endpoint PATCH /api/tasks/:id/restore that sets deleted_at = NULL\n8. Ensure all existing query endpoints filter WHERE deleted_at IS NULL\n9. Add database constraint or trigger to cascade soft delete to related entities if needed\n10. Log soft deletion events for audit trail",
          "id": 4,
          "parentId": "6",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: successful soft delete, double delete returns 404, unauthorized delete rejected, verify deleted_at timestamp set. Integration tests: soft-deleted tasks excluded from list_tasks, get_task returns 404 for soft-deleted, restore functionality if implemented, verify no data loss on soft delete.",
          "title": "Implement soft delete functionality for tasks",
          "updatedAt": "2025-12-06T11:55:44.483349919Z"
        },
        {
          "createdAt": "2025-12-06T11:55:44.483350002Z",
          "dependencies": [
            "4"
          ],
          "description": "Implement scheduled background job that permanently removes tasks soft-deleted more than 30 days ago. Use job scheduler (tokio-cron or similar) to run daily cleanup maintaining data retention policy.",
          "details": "1. Add tokio-cron-scheduler or apalis dependency to Cargo.toml\n2. Create src/jobs/cleanup.rs module\n3. Implement cleanup_deleted_tasks function: DELETE FROM tasks WHERE deleted_at IS NOT NULL AND deleted_at < NOW() - INTERVAL '30 days'\n4. Log number of records permanently deleted for monitoring\n5. Create job scheduler in main.rs startup: schedule daily at 2 AM UTC\n6. Add configuration for retention period (default 30 days, configurable via env var TASK_RETENTION_DAYS)\n7. Implement graceful shutdown for background jobs\n8. Add metrics/monitoring: track deletion count, job execution time, failures\n9. Consider transaction safety and batch deletion for large datasets (DELETE with LIMIT in loop)\n10. Add manual trigger endpoint (admin only) for testing: POST /api/admin/cleanup-tasks\n11. Document retention policy in API documentation",
          "id": 5,
          "parentId": "6",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: verify deletion query logic, date calculation correctness, configurable retention period. Integration tests: create soft-deleted tasks with various timestamps, run job, verify only 30+ day old tasks deleted. Load tests: performance with large number of soft-deleted records (10k+). Monitor job execution in staging environment.",
          "title": "Add background job for permanent deletion of expired soft-deleted tasks",
          "updatedAt": "2025-12-06T11:55:44.483350002Z"
        }
      ],
      "testStrategy": "Integration tests: create task, verify in list. Update status, verify change. Soft delete, verify not in default list but exists in DB. Test all filter combinations. Verify only team members can access tasks",
      "title": "Build task board CRUD API with filtering and soft delete"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "3"
      ],
      "description": "Add OAuth2 authentication flows for Google and GitHub providers with callback handling and user account linking",
      "details": "1. Add dependency: oauth2 = \"4.4\"\n2. Create src/domain/oauth.rs with:\n   - OAuth2 client configuration for Google and GitHub\n   - GET /api/auth/oauth/:provider: redirect to provider authorization URL with state parameter\n   - GET /api/auth/oauth/:provider/callback: handle callback\n     * Exchange code for token\n     * Fetch user info from provider API\n     * Find or create user in database by email\n     * Generate JWT tokens\n3. Store OAuth state in Redis with 10min TTL for CSRF protection\n4. Add oauth_provider, oauth_id columns to users table\n5. Environment variables: GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET",
      "id": "7",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:56:20.011772630Z",
          "dependencies": [],
          "description": "Configure OAuth2 dependencies, create environment variable structure, and set up base OAuth2 client configuration for Google and GitHub providers with proper credential management.",
          "details": "1. Add oauth2 = \"4.4\" dependency to Cargo.toml\n2. Create .env.example with GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET, OAUTH_REDIRECT_BASE_URL\n3. Create src/domain/oauth.rs module file\n4. Implement OAuth2Config struct to hold provider configurations\n5. Create GoogleOAuthClient and GitHubOAuthClient structs with client_id, client_secret, redirect_uri, and authorization/token URLs\n6. Implement initialization functions that read from environment variables\n7. Add error handling for missing or invalid configuration\n8. Document required scopes for each provider (Google: email, profile; GitHub: user:email)",
          "id": 1,
          "parentId": "7",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for configuration parsing, integration tests verifying environment variable loading, test cases for missing credentials returning appropriate errors",
          "title": "Set up OAuth2 client configuration and environment variables",
          "updatedAt": "2025-12-06T11:56:20.011772630Z"
        },
        {
          "createdAt": "2025-12-06T11:56:20.011783338Z",
          "dependencies": [
            "1"
          ],
          "description": "Build the GET /api/auth/oauth/:provider endpoint that generates secure state tokens, stores them in Redis, and redirects users to provider authorization URLs with proper parameters.",
          "details": "1. Create GET /api/auth/oauth/:provider endpoint handler\n2. Validate provider parameter (must be 'google' or 'github')\n3. Generate cryptographically secure random state token (32 bytes, base64 encoded)\n4. Store state token in Redis with key format 'oauth:state:{token}' and 10-minute TTL\n5. Build authorization URL with client_id, redirect_uri, scope, state, and response_type=code\n6. For Google: use https://accounts.google.com/o/oauth2/v2/auth\n7. For GitHub: use https://github.com/login/oauth/authorize\n8. Return 302 redirect response to authorization URL\n9. Add logging for authorization requests\n10. Handle Redis connection errors gracefully",
          "id": 2,
          "parentId": "7",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for state token generation and Redis storage, integration tests verifying redirect URLs contain correct parameters, security tests for state token randomness and uniqueness, tests for invalid provider names",
          "title": "Implement authorization redirect flow with CSRF state protection",
          "updatedAt": "2025-12-06T11:56:20.011783338Z"
        },
        {
          "createdAt": "2025-12-06T11:56:20.011783630Z",
          "dependencies": [
            "2"
          ],
          "description": "Implement GET /api/auth/oauth/:provider/callback endpoint to validate state, exchange authorization code for access token, and handle OAuth2 protocol errors from providers.",
          "details": "1. Create GET /api/auth/oauth/:provider/callback endpoint handler\n2. Extract 'code' and 'state' query parameters from callback URL\n3. Validate state parameter exists and retrieve from Redis (key: 'oauth:state:{state}')\n4. Delete state from Redis after validation (single-use token)\n5. Return 401 error if state is missing, invalid, or expired\n6. Exchange authorization code for access token via POST request\n7. For Google: POST to https://oauth2.googleapis.com/token\n8. For GitHub: POST to https://github.com/login/oauth/access_token\n9. Include client_id, client_secret, code, redirect_uri, grant_type=authorization_code\n10. Parse token response and extract access_token\n11. Handle OAuth errors (invalid_grant, invalid_client, etc.) with appropriate HTTP status codes\n12. Add comprehensive error logging",
          "id": 3,
          "parentId": "7",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for state validation logic, integration tests with mocked provider token endpoints, security tests for CSRF protection (invalid/missing state), tests for expired state tokens, error handling tests for various OAuth2 error responses",
          "title": "Build callback handler with token exchange",
          "updatedAt": "2025-12-06T11:56:20.011783630Z"
        },
        {
          "createdAt": "2025-12-06T11:56:20.011783755Z",
          "dependencies": [
            "3"
          ],
          "description": "Fetch user profile information from OAuth providers using access tokens, implement logic to find existing users by email or create new accounts, and link OAuth provider information to user records.",
          "details": "1. Create function to fetch user info from Google using access token (GET https://www.googleapis.com/oauth2/v2/userinfo)\n2. Create function to fetch user info from GitHub (GET https://api.github.com/user and https://api.github.com/user/emails)\n3. Parse provider responses to extract: email, name, provider user ID, avatar URL\n4. Query users table by email to find existing account\n5. If user exists: update oauth_provider and oauth_id columns, update last_login timestamp\n6. If user doesn't exist: create new user record with email, name, oauth_provider, oauth_id, and generated username\n7. Handle cases where email is not verified by provider (reject or flag)\n8. Handle multiple OAuth providers for same email (allow linking)\n9. Generate JWT access and refresh tokens for authenticated user\n10. Return tokens in response with user profile data\n11. Add transaction handling for database operations",
          "id": 4,
          "parentId": "7",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for user info parsing from both providers, integration tests for account creation and linking flows, tests for existing user scenarios, tests for email verification requirements, database transaction rollback tests",
          "title": "Implement user info fetching and account creation/linking logic",
          "updatedAt": "2025-12-06T11:56:20.011783755Z"
        },
        {
          "createdAt": "2025-12-06T11:56:20.011783880Z",
          "dependencies": [],
          "description": "Create and apply database migration to add OAuth-related columns to users table, with proper indexes and constraints to support provider-based authentication and account linking.",
          "details": "1. Create new migration file (e.g., 003_add_oauth_columns.sql)\n2. Add oauth_provider column to users table (VARCHAR(20), nullable, values: 'google', 'github', or NULL)\n3. Add oauth_id column to users table (VARCHAR(255), nullable, stores provider's unique user ID)\n4. Add oauth_linked_at column (TIMESTAMP, nullable, tracks when OAuth was linked)\n5. Create unique composite index on (oauth_provider, oauth_id) to prevent duplicate OAuth accounts\n6. Make password column nullable to support OAuth-only accounts\n7. Add CHECK constraint ensuring oauth_provider and oauth_id are both NULL or both NOT NULL\n8. Create migration rollback script\n9. Update user model struct to include new fields\n10. Document migration in schema documentation",
          "id": 5,
          "parentId": "7",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Migration tests verifying schema changes apply correctly, tests for unique constraint on oauth_provider+oauth_id, tests for nullable password field, rollback tests, integration tests creating OAuth users without passwords",
          "title": "Add database schema changes for OAuth provider tracking",
          "updatedAt": "2025-12-06T11:56:20.011783880Z"
        }
      ],
      "testStrategy": "Manual test: initiate OAuth flow, complete authorization, verify JWT returned. Unit test state validation. Test account linking when email exists. Mock provider APIs for integration tests",
      "title": "Implement OAuth2 integration for Google and GitHub"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "3",
        "5"
      ],
      "description": "Implement granular permission system for owner, admin, member, and viewer roles with route-level authorization guards",
      "details": "1. Define Role enum in src/domain/auth.rs: Owner, Admin, Member, Viewer\n2. Create src/api/middleware/authorize.rs:\n   - fn require_role(min_role: Role) -> Middleware\n   - Extract user_id and team_id from request\n   - Query team_members for user's role\n   - Compare with required role hierarchy: Owner > Admin > Member > Viewer\n   - Return 403 Forbidden if insufficient permissions\n3. Define permission matrix:\n   - Owner: all operations including team deletion\n   - Admin: manage members, tasks, settings\n   - Member: create/edit own tasks, view all\n   - Viewer: read-only access\n4. Apply middleware to route groups in router configuration\n5. Create helper function: async fn check_team_access(user_id, team_id, min_role) -> Result<()>",
      "id": "8",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:56:49.946009921Z",
          "dependencies": [],
          "description": "Create the Role enum in src/domain/auth.rs and document the complete permission matrix for all four roles (Owner, Admin, Member, Viewer) with clear hierarchical relationships and operation boundaries.",
          "details": "1. Create src/domain/auth.rs if not exists\n2. Define Role enum with variants: Owner, Admin, Member, Viewer\n3. Implement PartialOrd and Ord traits for Role to enable hierarchy comparisons (Owner > Admin > Member > Viewer)\n4. Create a PermissionMatrix struct or const documentation that maps each role to specific operations:\n   - Owner: delete_team, transfer_ownership, manage_admins, manage_members, manage_tasks, manage_settings, view_all\n   - Admin: manage_members, manage_tasks, manage_settings, view_all\n   - Member: create_task, edit_own_task, view_all_tasks, view_team_info\n   - Viewer: view_all_tasks, view_team_info (read-only)\n5. Add comprehensive doc comments explaining permission inheritance and edge cases\n6. Create unit tests for role comparison logic",
          "id": 1,
          "parentId": "8",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for Role ordering, equality checks, and permission matrix documentation validation",
          "title": "Define role hierarchy and permission matrix with documentation",
          "updatedAt": "2025-12-06T11:56:49.946009921Z"
        },
        {
          "createdAt": "2025-12-06T11:56:49.946010921Z",
          "dependencies": [
            "1"
          ],
          "description": "Build the core authorization middleware in src/api/middleware/authorize.rs that extracts user context, queries team membership, validates role hierarchy, and returns appropriate HTTP responses for authorization decisions.",
          "details": "1. Create src/api/middleware/authorize.rs\n2. Implement require_role(min_role: Role) -> Middleware function that:\n   - Extracts user_id from authenticated session/JWT token\n   - Extracts team_id from request path parameters or body\n   - Queries team_members table for user's role in the specified team\n   - Compares user's role with required min_role using Role ordering\n   - Returns 403 Forbidden with descriptive error if insufficient permissions\n   - Returns 404 Not Found if user is not a team member\n   - Attaches user's role to request extensions for downstream handlers\n3. Implement caching strategy for role lookups to reduce database queries\n4. Add proper error handling and logging for authorization failures\n5. Include request context in error messages for debugging",
          "id": 2,
          "parentId": "8",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with mock database for various role combinations, cache hit/miss scenarios, and error conditions (invalid user, invalid team, insufficient permissions)",
          "title": "Implement authorization middleware with role checking logic",
          "updatedAt": "2025-12-06T11:56:49.946010921Z"
        },
        {
          "createdAt": "2025-12-06T11:56:49.946011296Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Develop reusable helper functions for team access checks that can be called from handlers and middleware, including async fn check_team_access and resource-specific permission validators.",
          "details": "1. In src/domain/auth.rs or src/services/auth.rs, implement:\n   - async fn check_team_access(db: &DbPool, user_id: Uuid, team_id: Uuid, min_role: Role) -> Result<Role, AuthError>\n   - async fn get_user_team_role(db: &DbPool, user_id: Uuid, team_id: Uuid) -> Result<Option<Role>, DbError>\n   - async fn can_manage_member(db: &DbPool, actor_id: Uuid, target_member_id: Uuid, team_id: Uuid) -> Result<bool, AuthError>\n   - async fn can_modify_task(db: &DbPool, user_id: Uuid, task_id: Uuid) -> Result<bool, AuthError>\n2. Implement resource ownership checks (e.g., Members can only edit their own tasks)\n3. Add helper for role transition validation (e.g., Admins cannot promote to Owner)\n4. Include comprehensive error types: InsufficientPermissions, NotTeamMember, ResourceNotFound\n5. Add query optimization with prepared statements",
          "id": 3,
          "parentId": "8",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit and integration tests covering all helper functions with various role combinations, ownership scenarios, edge cases like self-modification, and database error handling",
          "title": "Create helper functions for team access validation",
          "updatedAt": "2025-12-06T11:56:49.946011296Z"
        },
        {
          "createdAt": "2025-12-06T11:56:49.946011505Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Integrate the authorization middleware into all protected routes in the router configuration and create comprehensive test suite covering all permission boundaries, role transitions, and edge cases.",
          "details": "1. Update router configuration to apply require_role middleware to route groups:\n   - Team deletion: require_role(Role::Owner)\n   - Member management (add/remove/update): require_role(Role::Admin)\n   - Task creation: require_role(Role::Member)\n   - Task updates: require_role(Role::Member) + ownership check\n   - Settings updates: require_role(Role::Admin)\n   - Read operations: require_role(Role::Viewer)\n2. Create integration test suite testing:\n   - Each role accessing each endpoint (16 role-endpoint combinations minimum)\n   - Role boundary conditions (Member trying Admin operations)\n   - Ownership checks (Member editing another's task)\n   - Role transitions (promoting/demoting members)\n   - Edge cases: last Owner leaving, self-demotion, concurrent role changes\n3. Add end-to-end tests simulating real user workflows\n4. Document authorization patterns in API documentation\n5. Add metrics/logging for authorization decisions",
          "id": 4,
          "parentId": "8",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Comprehensive integration test suite with test matrix covering all role-endpoint-operation combinations, permission boundary tests, and edge case scenarios. Include load testing for authorization overhead.",
          "title": "Apply authorization to routes with comprehensive permission testing",
          "updatedAt": "2025-12-06T11:56:49.946011505Z"
        }
      ],
      "testStrategy": "Unit test role hierarchy comparison. Integration tests: create team as owner, add member with viewer role, verify viewer cannot create tasks but can read. Test admin can manage members but not delete team",
      "title": "Create role-based authorization middleware and permission checks"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "1",
        "4",
        "6"
      ],
      "description": "Implement WebSocket connection handling with Redis pub/sub for broadcasting task changes to connected team members",
      "details": "1. Add dependencies: tokio-tungstenite = \"0.21\", futures-util = \"0.3\"\n2. Create src/api/websocket.rs:\n   - GET /api/ws: upgrade_to_websocket(Query<{token: String}>) -> WebSocketUpgrade\n   - Validate JWT from query param\n   - Store connection in HashMap<UserId, Vec<WebSocket>>\n3. Implement Redis pub/sub listener in src/infra/notifications.rs:\n   - Subscribe to channel: task_updates:{team_id}\n   - On message, broadcast to all connected team members\n4. Modify task update handlers to publish events:\n   - redis.publish(\"task_updates:{team_id}\", json!({\"type\": \"task_updated\", \"task\": task}))\n5. Message format: {\"type\": \"task_created|task_updated|task_deleted\", \"task\": TaskDto, \"timestamp\": ISO8601}\n6. Handle connection cleanup on disconnect",
      "id": "9",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:57:27.005882508Z",
          "dependencies": [],
          "description": "Create the WebSocket endpoint that handles HTTP upgrade requests and validates JWT tokens from query parameters. Add required dependencies (tokio-tungstenite, futures-util) and set up the basic WebSocket route structure.",
          "details": "Add dependencies to Cargo.toml: tokio-tungstenite = \"0.21\", futures-util = \"0.3\". Create src/api/websocket.rs with GET /api/ws endpoint using upgrade_to_websocket handler. Extract JWT token from query parameter (?token=xxx), validate it using existing JWT validation logic, and extract user_id and team_id from claims. Return 401 Unauthorized for invalid tokens. On successful validation, upgrade the HTTP connection to WebSocket protocol and pass the authenticated user context to the connection handler.",
          "id": 1,
          "parentId": "9",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for JWT validation with valid/invalid/expired tokens. Integration test for WebSocket upgrade handshake. Test query parameter extraction and error responses for missing/malformed tokens.",
          "title": "Implement WebSocket upgrade handler with JWT authentication",
          "updatedAt": "2025-12-06T11:57:27.005882508Z"
        },
        {
          "createdAt": "2025-12-06T11:57:27.005963300Z",
          "dependencies": [
            "1"
          ],
          "description": "Build a thread-safe connection registry that maintains mappings between user IDs and their active WebSocket connections, supporting multiple concurrent connections per user.",
          "details": "Create a ConnectionManager struct using Arc<RwLock<HashMap<UserId, Vec<WebSocketSender>>>> to store active connections. Implement methods: add_connection(user_id, socket), remove_connection(user_id, connection_id), get_connections_for_user(user_id), and get_connections_for_team(team_id). Each connection should have a unique connection_id (UUID) for identification. Use tokio channels (mpsc) for sending messages to individual WebSocket connections. Store additional metadata: team_id, connected_at timestamp, last_ping timestamp. Implement connection registry as a singleton accessible via application state.",
          "id": 2,
          "parentId": "9",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for concurrent add/remove operations using tokio::spawn. Test multiple connections per user. Verify thread safety with concurrent read/write operations. Test connection retrieval by user and team.",
          "title": "Create connection management system with user-to-socket mapping",
          "updatedAt": "2025-12-06T11:57:27.005963300Z"
        },
        {
          "createdAt": "2025-12-06T11:57:27.005964008Z",
          "dependencies": [
            "2"
          ],
          "description": "Implement Redis pub/sub subscriber that listens for task update events on team-specific channels and deserializes incoming messages for distribution to WebSocket clients.",
          "details": "Create src/infra/notifications.rs with RedisSubscriber struct. Implement async function subscribe_to_task_updates() that connects to Redis and subscribes to pattern: task_updates:*. Spawn a tokio task that continuously listens for messages using redis::aio::PubSub. Parse incoming messages as JSON with structure: {\"type\": \"task_created|task_updated|task_deleted\", \"task\": TaskDto, \"team_id\": i32, \"timestamp\": String}. Extract team_id from channel name (task_updates:{team_id}). On message receipt, pass the event to the ConnectionManager for broadcasting. Implement error handling for Redis connection failures with exponential backoff retry logic (max 5 retries). Add logging for subscription lifecycle events.",
          "id": 3,
          "parentId": "9",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration test with real Redis instance publishing test messages. Verify message deserialization and error handling. Test reconnection logic by simulating Redis disconnection. Mock Redis for unit tests of message parsing logic.",
          "title": "Set up Redis pub/sub listener for task events",
          "updatedAt": "2025-12-06T11:57:27.005964008Z"
        },
        {
          "createdAt": "2025-12-06T11:57:27.005964258Z",
          "dependencies": [
            "3"
          ],
          "description": "Create the message distribution system that receives task events from Redis and broadcasts them to all authorized WebSocket connections for the affected team, with proper error handling for failed deliveries.",
          "details": "Implement broadcast_to_team(team_id, message) method in ConnectionManager. Query all connections where team_id matches the event's team. Serialize message to JSON string format: {\"type\": \"task_created|task_updated|task_deleted\", \"task\": TaskDto, \"timestamp\": ISO8601}. Send message concurrently to all team connections using tokio::spawn for each send operation. Handle send failures by logging errors and marking connections for cleanup (don't block on failed sends). Implement message batching for high-frequency updates (buffer messages for 50ms). Add metrics tracking: messages_sent, messages_failed, connections_active. Modify existing task CRUD handlers (create, update, delete) to publish events: redis.publish(\"task_updates:{team_id}\", json!({...})). Ensure events are published after database transaction commits.",
          "id": 4,
          "parentId": "9",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration test creating/updating/deleting tasks and verifying WebSocket clients receive messages. Test with multiple concurrent connections. Verify only team members receive events (authorization check). Test message format compliance. Load test with 100+ concurrent connections and high-frequency task updates.",
          "title": "Implement event broadcasting logic to team members",
          "updatedAt": "2025-12-06T11:57:27.005964258Z"
        },
        {
          "createdAt": "2025-12-06T11:57:27.005964425Z",
          "dependencies": [
            "4"
          ],
          "description": "Implement comprehensive connection lifecycle handling including disconnect detection, automatic cleanup, heartbeat/ping-pong mechanism, and graceful shutdown procedures.",
          "details": "Implement WebSocket message loop that handles: incoming ping/pong frames, close frames, and client messages. Add heartbeat mechanism: send ping every 30 seconds, disconnect if no pong received within 10 seconds. On disconnect (close frame, error, or timeout), call ConnectionManager.remove_connection(user_id, connection_id) to clean up. Implement graceful shutdown: on server shutdown signal, send close frames to all connections with 5-second timeout. Add connection duration limits: warn at 24 hours, force disconnect at 48 hours. Implement connection_cleanup_task() background job running every 5 minutes to remove stale connections (no pong response). Log all connection lifecycle events: connect, disconnect, timeout. Add metrics: connection_duration_seconds, active_connections_gauge, disconnects_total counter. Handle edge cases: client reconnection with same user_id, network interruptions, server restarts.",
          "id": 5,
          "parentId": "9",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test disconnect scenarios: client closes connection, network timeout, server-initiated close. Verify cleanup removes connections from registry. Test heartbeat mechanism with delayed/missing pongs. Load test with 500+ concurrent connections, measure memory usage and CPU. Test reconnection scenarios and verify no connection leaks. Simulate server restart and verify graceful shutdown.",
          "title": "Add connection lifecycle management and cleanup",
          "updatedAt": "2025-12-06T11:57:27.005964425Z"
        }
      ],
      "testStrategy": "Integration test: connect 2 clients to same team, update task via REST API, verify both receive WebSocket message. Test 1000 concurrent connections. Test reconnection handling. Verify messages only sent to team members",
      "title": "Build WebSocket endpoint for real-time task updates"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "2",
        "6"
      ],
      "description": "Set up email service integration with template rendering for task mentions and due date reminders, respecting user preferences",
      "details": "1. Add dependencies: lettre = \"0.11\", tera = \"1.19\"\n2. Create src/infra/email.rs with SMTP configuration from env vars\n3. Add user_preferences table: (user_id uuid PRIMARY KEY, email_mentions bool, email_due_dates bool, email_frequency varchar)\n4. Create email templates in templates/:\n   - mention_notification.html: \"@{user} mentioned you in {task}\"\n   - due_date_reminder.html: \"Task {title} is due in {hours} hours\"\n5. Implement notification queue in src/domain/notifications.rs:\n   - fn send_mention_email(user_id, task_id, mentioner_id)\n   - Check user preferences before sending\n   - Parse task description for @mentions on task create/update\n6. Create background job (tokio::spawn) to check due dates every hour:\n   - Query tasks WHERE due_date BETWEEN NOW() AND NOW() + 24 hours\n   - Send reminders to assignees if email_due_dates = true",
      "id": "10",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:58:06.020969179Z",
          "dependencies": [],
          "description": "Configure email service with SMTP settings from environment variables and implement HTML template rendering using Tera for email notifications",
          "details": "1. Add dependencies to Cargo.toml: lettre = \"0.11\" for SMTP email sending, tera = \"1.19\" for template rendering\n2. Create src/infra/email.rs module with EmailService struct\n3. Implement SMTP configuration loading from environment variables (SMTP_HOST, SMTP_PORT, SMTP_USERNAME, SMTP_PASSWORD, FROM_EMAIL)\n4. Create Tera template engine initialization with templates directory\n5. Create templates/ directory structure\n6. Implement send_email() function that accepts template name, context data, recipient email\n7. Add error handling for SMTP connection failures and template rendering errors\n8. Create email templates: templates/mention_notification.html with placeholders for {mentioner_name}, {task_title}, {task_id}, {task_url} and templates/due_date_reminder.html with placeholders for {task_title}, {due_date}, {hours_remaining}, {task_url}\n9. Implement template rendering with proper HTML escaping and styling",
          "id": 1,
          "parentId": "10",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for template rendering with mock data, integration tests for SMTP connection using test email service (like Mailtrap), verify HTML output formatting and variable substitution",
          "title": "Set up SMTP configuration and email template rendering system",
          "updatedAt": "2025-12-06T11:58:06.020969179Z"
        },
        {
          "createdAt": "2025-12-06T11:58:06.020970429Z",
          "dependencies": [
            "1"
          ],
          "description": "Design and implement database schema for user email preferences with CRUD operations to manage notification settings per user",
          "details": "1. Create migration file for user_preferences table with columns: user_id (uuid PRIMARY KEY, FOREIGN KEY to users table), email_mentions (boolean DEFAULT true), email_due_dates (boolean DEFAULT true), email_frequency (varchar DEFAULT 'immediate', options: 'immediate', 'hourly', 'daily'), created_at (timestamp), updated_at (timestamp)\n2. Create src/domain/user_preferences.rs with UserPreferences struct and repository trait\n3. Implement PostgreSQL repository in src/infra/repositories/user_preferences_repo.rs\n4. Add CRUD operations: get_preferences(user_id), update_preferences(user_id, preferences), create_default_preferences(user_id)\n5. Create API endpoints in src/api/preferences.rs: GET /api/users/{id}/preferences, PUT /api/users/{id}/preferences\n6. Add request/response DTOs for preference updates\n7. Implement middleware to ensure users can only modify their own preferences\n8. Add database indexes on user_id for efficient lookups",
          "id": 2,
          "parentId": "10",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Database integration tests for CRUD operations, API endpoint tests with authentication, test default preference creation on user registration, verify foreign key constraints",
          "title": "Create user preferences table and management endpoints",
          "updatedAt": "2025-12-06T11:58:06.020970429Z"
        },
        {
          "createdAt": "2025-12-06T11:58:06.020971054Z",
          "dependencies": [
            "2"
          ],
          "description": "Create parser to extract @mentions from task descriptions and titles, validate mentioned users exist, and store mention relationships",
          "details": "1. Create src/domain/mentions.rs module with mention parsing logic\n2. Implement parse_mentions() function using regex to extract @username or @user_id patterns from text\n3. Create mentions table via migration: mention_id (uuid PRIMARY KEY), task_id (uuid FOREIGN KEY), mentioned_user_id (uuid FOREIGN KEY), mentioner_user_id (uuid FOREIGN KEY), created_at (timestamp)\n4. Implement validate_mentioned_users() to check if mentioned users exist in database\n5. Create MentionRepository with methods: create_mention(), get_mentions_for_task(), get_mentions_for_user()\n6. Integrate mention parsing into task creation and update handlers\n7. Add logic to detect new mentions (compare old vs new description on updates)\n8. Handle edge cases: invalid usernames, self-mentions, duplicate mentions in same text\n9. Add database indexes on task_id and mentioned_user_id for efficient queries",
          "id": 3,
          "parentId": "10",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for regex parsing with various mention formats, test mention extraction from complex text, validate user existence checks, integration tests for mention storage, test duplicate mention handling",
          "title": "Implement mention parsing logic in task descriptions",
          "updatedAt": "2025-12-06T11:58:06.020971054Z"
        },
        {
          "createdAt": "2025-12-06T11:58:06.020971221Z",
          "dependencies": [
            "3"
          ],
          "description": "Implement the notification queue and email sending logic for task mentions, respecting user email preferences before sending notifications",
          "details": "1. Create src/domain/notifications.rs module with notification queue logic\n2. Implement send_mention_email(user_id, task_id, mentioner_id) async function\n3. Add preference check: query user_preferences.email_mentions before sending\n4. Fetch user details (email, name) and task details (title, description) for email context\n5. Prepare template context with mentioner name, task title, task URL\n6. Call EmailService.send_email() with mention_notification.html template\n7. Implement notification batching for email_frequency preferences (store pending notifications in notifications table)\n8. Create process_notification_queue() function to handle batched notifications\n9. Add error handling and retry logic for failed email deliveries\n10. Log all notification attempts (sent, failed, skipped due to preferences)\n11. Integrate into task creation/update workflow: after saving task, trigger mention detection and notification sending\n12. Use tokio::spawn for async email sending to avoid blocking request handlers",
          "id": 4,
          "parentId": "10",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Mock email service to verify notification sending logic, test preference filtering (should not send when disabled), test notification batching for different frequency settings, integration tests with real database, verify async execution doesn't block API responses",
          "title": "Build mention notification system with preference checks",
          "updatedAt": "2025-12-06T11:58:06.020971221Z"
        },
        {
          "createdAt": "2025-12-06T11:58:06.020971387Z",
          "dependencies": [
            "4"
          ],
          "description": "Implement scheduled background job using tokio to check for upcoming task due dates and send reminder emails to assignees based on their preferences",
          "details": "1. Create src/jobs/due_date_reminder.rs module for background job\n2. Implement check_due_dates() async function that queries: SELECT * FROM tasks WHERE due_date BETWEEN NOW() AND NOW() + INTERVAL '24 hours' AND status != 'completed'\n3. For each task, get assignee user_id and check user_preferences.email_due_dates\n4. Calculate hours_remaining = (due_date - now).hours()\n5. Prepare email context with task_title, due_date, hours_remaining, task_url\n6. Send email using due_date_reminder.html template via EmailService\n7. Track sent reminders in task_reminders table (task_id, user_id, reminder_sent_at) to avoid duplicate reminders\n8. Implement tokio::spawn background task in main.rs that runs check_due_dates() every hour using tokio::time::interval\n9. Add graceful shutdown handling for background job\n10. Implement error recovery: log failures, continue processing remaining tasks if one fails\n11. Add configurable reminder thresholds (24h, 48h) via environment variables\n12. Create health check endpoint to monitor background job status",
          "id": 5,
          "parentId": "10",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for due date calculation logic, mock database queries to test task filtering, test reminder deduplication logic, integration test with test database and future due dates, verify hourly scheduling with tokio::time::advance, test error handling when email service fails, load test with large number of due tasks",
          "title": "Create due date reminder background job with scheduling",
          "updatedAt": "2025-12-06T11:58:06.020971387Z"
        }
      ],
      "testStrategy": "Unit test mention parsing. Integration test: create task with @mention, verify email sent (use mailhog in dev). Test preference controls prevent emails. Test due date reminder job with mock time",
      "title": "Implement email notification system for mentions and due dates"
    },
    {
      "agentHint": "tap",
      "dependencies": [
        "6",
        "10"
      ],
      "description": "Integrate Firebase Cloud Messaging for sending push notifications to mobile devices on task updates and mentions",
      "details": "1. Add dependency: fcm = \"0.9\"\n2. Create device_tokens table: (user_id uuid, device_token varchar, platform varchar, created_at timestamptz)\n3. Add POST /api/users/devices endpoint to register FCM tokens\n4. Create src/infra/push.rs:\n   - Initialize FCM client with service account JSON from env\n   - fn send_push(user_id, notification: PushNotification) -> Result<()>\n     * Query device_tokens for user\n     * Send to all registered devices\n     * Handle invalid tokens (remove from DB)\n5. Integrate with notification system:\n   - On task assignment, send push: \"You were assigned {task_title}\"\n   - On mention, send push: \"{user} mentioned you\"\n6. Add push_notifications bool to user_preferences",
      "id": "11",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:58:37.417331263Z",
          "dependencies": [],
          "description": "Configure Firebase Cloud Messaging client infrastructure including dependency setup, service account authentication, and basic FCM client initialization for sending push notifications to mobile devices.",
          "details": "1. Add fcm = \"0.9\" dependency to Cargo.toml\n2. Create environment variable FCM_SERVICE_ACCOUNT_JSON for service account credentials path\n3. Create src/infra/push.rs module with FCM client initialization\n4. Implement FcmClient struct with methods: new() to initialize from service account JSON, and validate_connection() to test FCM connectivity\n5. Add error handling for missing/invalid service account credentials\n6. Create unit tests for client initialization with mock credentials\n7. Document required FCM service account JSON format and setup steps in README",
          "id": 1,
          "parentId": "11",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for client initialization with valid/invalid credentials. Integration test to verify FCM connection with test service account. Manual verification of service account JSON loading from environment variable.",
          "title": "Set up FCM client with service account configuration",
          "updatedAt": "2025-12-06T11:58:37.417331263Z"
        },
        {
          "createdAt": "2025-12-06T11:58:37.417332930Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement database schema for storing device tokens and create REST API endpoint for mobile clients to register their FCM tokens with the backend system.",
          "details": "1. Create migration for device_tokens table with columns: id (uuid primary key), user_id (uuid foreign key to users), device_token (varchar 255 unique), platform (varchar 20, values: 'ios'/'android'), created_at (timestamptz), updated_at (timestamptz)\n2. Add index on user_id for efficient lookups\n3. Create src/models/device_token.rs with DeviceToken struct and database operations\n4. Implement POST /api/users/devices endpoint in src/routes/users.rs\n5. Add request validation: require authentication, validate device_token format (non-empty, max 255 chars), validate platform enum\n6. Implement upsert logic: if token exists for user+platform, update timestamp; otherwise insert new record\n7. Add endpoint tests with valid/invalid tokens and authentication scenarios",
          "id": 2,
          "parentId": "11",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Database migration tests to verify schema creation. API integration tests for device registration with valid tokens, duplicate tokens, invalid platforms, and unauthenticated requests. Test token uniqueness constraint and foreign key relationships.",
          "title": "Create device token registration endpoint and database schema",
          "updatedAt": "2025-12-06T11:58:37.417332930Z"
        },
        {
          "createdAt": "2025-12-06T11:58:37.417333638Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Build the core push notification sending functionality including querying device tokens, sending notifications via FCM, and handling invalid or expired tokens with automatic cleanup.",
          "details": "1. Create PushNotification struct in src/infra/push.rs with fields: title (String), body (String), data (HashMap<String, String>)\n2. Implement send_push(user_id: Uuid, notification: PushNotification) -> Result<()> function\n3. Query device_tokens table for all tokens belonging to user_id\n4. Iterate through tokens and send FCM message to each device using fcm crate\n5. Implement error handling: catch FCM errors for invalid/expired tokens (NotRegistered, InvalidRegistration)\n6. On invalid token error, delete the token from device_tokens table\n7. Log successful sends and failures with appropriate detail levels\n8. Add retry logic for transient FCM service errors (max 3 retries with exponential backoff)\n9. Create helper function send_push_to_multiple_users for batch operations\n10. Add unit tests with mocked FCM responses and integration tests with FCM test tokens",
          "id": 3,
          "parentId": "11",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mocked FCM client for success/failure scenarios. Test invalid token cleanup by simulating FCM error responses. Integration tests using FCM test tokens from Firebase console. Verify database cleanup after invalid token errors. Test retry logic with transient failures.",
          "title": "Implement push notification sending logic with error handling",
          "updatedAt": "2025-12-06T11:58:37.417333638Z"
        },
        {
          "createdAt": "2025-12-06T11:58:37.417334013Z",
          "dependencies": [
            "3"
          ],
          "description": "Connect push notification functionality to existing notification events (task assignments, mentions) and implement user preference controls to allow users to opt in/out of push notifications.",
          "details": "1. Add push_notifications boolean column to user_preferences table (default true) via migration\n2. Update src/models/user_preferences.rs to include push_notifications field\n3. Add PUT /api/users/preferences/push endpoint to toggle push notification preference\n4. In task assignment handler (src/services/tasks.rs), after creating task assignment: check if assignee has push_notifications enabled, then call send_push with notification \"You were assigned {task_title}\"\n5. In mention handler (src/services/mentions.rs), after detecting mention: check if mentioned user has push_notifications enabled, then call send_push with notification \"{mentioning_user} mentioned you in {context}\"\n6. Add notification data payload: include task_id, notification_type, and deep link URL for mobile app navigation\n7. Create integration tests simulating task assignment and mention flows with push notifications enabled/disabled\n8. Test with real iOS and Android devices using TestFlight/internal testing builds\n9. Add monitoring/logging for push notification delivery rates and failures",
          "id": 4,
          "parentId": "11",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for task assignment and mention flows with push enabled/disabled preferences. End-to-end testing with real mobile devices (iOS/Android) to verify notification delivery and deep linking. Test preference toggle endpoint. Verify notifications are not sent when preference is disabled. Monitor logs for delivery confirmation.",
          "title": "Integrate push notifications with notification system and user preferences",
          "updatedAt": "2025-12-06T11:58:37.417334013Z"
        }
      ],
      "testStrategy": "Manual test with real device token. Unit test notification payload construction. Integration test: register device, trigger task event, verify FCM API called. Test invalid token removal",
      "title": "Add FCM push notification integration for mobile"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "5",
        "6"
      ],
      "description": "Create React frontend with TypeScript, Tailwind CSS, Kanban board view using react-beautiful-dnd, and dark/light theme support",
      "details": "1. Initialize React app: `npx create-react-app frontend --template typescript`\n2. Install dependencies:\n   - npm install tailwindcss @headlessui/react @heroicons/react\n   - npm install react-beautiful-dnd @types/react-beautiful-dnd\n   - npm install axios react-router-dom zustand\n3. Configure Tailwind with dark mode: class strategy in tailwind.config.js\n4. Create components/:\n   - KanbanBoard.tsx: 3 columns (To Do, In Progress, Done)\n   - TaskCard.tsx: draggable card with title, assignee, due date\n   - DragDropContext with onDragEnd handler\n5. Implement state management with Zustand:\n   - Store: tasks, teams, currentUser, theme\n   - Actions: fetchTasks, updateTaskStatus, toggleTheme\n6. API client in services/api.ts with axios interceptors for JWT\n7. On drag end, call PATCH /api/tasks/:id with new status",
      "id": "12",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:59:22.153258672Z",
          "dependencies": [],
          "description": "Set up the React application with TypeScript template, install and configure Tailwind CSS with dark mode support using class strategy, and set up the basic project structure with necessary folders.",
          "details": "1. Run `npx create-react-app frontend --template typescript` to initialize the project\n2. Install Tailwind CSS: `npm install -D tailwindcss postcss autoprefixer`\n3. Run `npx tailwindcss init -p` to generate config files\n4. Configure tailwind.config.js with darkMode: 'class' and content paths\n5. Add Tailwind directives to src/index.css\n6. Create folder structure: src/components/, src/services/, src/store/, src/types/, src/hooks/\n7. Install additional UI dependencies: `npm install @headlessui/react @heroicons/react`\n8. Create a basic App.tsx with dark mode class toggling capability\n9. Test that Tailwind classes work and dark mode can be toggled manually",
          "id": 1,
          "parentId": "12",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Manual testing: verify Tailwind styles apply correctly, test dark mode toggle by adding/removing 'dark' class to html element, ensure TypeScript compilation works without errors",
          "title": "Initialize React project with TypeScript and configure Tailwind CSS with dark mode",
          "updatedAt": "2025-12-06T11:59:22.153258672Z"
        },
        {
          "createdAt": "2025-12-06T11:59:22.153268672Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement Zustand store for global state management including tasks, teams, users, and theme. Create an Axios-based API client with JWT authentication interceptors and error handling.",
          "details": "1. Install dependencies: `npm install zustand axios react-router-dom`\n2. Create src/types/index.ts with TypeScript interfaces for Task, Team, User, and AppState\n3. Create src/store/useStore.ts with Zustand store containing:\n   - State: tasks[], teams[], currentUser, theme ('light'|'dark'), isLoading, error\n   - Actions: fetchTasks, fetchTeams, updateTaskStatus, setCurrentUser, toggleTheme, setLoading, setError\n4. Create src/services/api.ts with Axios instance:\n   - Base URL configuration from environment variables\n   - Request interceptor to add JWT token from localStorage\n   - Response interceptor for error handling and token refresh\n   - API methods: getTasks(), getTeams(), updateTask(id, data), etc.\n5. Implement localStorage persistence for JWT token and theme preference\n6. Add error boundary component for graceful error handling",
          "id": 2,
          "parentId": "12",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: test Zustand store actions and state updates using @testing-library/react. Mock Axios calls to test API client interceptors and error handling. Verify localStorage operations for token and theme persistence.",
          "title": "Set up Zustand state management and API client with authentication",
          "updatedAt": "2025-12-06T11:59:22.153268672Z"
        },
        {
          "createdAt": "2025-12-06T11:59:22.153270506Z",
          "dependencies": [
            "2"
          ],
          "description": "Build the main KanbanBoard component with three columns (To Do, In Progress, Done) and implement the layout structure using Tailwind CSS with responsive design and dark mode support.",
          "details": "1. Create src/components/KanbanBoard.tsx as the main container component\n2. Create src/components/KanbanColumn.tsx for individual columns\n3. Implement column structure with headers showing column title and task count\n4. Use Tailwind grid/flex layout for responsive 3-column design\n5. Add dark mode styling with dark: prefixes for all components\n6. Connect to Zustand store to fetch and display tasks grouped by status\n7. Implement loading and error states with appropriate UI feedback\n8. Add empty state messaging when no tasks exist in a column\n9. Create src/components/Header.tsx with app title and theme toggle button\n10. Use @heroicons/react for icons (sun/moon for theme toggle)\n11. Implement useEffect to fetch initial data on component mount",
          "id": 3,
          "parentId": "12",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests: render KanbanBoard with mock data, verify three columns render correctly, test loading and error states, verify tasks are grouped by status. Test responsive layout at different breakpoints. Verify dark mode classes apply correctly.",
          "title": "Create the KanbanBoard component structure with columns",
          "updatedAt": "2025-12-06T11:59:22.153270506Z"
        },
        {
          "createdAt": "2025-12-06T11:59:22.153270714Z",
          "dependencies": [
            "3"
          ],
          "description": "Create a comprehensive TaskCard component that displays task information including title, description, assignee, due date, priority, and status with proper styling and dark mode support.",
          "details": "1. Create src/components/TaskCard.tsx with TypeScript props interface\n2. Display task properties:\n   - Title (bold, truncated with ellipsis if too long)\n   - Description (truncated with 'Read more' expansion)\n   - Assignee with avatar or initials badge\n   - Due date with formatting and overdue highlighting\n   - Priority indicator (color-coded badge: high=red, medium=yellow, low=green)\n   - Tags/labels if applicable\n3. Style card with Tailwind: rounded corners, shadow, hover effects, padding\n4. Implement dark mode variants for all colors and backgrounds\n5. Add visual indicators for overdue tasks (red border/background)\n6. Make cards visually distinct but not cluttered\n7. Add onClick handler for future task detail modal\n8. Ensure proper TypeScript typing for all props\n9. Make component reusable and maintain consistent spacing",
          "id": 4,
          "parentId": "12",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests: render TaskCard with various task data combinations, verify all fields display correctly, test truncation behavior, verify date formatting, test priority color coding, verify dark mode styling, test click handlers. Snapshot testing for consistent rendering.",
          "title": "Implement TaskCard component with all task details",
          "updatedAt": "2025-12-06T11:59:22.153270714Z"
        },
        {
          "createdAt": "2025-12-06T11:59:22.153270922Z",
          "dependencies": [
            "4"
          ],
          "description": "Integrate react-beautiful-dnd library to enable drag-and-drop functionality for tasks between columns, implement onDragEnd handler to update task status, and sync changes with the backend API.",
          "details": "1. Install react-beautiful-dnd: `npm install react-beautiful-dnd @types/react-beautiful-dnd`\n2. Wrap KanbanBoard with DragDropContext component\n3. Wrap each KanbanColumn with Droppable component with unique droppableId\n4. Wrap each TaskCard with Draggable component with unique draggableId and index\n5. Implement onDragEnd handler:\n   - Extract source and destination from result object\n   - Return early if dropped outside droppable area\n   - Calculate new status based on destination droppableId\n   - Optimistically update local state immediately\n   - Call API to persist change: PATCH /api/tasks/:id with new status\n   - Handle API errors and revert state if update fails\n6. Add visual feedback during drag: opacity changes, placeholder styling\n7. Implement drag handle for better UX (optional icon to grab)\n8. Add smooth animations and transitions\n9. Ensure proper TypeScript typing for drag handlers\n10. Test drag behavior with different scenarios",
          "id": 5,
          "parentId": "12",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: mock drag-and-drop events to test onDragEnd handler, verify state updates correctly, mock API calls and verify correct endpoints are called with correct data, test error handling and state reversion on API failure. E2E tests: use Cypress or Playwright to simulate actual drag-and-drop interactions across columns.",
          "title": "Add drag-and-drop functionality with react-beautiful-dnd",
          "updatedAt": "2025-12-06T11:59:22.153270922Z"
        },
        {
          "createdAt": "2025-12-06T11:59:22.153271672Z",
          "dependencies": [
            "5"
          ],
          "description": "Complete the theme toggle functionality with localStorage persistence, add smooth transitions, implement routing if needed, and create comprehensive component and E2E tests for the entire dashboard.",
          "details": "1. Enhance theme toggle in Header component:\n   - Connect to Zustand toggleTheme action\n   - Apply/remove 'dark' class to document.documentElement\n   - Persist preference to localStorage\n   - Load saved theme on app initialization\n2. Add smooth transition for theme changes using Tailwind transition classes\n3. Ensure all components respect theme setting\n4. Set up react-router-dom for navigation (if multiple views needed)\n5. Create src/App.test.tsx with comprehensive tests:\n   - Test initial render and data fetching\n   - Test theme toggle functionality\n   - Test task status updates\n6. Create E2E tests using Cypress or Playwright:\n   - Test complete user flow: load dashboard  drag task  verify status change\n   - Test theme persistence across page reloads\n   - Test error scenarios and recovery\n7. Add accessibility features: keyboard navigation, ARIA labels, focus management\n8. Optimize performance: memoization, lazy loading if needed\n9. Create README.md with setup instructions and architecture overview\n10. Add environment variables configuration (.env.example)",
          "id": 6,
          "parentId": "12",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests: test theme toggle updates store and DOM, verify localStorage operations, test routing if implemented. E2E tests: full user journey testing with Cypress/Playwright including drag-and-drop, theme switching, API integration, error handling. Accessibility testing with axe-core. Performance testing with React DevTools Profiler.",
          "title": "Implement theme toggle and persistence with comprehensive testing",
          "updatedAt": "2025-12-06T11:59:22.153271672Z"
        }
      ],
      "testStrategy": "Component tests with React Testing Library: render KanbanBoard, verify columns. Test drag-and-drop updates task status. E2E test with Playwright: login, drag task, verify API called. Test theme toggle persists in localStorage",
      "title": "Build React dashboard with Kanban board and drag-and-drop"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "12",
        "8"
      ],
      "description": "Create React components for displaying team activity timeline and managing team members with role assignment",
      "details": "1. Add activity_log table: (id uuid, team_id uuid, user_id uuid, action varchar, entity_type varchar, entity_id uuid, timestamp timestamptz)\n2. Create GET /api/teams/:id/activity endpoint:\n   - Return last 50 activities with user details\n3. Build React components:\n   - ActivityFeed.tsx: timeline view with icons for each action type\n   - MemberList.tsx: table with member name, role, actions\n   - RoleSelector.tsx: dropdown to change member role (owner/admin only)\n4. Implement member management endpoints:\n   - POST /api/teams/:id/members: add member via invite code\n   - PATCH /api/teams/:id/members/:user_id: update role\n   - DELETE /api/teams/:id/members/:user_id: remove member\n5. Log activities on task create/update/delete, member add/remove\n6. Real-time activity updates via WebSocket",
      "id": "13",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T11:59:54.460820882Z",
          "dependencies": [],
          "description": "Implement the activity_log table schema and create backend service functions to log team activities including task operations and member changes. Set up database migrations and implement logging middleware.",
          "details": "1. Create database migration for activity_log table with fields: id (uuid, primary key), team_id (uuid, foreign key), user_id (uuid, foreign key), action (varchar - e.g., 'task_created', 'member_added'), entity_type (varchar - e.g., 'task', 'member'), entity_id (uuid), metadata (jsonb for additional context), timestamp (timestamptz, default now())\n2. Add indexes on team_id and timestamp for efficient querying\n3. Create ActivityLogService class with methods: logActivity(teamId, userId, action, entityType, entityId, metadata)\n4. Implement activity logging hooks in existing task CRUD operations (create, update, delete)\n5. Implement activity logging for member operations (add, remove, role change)\n6. Add error handling and transaction support to ensure activity logs are atomic with main operations",
          "id": 1,
          "parentId": "13",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for ActivityLogService methods, integration tests verifying activities are logged correctly during task and member operations, test transaction rollback scenarios to ensure data consistency",
          "title": "Create activity logging system with database schema and backend infrastructure",
          "updatedAt": "2025-12-06T11:59:54.460820882Z"
        },
        {
          "createdAt": "2025-12-06T11:59:54.460823674Z",
          "dependencies": [
            "1"
          ],
          "description": "Create GET /api/teams/:id/activity endpoint that retrieves the last 50 activities with joined user information, implements cursor-based pagination, and includes proper authorization checks.",
          "details": "1. Create GET /api/teams/:id/activity endpoint in teams router\n2. Implement authorization middleware to verify user is team member\n3. Query activity_log table with JOIN on users table to include user name, avatar\n4. Implement cursor-based pagination using timestamp (support ?cursor=<timestamp>&limit=50)\n5. Return activities in descending timestamp order with structure: { activities: [{ id, action, entityType, entityId, metadata, timestamp, user: { id, name, avatar } }], nextCursor: string | null, hasMore: boolean }\n6. Add filtering support for action types via query parameter ?actions=task_created,member_added\n7. Optimize query performance with proper indexes\n8. Add response caching headers (Cache-Control: private, max-age=30)",
          "id": 2,
          "parentId": "13",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests for authorized and unauthorized access, test pagination with various page sizes, verify user details are correctly joined, test filtering by action types, performance tests with large activity datasets",
          "title": "Implement activity feed API endpoint with pagination and user details",
          "updatedAt": "2025-12-06T11:59:54.460823674Z"
        },
        {
          "createdAt": "2025-12-06T11:59:54.460824882Z",
          "dependencies": [
            "2"
          ],
          "description": "Create ActivityFeed.tsx component that displays a timeline view of team activities with action-specific icons, implements infinite scroll pagination, and receives real-time updates via WebSocket connections.",
          "details": "1. Create ActivityFeed.tsx component with timeline UI using vertical layout\n2. Implement useActivityFeed custom hook to fetch activities from API with pagination\n3. Add infinite scroll using Intersection Observer API to load more activities\n4. Create ActivityItem subcomponent with icon mapping for each action type (task_created  plus icon, member_added  user-plus icon, etc.)\n5. Display relative timestamps (e.g., '2 hours ago') with full timestamp on hover\n6. Implement WebSocket connection to listen for 'team:activity' events\n7. Add new activities to top of feed in real-time when WebSocket events received\n8. Show visual notification (subtle animation) when new activities arrive\n9. Implement optimistic UI updates for user's own actions\n10. Add loading states, empty states ('No activities yet'), and error handling\n11. Style with Tailwind CSS for responsive design",
          "id": 3,
          "parentId": "13",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component unit tests with React Testing Library, test infinite scroll behavior, mock WebSocket events to verify real-time updates, test various activity types render correctly with appropriate icons, accessibility tests for timeline navigation",
          "title": "Build ActivityFeed React component with real-time WebSocket updates",
          "updatedAt": "2025-12-06T11:59:54.460824882Z"
        },
        {
          "createdAt": "2025-12-06T11:59:54.460825090Z",
          "dependencies": [
            "1"
          ],
          "description": "Build MemberList.tsx and RoleSelector.tsx components to display team members, implement role change functionality, member removal, and create member management API endpoints with proper authorization.",
          "details": "1. Implement backend endpoints: POST /api/teams/:id/members (add via invite code), PATCH /api/teams/:id/members/:user_id (update role - owner/admin only), DELETE /api/teams/:id/members/:user_id (remove member - owner/admin only)\n2. Add authorization middleware checking user role for admin operations\n3. Create MemberList.tsx component displaying table with columns: avatar, name, email, role, joined date, actions\n4. Create RoleSelector.tsx dropdown component with options: member, admin, owner (conditional rendering based on current user's role)\n5. Implement useMemberManagement hook for API calls (updateRole, removeMember, addMember)\n6. Add confirmation dialog for member removal using modal component\n7. Show success/error toast notifications for all operations\n8. Disable role changes and removal for: current user, last owner, users with higher/equal role\n9. Add invite member form with invite code input field\n10. Implement optimistic UI updates with rollback on error\n11. Log all member management activities using ActivityLogService\n12. Update member list in real-time via WebSocket when changes occur",
          "id": 4,
          "parentId": "13",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API endpoint tests for authorization (verify non-admins cannot modify roles), test role change constraints (cannot remove last owner), component tests for MemberList and RoleSelector interactions, integration tests for add/update/remove member flows, test WebSocket updates when other users modify members",
          "title": "Create member management UI components with role assignment and removal",
          "updatedAt": "2025-12-06T11:59:54.460825090Z"
        }
      ],
      "testStrategy": "Integration test: perform actions, verify logged in activity_log. UI test: render activity feed, verify actions displayed. Test role change UI requires admin. Test member removal",
      "title": "Implement team activity feed and member management UI"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "1"
      ],
      "description": "Implement metrics endpoint, structured JSON logging with trace IDs, and health check endpoints for production monitoring",
      "details": "1. Add dependencies: tracing = \"0.1\", tracing-subscriber = { version = \"0.3\", features = [\"json\"] }, metrics = \"0.21\", metrics-exporter-prometheus = \"0.13\"\n2. Initialize tracing subscriber in main.rs:\n   - JSON formatter with trace_id, span_id, level, message, timestamp\n   - Filter by RUST_LOG env var\n3. Create src/api/observability.rs:\n   - GET /metrics: expose Prometheus metrics (request count, duration histogram, active connections)\n   - GET /health/live: liveness probe (return 200 if server running)\n   - GET /health/ready: readiness probe (check DB and Redis connectivity)\n4. Add middleware to track metrics:\n   - HTTP request counter by method, path, status\n   - Request duration histogram\n   - Active WebSocket connections gauge\n5. Log key events: auth success/failure, task operations, errors with context\n6. Generate trace_id per request, propagate through call chain",
      "id": "14",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T12:00:31.017957593Z",
          "dependencies": [],
          "description": "Initialize tracing-subscriber with JSON formatting, implement trace ID generation and propagation through the request lifecycle, and configure log filtering via RUST_LOG environment variable.",
          "details": "1. Add dependencies to Cargo.toml: tracing = \"0.1\", tracing-subscriber = { version = \"0.3\", features = [\"json\", \"env-filter\"] }, uuid = { version = \"1.0\", features = [\"v4\"] }\n2. In main.rs, initialize tracing subscriber before starting the server:\n   - Use tracing_subscriber::fmt() with JSON formatter\n   - Include fields: trace_id, span_id, level, message, timestamp, target\n   - Set up EnvFilter to respect RUST_LOG environment variable\n3. Create middleware or extractor to generate UUID v4 trace_id for each incoming request\n4. Store trace_id in request extensions for propagation through handlers\n5. Add #[instrument] macro to key functions (auth, task operations) to automatically include trace_id in logs\n6. Test trace_id propagation by making requests and verifying logs contain consistent trace_id throughout request lifecycle\n7. Verify JSON log format is parseable and contains all required fields",
          "id": 1,
          "parentId": "14",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Create integration tests that make HTTP requests, capture log output, parse JSON logs, and verify: 1) All logs for a single request share the same trace_id, 2) JSON structure contains required fields (trace_id, level, message, timestamp), 3) RUST_LOG filtering works correctly, 4) Nested function calls maintain trace context",
          "title": "Set up structured JSON logging with tracing and trace ID propagation",
          "updatedAt": "2025-12-06T12:00:31.017957593Z"
        },
        {
          "createdAt": "2025-12-06T12:00:31.017958968Z",
          "dependencies": [
            "1"
          ],
          "description": "Set up Prometheus metrics registry, define application metrics (counters, histograms, gauges), and create /metrics endpoint to expose metrics in Prometheus format.",
          "details": "1. Add dependencies: metrics = \"0.21\", metrics-exporter-prometheus = \"0.13\"\n2. In main.rs, initialize Prometheus exporter and install as global recorder:\n   - Create PrometheusBuilder with appropriate settings\n   - Install recorder before starting server\n3. Define application metrics:\n   - Counter: http_requests_total (labels: method, path, status)\n   - Histogram: http_request_duration_seconds (labels: method, path)\n   - Gauge: active_websocket_connections\n   - Gauge: active_http_connections\n4. Create src/api/observability.rs module\n5. Implement GET /metrics endpoint:\n   - Use prometheus exporter's handle to render metrics\n   - Return text/plain content-type with Prometheus format\n   - Should not require authentication for monitoring systems\n6. Register /metrics route in router configuration\n7. Test endpoint returns valid Prometheus format that can be scraped",
          "id": 2,
          "parentId": "14",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: 1) Verify /metrics endpoint returns 200 status, 2) Parse response as Prometheus text format, 3) Verify metrics are present in output. Integration tests: 1) Make requests to other endpoints, 2) Scrape /metrics and verify counters incremented, 3) Verify histogram buckets are populated, 4) Test metrics survive across multiple requests",
          "title": "Implement Prometheus metrics collection and exposition endpoint",
          "updatedAt": "2025-12-06T12:00:31.017958968Z"
        },
        {
          "createdAt": "2025-12-06T12:00:31.017959760Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement liveness and readiness probe endpoints that verify application and dependency health, including database and Redis connectivity checks.",
          "details": "1. In src/api/observability.rs, implement two health check endpoints:\n2. GET /health/live (liveness probe):\n   - Simple endpoint that returns 200 OK if server is running\n   - Return JSON: {\"status\": \"ok\", \"timestamp\": <ISO8601>}\n   - Should always succeed if server is accepting requests\n3. GET /health/ready (readiness probe):\n   - Check database connectivity: execute simple query (SELECT 1)\n   - Check Redis connectivity: execute PING command\n   - Return 200 if all dependencies healthy, 503 if any fail\n   - Return JSON: {\"status\": \"ready\"|\"not_ready\", \"checks\": {\"database\": \"ok\"|\"error\", \"redis\": \"ok\"|\"error\"}, \"timestamp\": <ISO8601>}\n   - Include error details in response when checks fail\n4. Add timeout for dependency checks (e.g., 2 seconds each)\n5. Log health check failures with trace_id and error context\n6. Register both routes in router (should not require authentication)\n7. Test both success and failure scenarios for readiness checks",
          "id": 3,
          "parentId": "14",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: 1) Mock database and Redis connections, 2) Test /health/live always returns 200, 3) Test /health/ready returns 200 when all checks pass, 4) Test /health/ready returns 503 when DB fails, 5) Test /health/ready returns 503 when Redis fails, 6) Verify JSON response structure. Integration tests: 1) Test with real dependencies, 2) Simulate connection failures, 3) Verify timeout behavior",
          "title": "Create health check endpoints with dependency checking",
          "updatedAt": "2025-12-06T12:00:31.017959760Z"
        },
        {
          "createdAt": "2025-12-06T12:00:31.017960218Z",
          "dependencies": [
            "2"
          ],
          "description": "Implement middleware layer to automatically collect and record metrics for all HTTP requests and WebSocket connections, tracking request counts, durations, and active connections.",
          "details": "1. Create src/middleware/metrics.rs module\n2. Implement HTTP metrics middleware using tower/axum middleware:\n   - Record start time when request begins\n   - Increment http_requests_total counter with labels (method, path, status)\n   - Record http_request_duration_seconds histogram on completion\n   - Increment/decrement active_http_connections gauge\n   - Extract method, path (normalized), and status code for labels\n   - Handle path normalization (e.g., /tasks/123 -> /tasks/:id)\n3. Implement WebSocket connection tracking:\n   - Increment active_websocket_connections when connection established\n   - Decrement when connection closes\n   - Add to WebSocket handler logic\n4. Apply HTTP metrics middleware to all routes except /metrics (avoid recursion)\n5. Ensure metrics are recorded even for failed requests (errors, panics)\n6. Add structured logging within middleware for request start/completion\n7. Test metrics accuracy with concurrent requests and various response codes\n8. Verify path normalization works correctly for dynamic routes",
          "id": 4,
          "parentId": "14",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: 1) Make multiple requests with different methods/paths/statuses, 2) Scrape /metrics and verify counters match expected values, 3) Verify histogram buckets are populated correctly, 4) Test concurrent requests increment active connections gauge properly, 5) Simulate WebSocket connections and verify gauge increments/decrements, 6) Test error cases (4xx, 5xx) are counted correctly, 7) Verify /metrics endpoint itself is not counted in metrics",
          "title": "Add metrics middleware for HTTP requests and WebSocket connections",
          "updatedAt": "2025-12-06T12:00:31.017960218Z"
        }
      ],
      "testStrategy": "Unit test health check logic. Integration test: make requests, verify metrics incremented. Test /health/ready returns 503 when DB unavailable. Verify JSON logs parseable and contain trace_id",
      "title": "Create observability infrastructure with Prometheus and structured logging"
    },
    {
      "agentHint": "bolt",
      "dependencies": [
        "14"
      ],
      "description": "Create optimized Docker build with multi-stage compilation, Kubernetes deployment with HPA, and production-ready configuration",
      "details": "1. Create Dockerfile:\n   - Stage 1: rust:1.75 as builder, copy source, cargo build --release\n   - Stage 2: debian:bookworm-slim, copy binary and templates, install ca-certificates\n   - EXPOSE 3000, CMD [\"./teamsync-api\"]\n2. Create infra/k8s/:\n   - deployment.yaml: 3 replicas, resource limits (500m CPU, 512Mi memory), liveness/readiness probes\n   - service.yaml: ClusterIP service on port 80 -> 3000\n   - hpa.yaml: HorizontalPodAutoscaler targeting 70% CPU, min 3, max 10 replicas\n   - configmap.yaml: non-sensitive config (LOG_LEVEL, CORS_ORIGINS)\n   - secret.yaml: template for DATABASE_URL, JWT_SECRET, etc.\n   - ingress.yaml: TLS termination, path routing\n3. Add .dockerignore: target/, node_modules/, .git/\n4. Create docker-compose.yml for local dev: API, PostgreSQL, Redis\n5. Build script: docker build --target production -t teamsync-api:latest .",
      "id": "15",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T12:01:15.224449128Z",
          "dependencies": [],
          "description": "Build a production-ready Dockerfile using multi-stage compilation to create an optimized container image with Rust builder stage and minimal Debian runtime stage.",
          "details": "Create Dockerfile with two stages: (1) Builder stage using rust:1.75 base image - copy Cargo.toml, Cargo.lock, and source code, run 'cargo build --release' to compile the binary with optimizations; (2) Runtime stage using debian:bookworm-slim - install only ca-certificates and required runtime dependencies, copy compiled binary from builder stage, copy templates directory if needed, set working directory, EXPOSE port 3000, and set CMD to run the binary. Create .dockerignore file excluding target/, node_modules/, .git/, .env, *.log, and other development artifacts. Add build script with command: 'docker build --target production -t teamsync-api:latest .' Test the image build process and verify final image size is under 100MB.",
          "id": 1,
          "parentId": "15",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Build the Docker image and verify: (1) Build completes successfully without errors, (2) Final image size is optimized (under 100MB), (3) Binary runs correctly in container with 'docker run' test, (4) All required files (binary, templates, ca-certificates) are present in final image, (5) .dockerignore properly excludes unnecessary files",
          "title": "Create optimized multi-stage Dockerfile with minimal final image",
          "updatedAt": "2025-12-06T12:01:15.224449128Z"
        },
        {
          "createdAt": "2025-12-06T12:01:15.224459336Z",
          "dependencies": [
            "1"
          ],
          "description": "Create a production-ready Kubernetes deployment manifest with proper resource management, health checks, and replica configuration for the teamsync-api application.",
          "details": "Create infra/k8s/deployment.yaml with: apiVersion apps/v1, kind Deployment, metadata with name 'teamsync-api' and appropriate labels. Spec should include: replicas set to 3 for high availability, selector matching app labels, template with pod spec containing: container using teamsync-api:latest image with imagePullPolicy IfNotPresent, containerPort 3000, resource requests (250m CPU, 256Mi memory) and limits (500m CPU, 512Mi memory), livenessProbe on /health endpoint with initialDelaySeconds 30, periodSeconds 10, failureThreshold 3, readinessProbe on /ready endpoint with initialDelaySeconds 10, periodSeconds 5, failureThreshold 3. Include environment variables from ConfigMap and Secret references. Add pod annotations for Prometheus scraping if applicable.",
          "id": 2,
          "parentId": "15",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Validate deployment manifest: (1) Run 'kubectl apply --dry-run=client -f deployment.yaml' to check syntax, (2) Deploy to test cluster and verify all 3 replicas start successfully, (3) Test liveness probe by checking pod restarts on failure, (4) Test readiness probe by verifying traffic routing only to ready pods, (5) Verify resource limits are enforced using 'kubectl top pods'",
          "title": "Build Kubernetes deployment manifest with resource limits and probes",
          "updatedAt": "2025-12-06T12:01:15.224459336Z"
        },
        {
          "createdAt": "2025-12-06T12:01:15.224460544Z",
          "dependencies": [
            "2"
          ],
          "description": "Build Kubernetes service for internal communication, ingress for external access with TLS, and HorizontalPodAutoscaler for automatic scaling based on resource utilization.",
          "details": "Create three manifests: (1) infra/k8s/service.yaml - ClusterIP service named 'teamsync-api-service' exposing port 80 targeting containerPort 3000, with selector matching deployment labels; (2) infra/k8s/ingress.yaml - Ingress resource with TLS termination using cert-manager annotations, host configuration for api.teamsync.com, path routing rules for '/' to teamsync-api-service:80, include CORS and rate-limiting annotations; (3) infra/k8s/hpa.yaml - HorizontalPodAutoscaler targeting teamsync-api deployment, minReplicas 3, maxReplicas 10, metrics targeting 70% CPU utilization and optionally 80% memory utilization. Include behavior configuration for scale-up (stabilizationWindow 60s) and scale-down (stabilizationWindow 300s) to prevent flapping.",
          "id": 3,
          "parentId": "15",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test each component: (1) Deploy service and verify endpoints are populated with 'kubectl get endpoints', (2) Test service connectivity internally using 'kubectl run curl --rm -it --image=curlimages/curl -- curl teamsync-api-service', (3) Deploy ingress and verify external access through configured domain, (4) Deploy HPA and simulate load using 'kubectl run -it load-generator --image=busybox -- /bin/sh' with load testing, (5) Verify HPA scales up pods when CPU exceeds 70% and scales down after load decreases",
          "title": "Create service, ingress, and HPA configurations",
          "updatedAt": "2025-12-06T12:01:15.224460544Z"
        },
        {
          "createdAt": "2025-12-06T12:01:15.224460961Z",
          "dependencies": [
            "2"
          ],
          "description": "Create Kubernetes ConfigMap for non-sensitive configuration and Secret template for sensitive credentials with proper structure for environment-based deployment.",
          "details": "Create two configuration manifests: (1) infra/k8s/configmap.yaml - ConfigMap named 'teamsync-api-config' containing non-sensitive key-value pairs: LOG_LEVEL (info), RUST_LOG (info), CORS_ORIGINS (comma-separated list), API_PORT (3000), ENVIRONMENT (production), METRICS_ENABLED (true), and any other non-sensitive application settings; (2) infra/k8s/secret.yaml.template - Secret template (not actual secret) showing structure for sensitive values: DATABASE_URL, JWT_SECRET, REDIS_URL, API_KEY, with placeholder values and comments indicating these should be created separately per environment using 'kubectl create secret generic teamsync-api-secrets --from-literal=DATABASE_URL=xxx'. Include README.md in infra/k8s/ with instructions for creating secrets securely and managing environment-specific configurations.",
          "id": 4,
          "parentId": "15",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Validate configuration management: (1) Apply ConfigMap and verify with 'kubectl get configmap teamsync-api-config -o yaml', (2) Create test Secret from template and verify it's properly formatted, (3) Update deployment.yaml to reference ConfigMap and Secret, deploy and verify environment variables are correctly injected using 'kubectl exec pod -- env', (4) Test configuration changes by updating ConfigMap and verifying pods pick up changes after restart, (5) Verify secrets are base64 encoded and not exposed in logs",
          "title": "Set up ConfigMap and Secret templates for configuration management",
          "updatedAt": "2025-12-06T12:01:15.224460961Z"
        },
        {
          "createdAt": "2025-12-06T12:01:15.224461336Z",
          "dependencies": [
            "1"
          ],
          "description": "Build a comprehensive docker-compose configuration that orchestrates the API service with PostgreSQL database and Redis cache for local development and testing.",
          "details": "Create docker-compose.yml in project root with three services: (1) 'api' service - build from local Dockerfile, ports mapping 3000:3000, environment variables for DATABASE_URL, REDIS_URL, JWT_SECRET (dev values), volumes mounting source code for hot-reload if applicable, depends_on postgres and redis with health checks; (2) 'postgres' service - using postgres:15-alpine image, environment variables for POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD, volume for data persistence named 'postgres_data', ports 5432:5432, healthcheck using pg_isready; (3) 'redis' service - using redis:7-alpine image, ports 6379:6379, volume for persistence named 'redis_data', healthcheck using redis-cli ping. Include networks configuration for service isolation. Add docker-compose.override.yml.example for developer-specific overrides. Create setup script (scripts/dev-setup.sh) to initialize database schema and seed data.",
          "id": 5,
          "parentId": "15",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test local development environment: (1) Run 'docker-compose up -d' and verify all services start successfully, (2) Check service health with 'docker-compose ps' showing all healthy, (3) Test API connectivity with curl to http://localhost:3000/health, (4) Verify database connection by checking API logs for successful connection, (5) Test Redis connection through API endpoints that use caching, (6) Run 'docker-compose down -v' and verify clean teardown, (7) Test rebuild with 'docker-compose up --build' to ensure changes are picked up",
          "title": "Create docker-compose.yml for local development environment",
          "updatedAt": "2025-12-06T12:01:15.224461336Z"
        }
      ],
      "testStrategy": "Build Docker image, verify size < 100MB. Run container, test health endpoints. Deploy to local k8s (minikube), verify pods start. Test HPA scales under load (use hey or k6). Verify logs appear in JSON format",
      "title": "Build Docker multi-stage image and Kubernetes deployment manifests"
    }
  ]
}