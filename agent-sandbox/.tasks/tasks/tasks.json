{
  "metadata": {
    "completedCount": 0,
    "lastModified": "2025-12-06T17:06:52.598132213+00:00",
    "taskCount": 17,
    "version": "1.0.0"
  },
  "tasks": [
    {
      "agentHint": "rex",
      "dependencies": [],
      "description": "Set up the foundational Rust project structure with Axum 0.7, configure sqlx for PostgreSQL 15, and establish Redis 7 connections for caching and pub/sub",
      "details": "1. Initialize Cargo project with workspace structure\n2. Add dependencies: axum = \"0.7\", tokio = { version = \"1\", features = [\"full\"] }, sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-rustls\", \"migrate\"] }, redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }\n3. Create directory structure: src/{api, domain, infra}, frontend/, infra/\n4. Configure .env with DATABASE_URL, REDIS_URL\n5. Setup sqlx migrations directory\n6. Create main.rs with basic Axum server scaffold\n7. Implement Redis connection pool in infra/redis.rs\n8. Implement PostgreSQL connection pool in infra/database.rs",
      "id": "16",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T16:56:25.471557547Z",
          "dependencies": [],
          "description": "Initialize a new Cargo workspace project with proper dependency configuration for Axum 0.7, sqlx 0.7 with PostgreSQL support, Redis 0.24, and Tokio runtime. Configure workspace structure and create initial Cargo.toml with all required features.",
          "details": "1. Run 'cargo init --name project-name' to create new project\n2. Configure Cargo.toml with workspace structure if needed\n3. Add dependencies with exact versions and features:\n   - axum = \"0.7\"\n   - tokio = { version = \"1\", features = [\"full\"] }\n   - sqlx = { version = \"0.7\", features = [\"postgres\", \"runtime-tokio-rustls\", \"migrate\"] }\n   - redis = { version = \"0.24\", features = [\"tokio-comp\", \"connection-manager\"] }\n   - tower = \"0.4\" (for middleware)\n   - serde = { version = \"1.0\", features = [\"derive\"] }\n   - dotenv = \"0.15\" (for environment variable management)\n4. Run 'cargo build' to verify all dependencies resolve correctly\n5. Create .cargo/config.toml for any build optimizations if needed",
          "id": 1,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify successful compilation with 'cargo build'. Check that all dependencies are downloaded and no version conflicts exist. Ensure Cargo.lock is generated correctly.",
          "title": "Setup Cargo workspace with Axum, sqlx, and Redis dependencies",
          "updatedAt": "2025-12-06T16:56:25.471557547Z"
        },
        {
          "createdAt": "2025-12-06T16:56:25.471561089Z",
          "dependencies": [
            "1"
          ],
          "description": "Establish the project directory structure with api, domain, and infra modules. Create main.rs with a basic Axum server that starts on a configured port and includes health check endpoint. Setup .env file for configuration management.",
          "details": "1. Create directory structure:\n   - src/api/ (for HTTP handlers and routes)\n   - src/domain/ (for business logic and models)\n   - src/infra/ (for infrastructure code - database, redis, etc.)\n   - frontend/ (placeholder for future frontend code)\n   - infra/ (for deployment configurations)\n2. Create .env file with placeholders:\n   - DATABASE_URL=postgresql://user:password@localhost:5432/dbname\n   - REDIS_URL=redis://localhost:6379\n   - SERVER_HOST=127.0.0.1\n   - SERVER_PORT=8080\n3. Create src/main.rs with basic Axum application:\n   - Load environment variables using dotenv\n   - Create Axum router with health check endpoint (GET /health)\n   - Configure server to bind to HOST:PORT from env\n   - Implement graceful shutdown handling\n4. Create module files: src/api/mod.rs, src/domain/mod.rs, src/infra/mod.rs\n5. Add .env to .gitignore and create .env.example as template",
          "id": 2,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run 'cargo run' and verify server starts successfully. Test health check endpoint with 'curl http://localhost:8080/health' and confirm 200 OK response. Verify environment variables are loaded correctly by printing configuration on startup.",
          "title": "Create directory structure and implement basic Axum server scaffold",
          "updatedAt": "2025-12-06T16:56:25.471561089Z"
        },
        {
          "createdAt": "2025-12-06T16:56:25.471564339Z",
          "dependencies": [
            "2"
          ],
          "description": "Create connection pool managers for PostgreSQL using sqlx and Redis using redis-rs. Implement initialization functions that read from environment configuration, establish connections, and provide reusable pool instances. Setup sqlx migrations directory structure.",
          "details": "1. Create src/infra/database.rs:\n   - Implement async function to create sqlx PgPool from DATABASE_URL\n   - Configure pool settings (max_connections: 5, min_connections: 1, connection_timeout)\n   - Add connection validation and error handling\n   - Run pending migrations on startup using sqlx::migrate!()\n2. Create src/infra/redis.rs:\n   - Implement async function to create Redis ConnectionManager from REDIS_URL\n   - Configure connection pool with multiplexed connections\n   - Add connection health check function\n   - Implement basic pub/sub client initialization\n3. Setup migrations:\n   - Create 'migrations/' directory at project root\n   - Run 'sqlx migrate add initial_setup' to create first migration file\n   - Add .sqlx/ to .gitignore for offline mode support\n4. Update src/main.rs:\n   - Initialize both connection pools on startup\n   - Pass pools to Axum application state using State extractor\n   - Add cleanup/connection closing on shutdown\n5. Create src/infra/mod.rs to export database and redis modules",
          "id": 3,
          "parentId": "16",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Create integration test that initializes both connection pools. For PostgreSQL: execute a simple query like 'SELECT 1' to verify connectivity. For Redis: perform PING command and verify PONG response. Test connection pool reuse by making multiple requests. Verify migrations run successfully with 'sqlx migrate run'. Test error handling by providing invalid connection strings.",
          "title": "Implement PostgreSQL and Redis connection pools with configuration",
          "updatedAt": "2025-12-06T16:56:25.471564339Z"
        }
      ],
      "testStrategy": "Verify cargo build succeeds, server starts on configured port, PostgreSQL connection pool initializes, Redis ping succeeds",
      "title": "Initialize Rust/Axum project with PostgreSQL and Redis integration"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "16"
      ],
      "description": "Create PostgreSQL schema for teams, users, tasks, invites, and audit tables with proper indexes and constraints",
      "details": "1. Create migration files using sqlx migrate add\n2. Schema tables:\n   - users: id (uuid), email, password_hash, oauth_provider, created_at, deleted_at\n   - teams: id (uuid), name, description, owner_id (fk users), created_at, deleted_at\n   - team_members: team_id, user_id, role (enum: owner/admin/member/viewer), joined_at\n   - tasks: id (uuid), team_id (fk), title, description, assignee_id (fk users), status (enum), due_date, created_at, updated_at, deleted_at\n   - invites: id (uuid), team_id (fk), token, expires_at, created_by (fk users)\n3. Add indexes: tasks(team_id, status), tasks(assignee_id), team_members(user_id), invites(token)\n4. Add CHECK constraints for status enum values\n5. Setup foreign key cascades appropriately",
      "id": "17",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T16:56:56.834416881Z",
          "dependencies": [],
          "description": "Design and implement the foundational database tables for users, teams, and team membership with proper relationships, constraints, and soft delete support using PostgreSQL and sqlx migrations.",
          "details": "Create migration file using 'sqlx migrate add create_core_tables'. Implement users table with uuid primary key, email (unique, not null), password_hash (nullable for OAuth), oauth_provider (nullable), created_at (timestamptz), deleted_at (timestamptz nullable for soft deletes). Create teams table with uuid primary key, name (not null), description (text nullable), owner_id (uuid foreign key to users), created_at, deleted_at. Create team_members junction table with composite primary key (team_id, user_id), role enum type (owner, admin, member, viewer), joined_at timestamp. Add foreign key constraints with appropriate CASCADE/SET NULL behavior. Include NOT NULL constraints where appropriate and ensure owner_id references are maintained properly.",
          "id": 1,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify migration runs successfully with 'sqlx migrate run'. Test constraints by attempting invalid inserts (duplicate emails, null required fields, invalid foreign keys). Verify soft delete functionality by setting deleted_at and confirming records are retained. Check that team owner must be a valid user_id.",
          "title": "Create core entity tables migration (users, teams, team_members)",
          "updatedAt": "2025-12-06T16:56:56.834416881Z"
        },
        {
          "createdAt": "2025-12-06T16:56:56.834420381Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the tasks table with comprehensive fields, custom enum types for status tracking, proper foreign key relationships, and CHECK constraints to enforce business rules and data integrity.",
          "details": "Create migration file using 'sqlx migrate add create_tasks_table'. Define task_status enum type with values: 'todo', 'in_progress', 'blocked', 'review', 'done', 'archived'. Implement tasks table with uuid primary key, team_id (fk to teams, ON DELETE CASCADE), title (varchar 255 not null), description (text nullable), assignee_id (fk to users, ON DELETE SET NULL), status (task_status enum, default 'todo', not null), due_date (timestamptz nullable), created_at (timestamptz not null), updated_at (timestamptz not null), deleted_at (timestamptz nullable). Add CHECK constraint to ensure due_date is in the future if set. Add CHECK constraint for status transitions if needed. Ensure foreign keys properly cascade deletes for team_id but preserve task history when users are deleted.",
          "id": 2,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test migration execution and rollback. Verify enum type creation and valid status values. Test foreign key cascades by deleting teams and verifying tasks are removed. Test SET NULL behavior by deleting users and confirming assignee_id becomes null. Validate CHECK constraints by attempting to insert invalid status values or past due dates.",
          "title": "Implement tasks table with status enums and constraints",
          "updatedAt": "2025-12-06T16:56:56.834420381Z"
        },
        {
          "createdAt": "2025-12-06T16:56:56.834431923Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement the team invitations system table with token-based authentication, expiration handling, and any additional audit or supporting tables needed for tracking changes and system events.",
          "details": "Create migration file using 'sqlx migrate add create_invites_and_audit'. Implement invites table with uuid primary key, team_id (fk to teams, ON DELETE CASCADE), token (varchar 64 unique not null), email (varchar 255 not null), role (same enum as team_members), expires_at (timestamptz not null), created_by (fk to users, ON DELETE SET NULL), created_at (timestamptz not null), used_at (timestamptz nullable), used_by (fk to users nullable). Add CHECK constraint to ensure expires_at is after created_at. Optionally create audit_log table with id, entity_type, entity_id, action, user_id, changes (jsonb), timestamp for comprehensive audit trail. Include indexes on frequently queried fields.",
          "id": 3,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify migration applies successfully. Test unique constraint on token field. Validate CHECK constraint for expires_at timing. Test foreign key cascades when teams or users are deleted. Verify token lookup performance. Test invite workflow: creation, token uniqueness, expiration logic, and usage tracking.",
          "title": "Create invites table and audit/supporting tables",
          "updatedAt": "2025-12-06T16:56:56.834431923Z"
        },
        {
          "createdAt": "2025-12-06T16:56:56.834432631Z",
          "dependencies": [
            "1",
            "2",
            "3"
          ],
          "description": "Create strategic database indexes for query optimization on frequently accessed columns and join patterns, then comprehensively validate all foreign key relationships, cascades, and constraints across the entire schema.",
          "details": "Create migration file using 'sqlx migrate add add_indexes_and_validation'. Add indexes: CREATE INDEX idx_tasks_team_status ON tasks(team_id, status) WHERE deleted_at IS NULL; CREATE INDEX idx_tasks_assignee ON tasks(assignee_id) WHERE deleted_at IS NULL AND assignee_id IS NOT NULL; CREATE INDEX idx_team_members_user ON team_members(user_id); CREATE INDEX idx_invites_token ON invites(token) WHERE used_at IS NULL; CREATE INDEX idx_users_email ON users(email) WHERE deleted_at IS NULL; CREATE INDEX idx_tasks_due_date ON tasks(due_date) WHERE deleted_at IS NULL AND status != 'done'. Add partial indexes for soft-deleted records if needed. Document index strategy and expected query patterns. Create validation queries to verify referential integrity, test cascade behaviors, and confirm constraint enforcement across all tables.",
          "id": 4,
          "parentId": "17",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Use EXPLAIN ANALYZE to verify indexes are being used for common queries. Test query performance with and without indexes using sample data. Validate all foreign key relationships by attempting invalid references. Test CASCADE deletes thoroughly (delete team, verify tasks removed). Test SET NULL behavior (delete user, verify task assignee nullified). Run integrity checks to ensure no orphaned records. Benchmark common query patterns to confirm performance improvements.",
          "title": "Add performance indexes and validate foreign key relationships",
          "updatedAt": "2025-12-06T16:56:56.834432631Z"
        }
      ],
      "testStrategy": "Run sqlx migrate run, verify schema with \\d commands, test constraint violations, verify indexes exist",
      "title": "Design and implement database schema with migrations"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "16",
        "17"
      ],
      "description": "Build JWT-based authentication system with access and refresh tokens, including token generation, validation, and rotation",
      "details": "1. Add dependencies: jsonwebtoken = \"9\", argon2 = \"0.5\"\n2. Create domain/auth.rs with:\n   - struct Claims { sub: Uuid, exp: i64, role: String }\n   - fn generate_access_token(user_id: Uuid, role: String) -> Result<String> (15 min expiry)\n   - fn generate_refresh_token(user_id: Uuid) -> Result<String> (7 day expiry)\n   - fn validate_token(token: &str) -> Result<Claims>\n3. Store refresh tokens in Redis with SET refresh:{user_id} {token} EX 604800\n4. Create api/auth.rs handlers:\n   - POST /api/auth/register (email, password)\n   - POST /api/auth/login (email, password) -> returns {access_token, refresh_token}\n   - POST /api/auth/refresh (refresh_token) -> returns new access_token\n5. Implement Axum middleware for JWT validation in api/middleware/auth.rs\n6. Hash passwords with argon2 before storage",
      "id": "18",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T16:57:31.446504759Z",
          "dependencies": [],
          "description": "Create the core JWT token infrastructure including Claims struct, token generation functions for access and refresh tokens, and token validation logic with proper error handling.",
          "details": "Add dependencies jsonwebtoken = \"9\" and argon2 = \"0.5\" to Cargo.toml. Create domain/auth.rs with: (1) Claims struct containing sub: Uuid, exp: i64, and role: String fields with Serialize/Deserialize derives, (2) generate_access_token function that creates JWT with 15-minute expiry using HS256 algorithm, (3) generate_refresh_token function with 7-day expiry, (4) validate_token function that decodes and validates JWT signature and expiration. Use environment variable JWT_SECRET for signing key. Include proper error types for token validation failures (expired, invalid signature, malformed).",
          "id": 1,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for token generation with fixed timestamps to verify expiry calculation. Test validate_token with valid tokens, expired tokens, tampered tokens, and invalid signatures. Verify Claims deserialization from valid JWT payload.",
          "title": "Implement JWT token generation and validation with Claims structure",
          "updatedAt": "2025-12-06T16:57:31.446504759Z"
        },
        {
          "createdAt": "2025-12-06T16:57:31.446507925Z",
          "dependencies": [
            "1"
          ],
          "description": "Build secure password hashing using Argon2 and create the user registration endpoint that stores hashed passwords in the database.",
          "details": "Create helper functions in domain/auth.rs: (1) hash_password(password: &str) -> Result<String> using argon2::hash_encoded with default config (Argon2id variant, memory cost 65536 KB, time cost 3, parallelism 4), (2) verify_password(password: &str, hash: &str) -> Result<bool> using argon2::verify_encoded. Create POST /api/auth/register handler in api/auth.rs that accepts JSON payload {email: String, password: String}, validates email format and password strength (min 8 chars), hashes password with Argon2, stores user in database with hashed password, and returns 201 Created with user_id. Handle duplicate email errors with 409 Conflict.",
          "id": 2,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for hash_password and verify_password functions. Integration tests for /api/auth/register endpoint: test successful registration, duplicate email rejection, invalid email format, weak password rejection. Verify passwords are never stored in plaintext by checking database directly.",
          "title": "Implement password hashing with Argon2 and user registration endpoint",
          "updatedAt": "2025-12-06T16:57:31.446507925Z"
        },
        {
          "createdAt": "2025-12-06T16:57:31.446509634Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Implement the login endpoint that authenticates users with email/password and issues both access and refresh tokens upon successful authentication.",
          "details": "Create POST /api/auth/login handler in api/auth.rs that: (1) accepts JSON {email: String, password: String}, (2) queries database for user by email, (3) verifies password hash using verify_password function, (4) generates access_token using generate_access_token with user_id and role, (5) generates refresh_token using generate_refresh_token, (6) returns JSON response {access_token: String, refresh_token: String, token_type: \"Bearer\", expires_in: 900}. Return 401 Unauthorized for invalid credentials with generic error message to prevent user enumeration. Add rate limiting consideration (5 attempts per 15 minutes per IP).",
          "id": 3,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for /api/auth/login: test successful login with valid credentials returns both tokens, invalid password returns 401, non-existent email returns 401, verify returned tokens are valid JWTs with correct claims. Test token expiry times are set correctly. Verify generic error messages don't leak user existence information.",
          "title": "Create login endpoint with JWT token issuance",
          "updatedAt": "2025-12-06T16:57:31.446509634Z"
        },
        {
          "createdAt": "2025-12-06T16:57:31.446510217Z",
          "dependencies": [
            "1",
            "3"
          ],
          "description": "Build Redis-based refresh token storage with automatic expiration and create the token refresh endpoint that validates refresh tokens and issues new access tokens.",
          "details": "Modify login handler to store refresh token in Redis using SET refresh:{user_id} {refresh_token} EX 604800 (7 days TTL). Create POST /api/auth/refresh handler that: (1) accepts JSON {refresh_token: String}, (2) validates refresh token JWT signature and expiration, (3) extracts user_id from token claims, (4) verifies token exists in Redis at key refresh:{user_id} and matches provided token, (5) generates new access_token, (6) optionally rotates refresh token (generate new one, delete old from Redis, store new), (7) returns {access_token: String, refresh_token: String}. Return 401 for invalid/expired/revoked tokens. Implement token revocation by deleting from Redis. Handle race conditions with Redis transactions.",
          "id": 4,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: test successful token refresh with valid refresh token, expired refresh token returns 401, revoked token (deleted from Redis) returns 401, tampered token returns 401. Test token rotation updates Redis correctly. Test concurrent refresh attempts with same token. Verify old tokens are invalidated after refresh. Test Redis connection failures return appropriate errors.",
          "title": "Implement refresh token storage in Redis and token rotation endpoint",
          "updatedAt": "2025-12-06T16:57:31.446510217Z"
        },
        {
          "createdAt": "2025-12-06T16:57:31.446511634Z",
          "dependencies": [
            "1"
          ],
          "description": "Create reusable Axum middleware that validates JWT access tokens from Authorization header and injects authenticated user context into protected route handlers.",
          "details": "Create api/middleware/auth.rs with: (1) AuthMiddleware struct that extracts Bearer token from Authorization header, (2) validates token using validate_token function, (3) creates AuthUser struct {user_id: Uuid, role: String} from validated claims, (4) injects AuthUser into request extensions for handler access. Implement as Axum middleware using axum::middleware::from_fn. Handle missing Authorization header (401), invalid Bearer format (401), expired tokens (401), invalid signatures (401). Create extractor FromRequestParts implementation for AuthUser to simplify handler signatures. Add optional RequireRole middleware for role-based access control. Document usage pattern for protecting routes.",
          "id": 5,
          "parentId": "18",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: test protected endpoint with valid token succeeds and provides user context, missing Authorization header returns 401, malformed header returns 401, expired token returns 401, invalid signature returns 401. Test role-based middleware correctly allows/denies access. Unit test AuthUser extractor. Test middleware doesn't interfere with public endpoints.",
          "title": "Implement Axum middleware for JWT validation and request authentication",
          "updatedAt": "2025-12-06T16:57:31.446511634Z"
        }
      ],
      "testStrategy": "Unit tests for token generation/validation, integration tests for register/login flow, verify expired tokens rejected, test refresh token rotation",
      "title": "Implement JWT authentication with refresh token mechanism"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "18"
      ],
      "description": "Add OAuth2 authentication flows for Google and GitHub providers with user account linking",
      "details": "1. Add dependency: oauth2 = \"4.4\"\n2. Create domain/oauth.rs with:\n   - enum OAuthProvider { Google, GitHub }\n   - struct OAuthConfig { client_id, client_secret, redirect_uri }\n   - fn get_authorization_url(provider: OAuthProvider) -> String\n   - async fn exchange_code(provider: OAuthProvider, code: String) -> Result<UserInfo>\n3. Create api/oauth.rs handlers:\n   - GET /api/auth/oauth/{provider} -> redirect to provider\n   - GET /api/auth/oauth/{provider}/callback?code=xxx -> exchange code, create/link user, return JWT\n4. Store OAuth tokens in users table with oauth_provider, oauth_id fields\n5. Handle account linking: if email exists, link OAuth to existing account\n6. Configure Google OAuth 2.0 client (https://console.cloud.google.com)\n7. Configure GitHub OAuth app (https://github.com/settings/developers)",
      "id": "19",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T16:58:21.259085129Z",
          "dependencies": [],
          "description": "Create the foundational OAuth2 infrastructure including configuration structures, provider enums, and authorization URL generation logic with CSRF state management.",
          "details": "1. Add oauth2 = \"4.4\" dependency to Cargo.toml\n2. Create domain/oauth.rs with:\n   - enum OAuthProvider { Google, GitHub } with serialization support\n   - struct OAuthConfig { client_id, client_secret, redirect_uri, scopes }\n   - struct OAuthState { state_token, timestamp, provider } for CSRF protection\n   - fn get_authorization_url(provider: OAuthProvider) -> Result<String> that generates provider-specific URLs\n   - fn generate_state_token() -> String for CSRF protection\n3. Implement provider-specific authorization endpoints:\n   - Google: https://accounts.google.com/o/oauth2/v2/auth\n   - GitHub: https://github.com/login/oauth/authorize\n4. Store state tokens temporarily (in-memory cache or Redis) with 10-minute expiration\n5. Add environment variables: GOOGLE_CLIENT_ID, GOOGLE_CLIENT_SECRET, GITHUB_CLIENT_ID, GITHUB_CLIENT_SECRET, OAUTH_REDIRECT_BASE_URL",
          "id": 1,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for authorization URL generation with different providers, state token generation uniqueness, and configuration validation. Mock tests for environment variable loading.",
          "title": "Set up OAuth2 configuration and authorization URL generation",
          "updatedAt": "2025-12-06T16:58:21.259085129Z"
        },
        {
          "createdAt": "2025-12-06T16:58:21.259088962Z",
          "dependencies": [
            "1"
          ],
          "description": "Build complete Google OAuth2 flow including callback handling, authorization code exchange for access tokens, and user profile retrieval from Google APIs.",
          "details": "1. Create api/oauth.rs with Google-specific handlers:\n   - GET /api/auth/oauth/google -> validate config, generate state, redirect to Google authorization URL\n   - GET /api/auth/oauth/google/callback?code=xxx&state=yyy -> handle callback\n2. Implement token exchange:\n   - async fn exchange_google_code(code: String) -> Result<GoogleTokenResponse>\n   - POST to https://oauth2.googleapis.com/token with code, client_id, client_secret, redirect_uri\n   - Parse access_token, refresh_token, expires_in from response\n3. Implement user info retrieval:\n   - async fn get_google_user_info(access_token: String) -> Result<GoogleUserInfo>\n   - GET https://www.googleapis.com/oauth2/v2/userinfo with Bearer token\n   - Extract email, name, picture, verified_email, google_id (sub field)\n4. Add struct GoogleUserInfo { sub: String, email: String, name: String, picture: Option<String>, verified_email: bool }\n5. Validate state token matches stored value before processing\n6. Handle errors: invalid_grant, invalid_client, access_denied",
          "id": 2,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with Google OAuth playground/sandbox. Mock HTTP responses for token exchange and user info endpoints. Test error scenarios: invalid code, expired code, network timeout, malformed responses.",
          "title": "Implement Google OAuth flow with token exchange",
          "updatedAt": "2025-12-06T16:58:21.259088962Z"
        },
        {
          "createdAt": "2025-12-06T16:58:21.259090462Z",
          "dependencies": [
            "1"
          ],
          "description": "Build complete GitHub OAuth2 flow including callback handling, authorization code exchange for access tokens, and user profile retrieval from GitHub APIs.",
          "details": "1. Add GitHub-specific handlers to api/oauth.rs:\n   - GET /api/auth/oauth/github -> validate config, generate state, redirect to GitHub authorization URL with scope=user:email\n   - GET /api/auth/oauth/github/callback?code=xxx&state=yyy -> handle callback\n2. Implement token exchange:\n   - async fn exchange_github_code(code: String) -> Result<GitHubTokenResponse>\n   - POST to https://github.com/login/oauth/access_token with code, client_id, client_secret\n   - Set Accept: application/json header\n   - Parse access_token, token_type, scope from response\n3. Implement user info retrieval:\n   - async fn get_github_user_info(access_token: String) -> Result<GitHubUserInfo>\n   - GET https://api.github.com/user with Authorization: token header\n   - Extract login, id, email, name, avatar_url\n   - If email is null, GET https://api.github.com/user/emails to fetch primary verified email\n4. Add struct GitHubUserInfo { id: u64, login: String, email: String, name: Option<String>, avatar_url: Option<String> }\n5. Validate state token before processing\n6. Handle errors: bad_verification_code, redirect_uri_mismatch, rate limiting (403)",
          "id": 3,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with GitHub OAuth app in development mode. Mock HTTP client for token exchange and API calls. Test edge cases: missing email, private email, multiple emails, API rate limiting.",
          "title": "Implement GitHub OAuth flow with token exchange",
          "updatedAt": "2025-12-06T16:58:21.259090462Z"
        },
        {
          "createdAt": "2025-12-06T16:58:21.259090670Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Extend user database schema to support OAuth providers and implement secure account linking logic that connects OAuth accounts to existing users by email.",
          "details": "1. Create migration to add OAuth fields to users table:\n   - oauth_provider: VARCHAR(20) NULL (values: 'google', 'github', null for password auth)\n   - oauth_id: VARCHAR(255) NULL (provider's unique user ID)\n   - oauth_access_token: TEXT NULL (encrypted)\n   - oauth_refresh_token: TEXT NULL (encrypted, for Google)\n   - oauth_token_expires_at: TIMESTAMP NULL\n   - Add unique constraint on (oauth_provider, oauth_id)\n   - Add index on email for linking lookups\n2. Implement account linking logic:\n   - async fn link_or_create_oauth_user(provider: OAuthProvider, user_info: UserInfo) -> Result<User>\n   - Check if oauth_provider + oauth_id exists -> return existing user\n   - Check if email exists with null oauth_provider -> link OAuth to existing account (UPDATE)\n   - Check if email exists with different oauth_provider -> return error (account already linked to different provider)\n   - If no existing user -> create new user with OAuth details\n3. Handle race conditions with database transactions\n4. Encrypt tokens before storage using application secret key\n5. Add audit logging for account linking events",
          "id": 4,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for linking logic with different scenarios: new user, existing password user, existing OAuth user, duplicate email with different provider. Database transaction tests for race conditions. Integration tests with actual database.",
          "title": "Implement database schema and account linking logic",
          "updatedAt": "2025-12-06T16:58:21.259090670Z"
        },
        {
          "createdAt": "2025-12-06T16:58:21.259090962Z",
          "dependencies": [
            "4"
          ],
          "description": "Create or update user records with OAuth provider information and generate JWT tokens for authenticated sessions with proper claims and expiration.",
          "details": "1. Implement user creation/update in domain/oauth.rs:\n   - async fn create_oauth_user(provider: OAuthProvider, user_info: UserInfo, tokens: OAuthTokens) -> Result<User>\n   - Set username from email prefix or provider username\n   - Set email, name, profile picture URL\n   - Store encrypted OAuth tokens\n   - Set email_verified=true for Google verified emails\n   - Generate random password hash (user cannot login with password)\n2. Implement user update for linking:\n   - async fn link_oauth_to_user(user_id: i64, provider: OAuthProvider, oauth_id: String, tokens: OAuthTokens) -> Result<User>\n   - Update oauth_provider, oauth_id, tokens\n   - Preserve existing user data (username, created_at)\n3. Generate JWT after successful OAuth:\n   - Include user_id, email, oauth_provider in claims\n   - Set expiration to 7 days\n   - Return JWT in response body or Set-Cookie header\n4. Update callback handlers to call link_or_create_oauth_user and generate JWT\n5. Return JSON response: { token: String, user: UserProfile }\n6. Handle edge cases: email format validation, duplicate username resolution",
          "id": 5,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for user creation with different provider data. Test JWT generation and claim validation. Test duplicate username handling. Integration tests for complete OAuth flow end-to-end with database.",
          "title": "Implement user creation/update with OAuth provider data and JWT generation",
          "updatedAt": "2025-12-06T16:58:21.259090962Z"
        },
        {
          "createdAt": "2025-12-06T16:58:21.259091629Z",
          "dependencies": [
            "2",
            "3",
            "5"
          ],
          "description": "Implement robust error handling for all OAuth failure scenarios, add retry logic for transient failures, and create documentation for configuring Google and GitHub OAuth applications.",
          "details": "1. Implement comprehensive error handling:\n   - Invalid/expired authorization codes\n   - Network timeouts and connection failures (add retry with exponential backoff)\n   - Provider API errors (rate limiting, service unavailable)\n   - Invalid state token (CSRF attack detection)\n   - User denied permissions\n   - Email verification failures\n   - Database constraint violations\n2. Create custom error types:\n   - enum OAuthError { InvalidState, CodeExchangeFailed, UserInfoFailed, AccountLinkingFailed, ProviderError }\n   - Map to appropriate HTTP status codes (400, 401, 403, 500)\n3. Add user-friendly error messages and redirect to frontend with error query params\n4. Implement logging for all OAuth events (successful auth, failures, account linking)\n5. Create documentation in docs/oauth_setup.md:\n   - Google OAuth setup: Create project in Google Cloud Console, enable Google+ API, create OAuth 2.0 credentials, configure authorized redirect URIs\n   - GitHub OAuth setup: Register new OAuth app in GitHub Developer Settings, set callback URL, copy client ID and secret\n   - Environment variable configuration\n   - Testing with localhost redirect URIs\n6. Add health check endpoint to verify OAuth configuration validity",
          "id": 6,
          "parentId": "19",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test all error scenarios with mocked provider responses. Test retry logic with transient failures. Test CSRF protection with invalid state tokens. Manual testing with actual Google and GitHub OAuth apps. Load testing for rate limiting scenarios.",
          "title": "Add comprehensive error handling and provider configuration documentation",
          "updatedAt": "2025-12-06T16:58:21.259091629Z"
        }
      ],
      "testStrategy": "Manual OAuth flow testing with real providers, verify account creation, test account linking, verify JWT returned after successful OAuth",
      "title": "Implement OAuth2 integration for Google and GitHub"
    },
    {
      "agentHint": "cipher",
      "dependencies": [
        "18"
      ],
      "description": "Create token bucket rate limiter using Redis to enforce 100 req/min for authenticated users and 20 req/min for anonymous",
      "details": "1. Create infra/rate_limiter.rs with:\n   - struct RateLimiter { redis: ConnectionManager }\n   - async fn check_rate_limit(key: &str, max_requests: u32, window_secs: u64) -> Result<bool>\n2. Use Redis commands: INCR rate:{key}:{window}, EXPIRE if first request\n3. Create api/middleware/rate_limit.rs:\n   - Extract user_id from JWT or use IP address for anonymous\n   - Apply limits: authenticated users 100/60s, anonymous 20/60s\n   - Return 429 Too Many Requests if exceeded with Retry-After header\n4. Integrate middleware into Axum router before auth middleware\n5. Add rate limit headers to all responses: X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset",
      "id": "20",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T16:58:54.494547797Z",
          "dependencies": [],
          "description": "Create the foundational rate limiting logic in infra/rate_limiter.rs using Redis INCR and EXPIRE commands to implement a token bucket algorithm that accurately tracks request counts within time windows.",
          "details": "Create infra/rate_limiter.rs with struct RateLimiter containing a Redis ConnectionManager. Implement async fn check_rate_limit(key: &str, max_requests: u32, window_secs: u64) -> Result<(bool, RateLimitInfo)> where RateLimitInfo contains remaining requests and reset timestamp. Use Redis INCR to atomically increment rate:{key}:{window} counter, set EXPIRE on first request to window_secs. Return true if count <= max_requests, false otherwise. Include proper error handling for Redis connection failures with fallback behavior (fail open vs fail closed). Implement helper method to calculate current window timestamp. Add comprehensive unit tests with mock Redis to verify token bucket behavior, window rollovers, and concurrent request handling.",
          "id": 1,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mock Redis client to verify: correct counting within windows, proper EXPIRE setting, window boundary transitions, concurrent request scenarios, error handling when Redis is unavailable, and token bucket refill behavior across multiple windows",
          "title": "Implement core Redis-based rate limiter with token bucket algorithm",
          "updatedAt": "2025-12-06T16:58:54.494547797Z"
        },
        {
          "createdAt": "2025-12-06T16:58:54.494550714Z",
          "dependencies": [
            "1"
          ],
          "description": "Develop api/middleware/rate_limit.rs that extracts user identity from JWT tokens or falls back to IP addresses for anonymous users, then applies the appropriate rate limits using the core rate limiter.",
          "details": "Create api/middleware/rate_limit.rs with async middleware function that extracts user_id from JWT claims if present, otherwise uses client IP address from request headers (X-Forwarded-For, X-Real-IP, or socket address). Generate rate limit keys as 'rate:user:{user_id}' or 'rate:ip:{ip_address}'. Apply differentiated limits: authenticated users get 100 requests per 60 seconds, anonymous users get 20 requests per 60 seconds. Call RateLimiter::check_rate_limit with appropriate key and limits. Handle edge cases like missing IP addresses, malformed JWTs, and IPv6 addresses. Include logging for rate limit violations with user/IP identification. Add integration tests that verify correct limit application for both user types.",
          "id": 2,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests verifying: correct user_id extraction from valid JWTs, IP address fallback for anonymous requests, proper rate limit key generation, differentiated limits applied correctly, handling of edge cases (missing headers, invalid tokens), and middleware behavior under various authentication states",
          "title": "Create rate limiting middleware with user/IP identification",
          "updatedAt": "2025-12-06T16:58:54.494550714Z"
        },
        {
          "createdAt": "2025-12-06T16:58:54.494552297Z",
          "dependencies": [
            "2"
          ],
          "description": "Add standard rate limit response headers (X-RateLimit-Limit, X-RateLimit-Remaining, X-RateLimit-Reset) to all API responses and generate proper 429 Too Many Requests responses with Retry-After headers when limits are exceeded.",
          "details": "Extend the rate limit middleware to inject headers into all responses: X-RateLimit-Limit (max requests allowed), X-RateLimit-Remaining (requests left in current window), X-RateLimit-Reset (Unix timestamp when window resets). When rate limit is exceeded, return 429 status code with Retry-After header indicating seconds until window reset. Include JSON response body with error message, limit details, and reset time. Ensure headers are added even on successful requests so clients can track their usage. Implement proper header formatting following RFC standards. Add response interceptor to ensure headers are present on all routes. Create helper functions for calculating remaining requests and reset times. Test header presence and accuracy across various scenarios including limit exceeded and within limits.",
          "id": 3,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests checking: presence of rate limit headers on all responses, correct header values at different usage levels, proper 429 response format with Retry-After header, header accuracy after multiple requests, and correct reset timestamp calculations across window boundaries",
          "title": "Implement rate limit headers and 429 response handling",
          "updatedAt": "2025-12-06T16:58:54.494552297Z"
        },
        {
          "createdAt": "2025-12-06T16:58:54.494553505Z",
          "dependencies": [
            "3"
          ],
          "description": "Wire the rate limiting middleware into the Axum application router before authentication middleware, then perform comprehensive load testing to verify performance, accuracy under concurrent load, and proper handling of both authenticated and anonymous traffic patterns.",
          "details": "Add rate limiting middleware to Axum router configuration, ensuring it runs before auth middleware so rate limits apply to all requests including unauthenticated ones. Configure middleware ordering: rate_limit -> auth -> routes. Verify middleware doesn't create bottlenecks by profiling request latency. Implement comprehensive load tests using tools like k6 or Apache Bench: test sustained load at 80% of limits, burst traffic exceeding limits, concurrent requests from multiple users, mixed authenticated/anonymous traffic, Redis failure scenarios, and recovery after rate limit windows reset. Measure P50, P95, P99 latencies under load. Verify no race conditions cause incorrect limit enforcement. Test with realistic production-like traffic patterns. Document performance characteristics and recommended Redis configuration (connection pool size, timeout settings). Add monitoring metrics for rate limit hits, rejections, and Redis operation latencies.",
          "id": 4,
          "parentId": "20",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Load testing with 1000+ concurrent requests verifying: accurate rate limiting under high concurrency, no race conditions in Redis operations, middleware latency remains under 10ms at P95, correct limit enforcement for mixed user types, graceful degradation when Redis is slow/unavailable, and system recovery after limit windows expire",
          "title": "Integrate middleware into Axum router with comprehensive load testing",
          "updatedAt": "2025-12-06T16:58:54.494553505Z"
        }
      ],
      "testStrategy": "Load test with >100 requests in 60s, verify 429 responses, test both authenticated and anonymous limits, verify Redis keys expire correctly",
      "title": "Implement Redis-based rate limiting middleware"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "18",
        "20"
      ],
      "description": "Create CRUD endpoints for team operations including creation, retrieval, updates, and invite link generation",
      "details": "1. Create domain/team.rs with:\n   - struct Team { id, name, description, owner_id, member_count, created_at }\n   - struct TeamRepository trait with CRUD methods\n2. Implement infra/repositories/team_repo.rs with sqlx queries\n3. Create api/teams.rs handlers:\n   - POST /api/teams { name, description } -> create team, add creator as owner\n   - GET /api/teams/:id -> join with team_members to get member_count\n   - PATCH /api/teams/:id { name?, description? } -> verify requester is owner/admin\n   - POST /api/teams/:id/invite -> generate uuid token, store in invites table with 7-day expiry, return invite URL\n   - POST /api/teams/join/:token -> verify token not expired, add user to team_members as member\n4. Implement role-based authorization: only owner/admin can update team\n5. Add validation: name length 3-100 chars, description max 500 chars",
      "id": "21",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T16:59:32.499919092Z",
          "dependencies": [],
          "description": "Create the Team domain model with all required fields and define the TeamRepository trait with CRUD method signatures for team operations.",
          "details": "Create domain/team.rs with: (1) Team struct containing id (UUID), name (String), description (Option<String>), owner_id (UUID), member_count (i32), created_at (DateTime), and updated_at (DateTime). (2) Define TeamRepository trait with methods: create_team, get_team_by_id, update_team, delete_team, and get_teams_by_member. (3) Add validation constants: MIN_NAME_LENGTH=3, MAX_NAME_LENGTH=100, MAX_DESCRIPTION_LENGTH=500. (4) Implement validation methods on Team struct to validate name length (3-100 chars) and description length (max 500 chars). (5) Add proper derives (Debug, Clone, Serialize, Deserialize) and ensure all fields have appropriate types for database mapping.",
          "id": 1,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for Team struct validation methods, ensuring name length constraints (3-100 chars) are enforced and description max length (500 chars) is validated. Test edge cases like empty strings, boundary values, and null descriptions.",
          "title": "Implement Team domain model and repository trait",
          "updatedAt": "2025-12-06T16:59:32.499919092Z"
        },
        {
          "createdAt": "2025-12-06T16:59:32.499922759Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the concrete implementation of TeamRepository trait using sqlx for PostgreSQL database operations including all CRUD methods and member count aggregation.",
          "details": "Create infra/repositories/team_repo.rs with: (1) Struct TeamRepositoryImpl with sqlx PgPool. (2) Implement create_team with INSERT query, setting owner_id and initializing member_count to 1. (3) Implement get_team_by_id with LEFT JOIN on team_members table to calculate actual member_count using COUNT aggregate. (4) Implement update_team with UPDATE query for name and description fields only. (5) Add get_team_with_member_role query that joins teams and team_members to return both team data and the requesting user's role. (6) Use transactions where needed for data consistency. (7) Handle sqlx errors properly and map to domain errors. (8) Add indexes on team_members(team_id) for efficient member count queries.",
          "id": 2,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with test database: verify team creation adds owner to team_members, test member_count aggregation accuracy with multiple members, validate update operations, and test concurrent team operations for race conditions.",
          "title": "Implement TeamRepository with sqlx database operations",
          "updatedAt": "2025-12-06T16:59:32.499922759Z"
        },
        {
          "createdAt": "2025-12-06T16:59:32.499924301Z",
          "dependencies": [
            "2"
          ],
          "description": "Create API handlers for team creation (POST /api/teams) and retrieval (GET /api/teams/:id) with proper validation, authentication, and member count aggregation.",
          "details": "Create api/teams.rs with: (1) POST /api/teams handler accepting CreateTeamRequest { name, description } with authentication middleware. Validate name (3-100 chars) and description (max 500 chars), create team with authenticated user as owner_id, insert owner into team_members table with role='owner', return created team with HTTP 201. (2) GET /api/teams/:id handler that retrieves team with member_count from JOIN query, return HTTP 200 with team data or HTTP 404 if not found. (3) Add proper error handling for validation failures (HTTP 400) and database errors (HTTP 500). (4) Use transaction for POST to ensure team and team_member insertion are atomic. (5) Add CreateTeamRequest and TeamResponse DTOs with proper serialization.",
          "id": 3,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests: test team creation with valid/invalid inputs, verify owner is added to team_members, test name/description validation boundaries, verify member_count accuracy in GET endpoint, test authentication requirement, and verify 404 for non-existent teams.",
          "title": "Implement team creation and retrieval endpoints",
          "updatedAt": "2025-12-06T16:59:32.499924301Z"
        },
        {
          "createdAt": "2025-12-06T16:59:32.499924592Z",
          "dependencies": [
            "2"
          ],
          "description": "Create PATCH /api/teams/:id endpoint with authorization checks ensuring only team owners and admins can update team details.",
          "details": "Add to api/teams.rs: (1) PATCH /api/teams/:id handler accepting UpdateTeamRequest { name: Option<String>, description: Option<String> }. (2) Implement authorization middleware/check that queries team_members table to verify requesting user has role='owner' OR role='admin' for the team_id. Return HTTP 403 if unauthorized. (3) Validate name (3-100 chars if provided) and description (max 500 chars if provided). (4) Only update fields that are present in request (partial updates). (5) Return updated team with HTTP 200 or appropriate error codes: HTTP 400 for validation errors, HTTP 403 for authorization failures, HTTP 404 for non-existent teams. (6) Add authorization helper function check_team_permission(user_id, team_id, required_roles) for reusability.",
          "id": 4,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: verify only owners/admins can update teams (test with member role for negative case), test partial updates (name only, description only, both), validate authorization failures return HTTP 403, test validation for updated fields, and verify non-members cannot update teams.",
          "title": "Implement team update endpoint with role-based authorization",
          "updatedAt": "2025-12-06T16:59:32.499924592Z"
        },
        {
          "createdAt": "2025-12-06T16:59:32.499925134Z",
          "dependencies": [
            "2",
            "4"
          ],
          "description": "Create POST /api/teams/:id/invite endpoint for generating invite tokens and POST /api/teams/join/:token endpoint for joining teams via invite links with token validation and expiration handling.",
          "details": "Add to api/teams.rs: (1) Create team_invites table migration with columns: id (UUID), team_id (UUID FK), token (UUID unique), created_by (UUID), expires_at (TIMESTAMP), used_at (TIMESTAMP nullable). (2) POST /api/teams/:id/invite handler: verify requester is owner/admin using check_team_permission, generate UUID token, insert into team_invites with expires_at = now() + 7 days, return { invite_url: '/api/teams/join/{token}', expires_at } with HTTP 201. (3) POST /api/teams/join/:token handler: query team_invites by token, verify token exists and expires_at > now() and used_at IS NULL, insert authenticated user into team_members with role='member' (check not already member), update invite used_at = now(), return team details with HTTP 200. (4) Handle errors: HTTP 404 for invalid token, HTTP 410 for expired/used token, HTTP 409 if user already member. (5) Use transactions for join operation to ensure atomicity.",
          "id": 5,
          "parentId": "21",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: verify invite generation requires owner/admin role, test token uniqueness, validate 7-day expiration calculation, test join with valid token adds user as member, verify expired tokens return HTTP 410, test used tokens cannot be reused, verify already-members cannot join again (HTTP 409), and test concurrent joins with same token.",
          "title": "Implement invite generation and team join endpoints",
          "updatedAt": "2025-12-06T16:59:32.499925134Z"
        }
      ],
      "testStrategy": "Integration tests for all endpoints, verify authorization checks, test expired invite rejection, verify member count accuracy, test concurrent team creation",
      "title": "Implement Team Management API endpoints"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "21"
      ],
      "description": "Create comprehensive task management endpoints with status transitions, assignment, filtering, and soft delete functionality",
      "details": "1. Create domain/task.rs with:\n   - enum TaskStatus { Todo, InProgress, Done }\n   - struct Task { id, team_id, title, description, assignee_id, status, due_date, created_at, updated_at, deleted_at }\n   - struct TaskRepository trait\n2. Implement infra/repositories/task_repo.rs with sqlx\n3. Create api/tasks.rs handlers:\n   - POST /api/teams/:team_id/tasks { title, description, assignee_id?, due_date? } -> verify user is team member\n   - GET /api/teams/:team_id/tasks?status=&assignee_id=&due_date_from=&due_date_to= -> filter with WHERE clauses, exclude deleted_at IS NOT NULL\n   - GET /api/tasks/:id -> verify user has team access\n   - PATCH /api/tasks/:id { title?, description?, assignee_id?, status?, due_date? } -> verify team membership\n   - DELETE /api/tasks/:id -> soft delete by setting deleted_at = NOW()\n4. Add validation: title 1-200 chars, assignee must be team member\n5. Publish task events to Redis pub/sub on create/update/delete",
      "id": "22",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:00:11.076400555Z",
          "dependencies": [],
          "description": "Define the Task entity with TaskStatus enum, Task struct with all required fields including soft delete support, and TaskRepository trait interface for database operations.",
          "details": "Create domain/task.rs with: (1) TaskStatus enum with variants Todo, InProgress, Done implementing serialization; (2) Task struct with fields: id (i64), team_id (i64), title (String), description (Option<String>), assignee_id (Option<i64>), status (TaskStatus), due_date (Option<DateTime>), created_at (DateTime), updated_at (DateTime), deleted_at (Option<DateTime>); (3) TaskRepository trait defining methods: create, find_by_id, find_by_team_id_with_filters (status, assignee_id, due_date_from, due_date_to), update, soft_delete; (4) Add validation helpers for title length (1-200 chars). Include proper derives for Clone, Debug, Serialize, Deserialize.",
          "id": 1,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for TaskStatus enum serialization/deserialization, validation helper functions for title length constraints",
          "title": "Create Task domain model and repository trait",
          "updatedAt": "2025-12-06T17:00:11.076400555Z"
        },
        {
          "createdAt": "2025-12-06T17:00:11.076403680Z",
          "dependencies": [
            "1"
          ],
          "description": "Create the concrete implementation of TaskRepository trait using sqlx with PostgreSQL, including all CRUD operations and complex filtering logic while respecting soft delete semantics.",
          "details": "Implement infra/repositories/task_repo.rs with: (1) SqlxTaskRepository struct wrapping PgPool; (2) create() method with INSERT query returning created task; (3) find_by_id() with WHERE id = $1 AND deleted_at IS NULL; (4) find_by_team_id_with_filters() building dynamic WHERE clause with optional filters (status, assignee_id, due_date range) and always excluding soft-deleted records; (5) update() method with UPDATE query setting updated_at = NOW() and returning updated task; (6) soft_delete() setting deleted_at = NOW() WHERE id = $1 AND deleted_at IS NULL. Use sqlx query builder for dynamic filtering. Add database indexes on team_id, status, assignee_id, due_date, deleted_at for performance.",
          "id": 2,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with test database: verify CRUD operations, test filtering combinations, validate soft delete excludes records from queries, test concurrent updates",
          "title": "Implement TaskRepository with sqlx and soft delete support",
          "updatedAt": "2025-12-06T17:00:11.076403680Z"
        },
        {
          "createdAt": "2025-12-06T17:00:11.076405430Z",
          "dependencies": [
            "2"
          ],
          "description": "Create POST /api/teams/:team_id/tasks endpoint with comprehensive input validation, team membership verification, and assignee validation if provided.",
          "details": "Implement in api/tasks.rs: (1) CreateTaskRequest struct with title (String), description (Option<String>), assignee_id (Option<i64>), due_date (Option<DateTime>); (2) POST handler extracting authenticated user from JWT, validating team_id path parameter; (3) Verify user is member of team_id using TeamRepository; (4) Validate title length (1-200 chars); (5) If assignee_id provided, verify assignee is team member; (6) Call TaskRepository::create() with validated data, default status to Todo; (7) Return 201 Created with task JSON; (8) Error handling: 400 for validation errors, 403 if user not team member, 404 if assignee not found in team. Use axum extractors for path, JSON body, and auth.",
          "id": 3,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: successful creation, validation failures (title too long/short), authorization failures (non-team member), assignee validation (invalid assignee, assignee not in team)",
          "title": "Implement task creation endpoint with validation and authorization",
          "updatedAt": "2025-12-06T17:00:11.076405430Z"
        },
        {
          "createdAt": "2025-12-06T17:00:11.076406638Z",
          "dependencies": [
            "2"
          ],
          "description": "Create GET endpoints for listing team tasks with complex filtering (status, assignee, date ranges) and retrieving individual tasks with authorization checks.",
          "details": "Implement in api/tasks.rs: (1) GET /api/teams/:team_id/tasks with query parameters: status (Option<TaskStatus>), assignee_id (Option<i64>), due_date_from (Option<DateTime>), due_date_to (Option<DateTime>); (2) Verify authenticated user is team member; (3) Call TaskRepository::find_by_team_id_with_filters() passing all filter parameters; (4) Return 200 with array of tasks; (5) GET /api/tasks/:id handler extracting task_id from path; (6) Fetch task by ID, verify it exists and not soft-deleted; (7) Verify user has access to task's team; (8) Return 200 with task JSON; (9) Error handling: 403 if unauthorized, 404 if task not found or soft-deleted. Ensure queries exclude deleted_at IS NOT NULL records.",
          "id": 4,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: list tasks with various filter combinations, verify filtering accuracy, test pagination if implemented, individual task retrieval with auth checks, verify soft-deleted tasks return 404",
          "title": "Implement task retrieval endpoints with filtering",
          "updatedAt": "2025-12-06T17:00:11.076406638Z"
        },
        {
          "createdAt": "2025-12-06T17:00:11.076406888Z",
          "dependencies": [
            "2"
          ],
          "description": "Create PATCH /api/tasks/:id endpoint supporting partial updates to title, description, assignee, status, and due_date with proper validation and authorization.",
          "details": "Implement in api/tasks.rs: (1) UpdateTaskRequest struct with all optional fields: title (Option<String>), description (Option<Option<String>>), assignee_id (Option<Option<i64>>), status (Option<TaskStatus>), due_date (Option<Option<DateTime>>); (2) PATCH handler fetching existing task by ID; (3) Verify task exists and not soft-deleted (404 if not found); (4) Verify user is member of task's team (403 if not); (5) Validate updates: title length if provided, assignee_id is team member if provided; (6) Build updated task merging existing values with provided updates; (7) Call TaskRepository::update() with merged data; (8) Return 200 with updated task JSON; (9) Handle nested Options for nullable fields correctly. Consider adding business rules for status transitions if needed.",
          "id": 5,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: partial updates (single field, multiple fields), validation (title length, invalid assignee), authorization checks, status transition scenarios, updating nullable fields to null",
          "title": "Implement task update endpoint with status transitions and validation",
          "updatedAt": "2025-12-06T17:00:11.076406888Z"
        },
        {
          "createdAt": "2025-12-06T17:00:11.076407305Z",
          "dependencies": [
            "2",
            "3",
            "5"
          ],
          "description": "Create DELETE endpoint for soft deletion and integrate Redis pub/sub to publish task lifecycle events (created, updated, deleted) for real-time notifications.",
          "details": "Implement in api/tasks.rs: (1) DELETE /api/tasks/:id handler verifying task exists and user has team access; (2) Call TaskRepository::soft_delete() setting deleted_at = NOW(); (3) Return 204 No Content on success; (4) Create TaskEvent enum with variants Created, Updated, Deleted containing task data; (5) Implement publish_task_event() function using Redis client to PUBLISH to 'task:events' channel with JSON serialized event; (6) Integrate event publishing in POST handler (Created event), PATCH handler (Updated event), DELETE handler (Deleted event); (7) Handle Redis connection errors gracefully (log but don't fail request); (8) Include team_id in event payload for subscribers to filter. Consider adding event metadata like timestamp and user_id.",
          "id": 6,
          "parentId": "22",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API tests: soft delete success and authorization, verify deleted tasks excluded from queries; Integration tests: verify Redis events published with correct payload, test event publishing doesn't block requests, handle Redis unavailability gracefully",
          "title": "Implement soft delete and Redis pub/sub event publishing",
          "updatedAt": "2025-12-06T17:00:11.076407305Z"
        }
      ],
      "testStrategy": "Integration tests for CRUD operations, verify filtering combinations, test soft delete and 30-day retention, verify authorization checks, test assignee validation",
      "title": "Implement Task Board CRUD API with filtering"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "22"
      ],
      "description": "Create background job to permanently delete tasks older than 30 days from soft deletion",
      "details": "1. Add dependency: tokio-cron-scheduler = \"0.9\"\n2. Create infra/jobs/cleanup.rs with:\n   - async fn cleanup_deleted_tasks(pool: &PgPool) -> Result<u64>\n   - SQL: DELETE FROM tasks WHERE deleted_at IS NOT NULL AND deleted_at < NOW() - INTERVAL '30 days'\n3. In main.rs, spawn background task:\n   - tokio::spawn(async move { loop { cleanup_deleted_tasks(&pool).await; tokio::time::sleep(Duration::from_secs(86400)).await; } })\n4. Log cleanup results with count of deleted tasks\n5. Add metrics counter for deleted tasks",
      "id": "23",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:00:47.208436043Z",
          "dependencies": [],
          "description": "Add the tokio-cron-scheduler crate version 0.9 to the project dependencies to enable scheduled background job execution.",
          "details": "Update Cargo.toml to include 'tokio-cron-scheduler = \"0.9\"' in the [dependencies] section. Run 'cargo check' to verify the dependency resolves correctly and compiles without errors.",
          "id": 1,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify cargo build succeeds and the dependency is properly resolved",
          "title": "Add tokio-cron-scheduler dependency to Cargo.toml",
          "updatedAt": "2025-12-06T17:00:47.208436043Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208436877Z",
          "dependencies": [],
          "description": "Set up the infrastructure directory structure for background jobs to organize cleanup and future job implementations.",
          "details": "Create the directory path 'src/infra/jobs' if it doesn't exist. Create a mod.rs file in the jobs directory to export job modules. Update the parent infra/mod.rs to include 'pub mod jobs;' to make the jobs module accessible.",
          "id": 2,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify directory structure exists and module can be imported in main.rs",
          "title": "Create infra/jobs directory structure",
          "updatedAt": "2025-12-06T17:00:47.208436877Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208437002Z",
          "dependencies": [
            "2"
          ],
          "description": "Create the core async function that executes the SQL DELETE query to permanently remove soft-deleted tasks older than 30 days.",
          "details": "Create src/infra/jobs/cleanup.rs file. Implement 'pub async fn cleanup_deleted_tasks(pool: &PgPool) -> Result<u64, sqlx::Error>' that executes the SQL query: 'DELETE FROM tasks WHERE deleted_at IS NOT NULL AND deleted_at < NOW() - INTERVAL '30 days' RETURNING id'. Use sqlx::query!() macro for type-safe query execution. Return the count of deleted rows using .execute().await and extracting rows_affected().",
          "id": 3,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit test with test database containing tasks with various deleted_at timestamps",
          "title": "Implement cleanup_deleted_tasks function",
          "updatedAt": "2025-12-06T17:00:47.208437002Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208437710Z",
          "dependencies": [
            "3"
          ],
          "description": "Implement comprehensive logging for the cleanup job to track execution, success, failures, and the number of tasks deleted.",
          "details": "In cleanup.rs, add tracing/log statements: log job start with timestamp, log successful deletion count at info level ('Cleanup job completed: {} tasks permanently deleted'), log errors at error level with context, log when no tasks are deleted at debug level. Use structured logging with fields for count and duration.",
          "id": 4,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify log output appears correctly during test execution with different scenarios",
          "title": "Add logging infrastructure for cleanup job",
          "updatedAt": "2025-12-06T17:00:47.208437710Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208437877Z",
          "dependencies": [
            "3"
          ],
          "description": "Add a metrics counter to track the number of tasks permanently deleted by the cleanup job for monitoring and observability.",
          "details": "Add metrics dependency if not present (e.g., prometheus or metrics crate). Create a counter metric named 'cleanup_tasks_deleted_total' with appropriate labels. In cleanup_deleted_tasks function, increment the counter by the number of deleted tasks after successful execution. Ensure metrics are properly registered and exported.",
          "id": 5,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify metric increments correctly and can be scraped/observed",
          "title": "Implement metrics counter for deleted tasks",
          "updatedAt": "2025-12-06T17:00:47.208437877Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208438127Z",
          "dependencies": [
            "3",
            "4",
            "5"
          ],
          "description": "Spawn a background tokio task that runs the cleanup job every 24 hours using an infinite loop with sleep intervals.",
          "details": "In main.rs, after server initialization, clone the database pool. Use tokio::spawn to create a background task with: 'tokio::spawn(async move { loop { match cleanup_deleted_tasks(&pool).await { Ok(count) => log::info!(\"Cleanup completed: {} tasks deleted\", count), Err(e) => log::error!(\"Cleanup failed: {}\", e) }; tokio::time::sleep(Duration::from_secs(86400)).await; } })'. Add proper error handling and ensure the task doesn't block server startup.",
          "id": 6,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration test verifying task spawns and executes without blocking main thread",
          "title": "Create background task scheduler in main.rs",
          "updatedAt": "2025-12-06T17:00:47.208438127Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208438543Z",
          "dependencies": [
            "6"
          ],
          "description": "Make the cleanup job interval configurable through environment variables or configuration file instead of hardcoding 86400 seconds.",
          "details": "Add CLEANUP_INTERVAL_SECONDS to environment configuration with default value of 86400 (24 hours). Update the tokio::time::sleep duration to use the configured value. Document the configuration option in README or configuration documentation. Validate that the interval is reasonable (e.g., minimum 1 hour).",
          "id": 7,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test with different interval values to ensure configuration is respected",
          "title": "Add configuration for cleanup interval",
          "updatedAt": "2025-12-06T17:00:47.208438543Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208438918Z",
          "dependencies": [
            "6"
          ],
          "description": "Implement comprehensive integration tests that verify the cleanup job correctly deletes old soft-deleted tasks while preserving recent ones.",
          "details": "Create tests/cleanup_job_test.rs. Test scenarios: 1) Tasks deleted >30 days ago are removed, 2) Tasks deleted <30 days ago are preserved, 3) Non-deleted tasks are never touched, 4) Job returns correct count, 5) Job handles empty result set. Use test fixtures with tasks having specific deleted_at timestamps. Mock time if necessary using tokio::time::pause().",
          "id": 8,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Run integration tests against test database with time-manipulated fixtures",
          "title": "Create integration tests for cleanup job",
          "updatedAt": "2025-12-06T17:00:47.208438918Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208439293Z",
          "dependencies": [
            "6"
          ],
          "description": "Ensure the cleanup background task can be gracefully terminated when the application shuts down to prevent data corruption or incomplete operations.",
          "details": "Modify the spawned task to listen for shutdown signals using tokio::select! with a cancellation token or broadcast channel. When shutdown is signaled, allow current cleanup operation to complete before terminating. Add timeout for cleanup operation (e.g., 30 seconds) to prevent hanging during shutdown. Log shutdown events appropriately.",
          "id": 9,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test shutdown scenarios to ensure cleanup task terminates gracefully",
          "title": "Add graceful shutdown handling for cleanup job",
          "updatedAt": "2025-12-06T17:00:47.208439293Z"
        },
        {
          "createdAt": "2025-12-06T17:00:47.208439627Z",
          "dependencies": [
            "7",
            "9"
          ],
          "description": "Add comprehensive documentation explaining the cleanup job's purpose, behavior, configuration options, and operational considerations.",
          "details": "Update README.md or create OPERATIONS.md with sections covering: cleanup job purpose, 30-day retention policy rationale, configuration options (interval), monitoring via logs and metrics, manual trigger instructions if needed, database impact and performance considerations. Add inline code documentation for public functions.",
          "id": 10,
          "parentId": "23",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Documentation review for completeness and accuracy",
          "title": "Document cleanup job behavior and configuration",
          "updatedAt": "2025-12-06T17:00:47.208439627Z"
        }
      ],
      "testStrategy": "Unit test with mock database, integration test with test data, verify tasks deleted after 30 days, verify tasks within 30 days retained",
      "title": "Implement scheduled cleanup job for soft-deleted tasks"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "22"
      ],
      "description": "Create WebSocket server for live task notifications using Redis pub/sub as message broker",
      "details": "1. Add dependencies: axum-extra = { version = \"0.9\", features = [\"ws\"] }, futures = \"0.3\"\n2. Create api/websocket.rs with:\n   - GET /api/ws -> upgrade to WebSocket, authenticate via query param ?token=xxx\n   - struct WsConnection { user_id, team_ids: Vec<Uuid>, sender: mpsc::Sender }\n   - On connection: subscribe to Redis channels team:{team_id}:tasks for user's teams\n3. Create infra/pubsub.rs:\n   - async fn publish_task_event(redis: &ConnectionManager, team_id: Uuid, event: TaskEvent)\n   - async fn subscribe_to_team(redis: &ConnectionManager, team_id: Uuid) -> Receiver<TaskEvent>\n4. Message format: { type: \"task.created\"|\"task.updated\"|\"task.deleted\", task: Task }\n5. Integrate with task API: publish events on create/update/delete\n6. Handle connection limits: max 1000 concurrent connections, reject new connections if exceeded\n7. Implement ping/pong for connection health",
      "id": "24",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:01:22.255824504Z",
          "dependencies": [],
          "description": "Create the WebSocket endpoint at GET /api/ws that upgrades HTTP connections to WebSocket protocol and authenticates users via JWT token passed as query parameter",
          "details": "Add axum-extra and futures dependencies to Cargo.toml. Create api/websocket.rs module with a handler function that accepts WebSocket upgrade requests. Extract and validate JWT token from query parameter ?token=xxx. Return 401 Unauthorized if token is invalid or missing. On successful authentication, extract user_id and team_ids from token claims. Use axum::extract::ws::WebSocketUpgrade to perform the protocol upgrade. Store authenticated user context for the connection lifecycle.",
          "id": 1,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for token validation logic. Integration tests for endpoint with valid/invalid tokens. Test WebSocket upgrade handshake. Verify 401 responses for missing/invalid tokens.",
          "title": "Implement WebSocket upgrade endpoint with JWT authentication",
          "updatedAt": "2025-12-06T17:01:22.255824504Z"
        },
        {
          "createdAt": "2025-12-06T17:01:22.255837796Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement WsConnection struct and connection registry to track active WebSocket connections with their associated users and teams",
          "details": "Define WsConnection struct containing user_id (Uuid), team_ids (Vec<Uuid>), and sender (mpsc::Sender for outbound messages). Create a thread-safe ConnectionRegistry using Arc<RwLock<HashMap<ConnectionId, WsConnection>>> to track all active connections. Implement methods: add_connection, remove_connection, get_connections_for_team. Generate unique ConnectionId for each connection. Handle connection cleanup on disconnect by removing from registry and closing channels. Implement graceful shutdown that closes all connections.",
          "id": 2,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for ConnectionRegistry operations. Test concurrent add/remove operations. Verify cleanup on disconnect. Test retrieval of connections by team_id.",
          "title": "Create connection management system with user and team tracking",
          "updatedAt": "2025-12-06T17:01:22.255837796Z"
        },
        {
          "createdAt": "2025-12-06T17:01:22.255839671Z",
          "dependencies": [],
          "description": "Create infra/pubsub.rs module with Redis pub/sub functionality to subscribe to team-specific task event channels",
          "details": "Create infra/pubsub.rs with async functions using redis crate. Implement subscribe_to_team(redis: &ConnectionManager, team_id: Uuid) that subscribes to Redis channel 'team:{team_id}:tasks' and returns a Receiver<TaskEvent>. Define TaskEvent enum with variants: Created(Task), Updated(Task), Deleted(Uuid). Implement deserialization from Redis messages to TaskEvent. Handle Redis connection errors and implement reconnection logic. Use tokio::spawn for background subscription tasks. Ensure proper cleanup of Redis subscriptions when connections close.",
          "id": 3,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mock Redis. Integration tests with real Redis instance. Test subscription/unsubscription. Verify message deserialization. Test reconnection on Redis failure.",
          "title": "Implement Redis pub/sub subscriber for team channels",
          "updatedAt": "2025-12-06T17:01:22.255839671Z"
        },
        {
          "createdAt": "2025-12-06T17:01:22.255839796Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Implement the routing logic that receives messages from Redis pub/sub channels and forwards them to appropriate WebSocket client connections",
          "details": "Create a message router that spawns a task per team channel subscription. When TaskEvent arrives from Redis, query ConnectionRegistry to find all connections subscribed to that team. Format messages as JSON: {type: 'task.created'|'task.updated'|'task.deleted', task: Task}. Send formatted messages to all relevant WebSocket connections via their mpsc::Sender channels. Handle send failures gracefully by removing dead connections. Implement fan-out logic to broadcast to multiple clients efficiently. Use tokio::select! to handle multiple Redis subscriptions concurrently.",
          "id": 4,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests simulating Redis messages and verifying WebSocket delivery. Test fan-out to multiple clients. Verify filtering by team_id. Test handling of dead connections.",
          "title": "Build message routing system from Redis to WebSocket clients",
          "updatedAt": "2025-12-06T17:01:22.255839796Z"
        },
        {
          "createdAt": "2025-12-06T17:01:22.255840212Z",
          "dependencies": [
            "2"
          ],
          "description": "Add WebSocket ping/pong heartbeat mechanism to detect and close stale connections",
          "details": "Implement periodic ping sending (every 30 seconds) using tokio::time::interval. Send WebSocket Ping frames to clients and expect Pong responses. Track last_pong_time for each connection. Implement timeout detection: if no pong received within 60 seconds, consider connection dead and close it. Handle incoming Pong messages from clients. Update last_pong_time on receipt. Automatically remove timed-out connections from ConnectionRegistry. Log connection health metrics. Handle client-initiated pings appropriately.",
          "id": 5,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for timeout detection logic. Integration tests simulating unresponsive clients. Verify connections are closed after timeout. Test that responsive clients remain connected.",
          "title": "Implement ping/pong mechanism for connection health monitoring",
          "updatedAt": "2025-12-06T17:01:22.255840212Z"
        },
        {
          "createdAt": "2025-12-06T17:01:22.255840462Z",
          "dependencies": [
            "2"
          ],
          "description": "Implement connection limit enforcement to reject new connections when maximum concurrent connections (1000) is reached",
          "details": "Add atomic counter (AtomicUsize) to track current connection count. Before accepting new WebSocket connection, check if count >= 1000. If limit exceeded, return HTTP 503 Service Unavailable with JSON error message: {error: 'Connection limit reached', max_connections: 1000}. Increment counter on successful connection, decrement on disconnect. Make limit configurable via environment variable MAX_WS_CONNECTIONS (default 1000). Add metrics for connection count and rejection rate. Implement connection draining for graceful shutdown.",
          "id": 6,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Load tests simulating 1000+ concurrent connections. Verify rejections at limit. Test counter accuracy with concurrent connects/disconnects. Verify graceful error responses.",
          "title": "Enforce connection limits with graceful rejection handling",
          "updatedAt": "2025-12-06T17:01:22.255840462Z"
        },
        {
          "createdAt": "2025-12-06T17:01:22.255840712Z",
          "dependencies": [
            "3"
          ],
          "description": "Add Redis pub/sub event publishing to task create, update, and delete API endpoints to broadcast changes in real-time",
          "details": "Implement publish_task_event(redis: &ConnectionManager, team_id: Uuid, event: TaskEvent) in infra/pubsub.rs. Serialize TaskEvent to JSON and publish to Redis channel 'team:{team_id}:tasks'. Integrate into task API handlers: on POST /tasks (create), publish TaskEvent::Created; on PUT /tasks/:id (update), publish TaskEvent::Updated; on DELETE /tasks/:id, publish TaskEvent::Deleted. Handle publish failures gracefully without blocking API responses. Add configuration flag to enable/disable real-time notifications. Log publish errors but don't fail the API request.",
          "id": 7,
          "parentId": "24",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests verifying events are published on API operations. Test with Redis unavailable to ensure API still works. Verify correct event types and payloads. End-to-end tests with WebSocket clients receiving updates.",
          "title": "Integrate event publishing with task API endpoints",
          "updatedAt": "2025-12-06T17:01:22.255840712Z"
        }
      ],
      "testStrategy": "WebSocket client tests, verify events published on task operations, test connection limit enforcement, verify user only receives events for their teams, load test with 1000 connections",
      "title": "Implement WebSocket endpoint for real-time task updates"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "22"
      ],
      "description": "Build email notification service for mentions and due date reminders with user-configurable preferences",
      "details": "1. Add dependencies: lettre = { version = \"0.11\", features = [\"tokio1-native-tls\"] }, tera = \"1.19\"\n2. Create domain/notification.rs with:\n   - enum NotificationType { Mention, DueDateReminder, TaskAssigned }\n   - struct NotificationPreferences { user_id, email_mentions, email_due_dates, email_assignments }\n3. Add notification_preferences table to schema\n4. Create infra/email.rs with:\n   - struct EmailService { smtp_transport: SmtpTransport, templates: Tera }\n   - async fn send_email(to: &str, subject: &str, template: &str, context: Context)\n5. Create templates in templates/emails/: mention.html, due_date_reminder.html\n6. Create infra/jobs/notification_job.rs:\n   - Check tasks with due_date within 24 hours, send reminders\n   - Run daily via tokio cron\n7. On task assignment/mention, check user preferences and send email asynchronously\n8. Configure SMTP via env vars: SMTP_HOST, SMTP_PORT, SMTP_USERNAME, SMTP_PASSWORD",
      "id": "25",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:02:08.439269179Z",
          "dependencies": [],
          "description": "Implement database schema for notification preferences and REST API endpoints to allow users to configure their email notification settings for mentions, due dates, and task assignments.",
          "details": "1. Create domain/notification.rs with NotificationType enum (Mention, DueDateReminder, TaskAssigned) and NotificationPreferences struct containing user_id, email_mentions, email_due_dates, email_assignments fields. 2. Add notification_preferences table to schema.rs with columns: id, user_id (FK), email_mentions (bool), email_due_dates (bool), email_assignments (bool), created_at, updated_at. 3. Create repository methods in infra/repositories/notification_repository.rs for CRUD operations on preferences. 4. Implement API endpoints in handlers/notification_handler.rs: GET /api/users/{user_id}/notification-preferences (retrieve), PUT /api/users/{user_id}/notification-preferences (update). 5. Add validation to ensure only authenticated users can modify their own preferences. 6. Include default preferences creation on user registration.",
          "id": 1,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for domain models, repository tests with test database, integration tests for API endpoints verifying CRUD operations and authorization checks, test default preference creation on user signup",
          "title": "Create notification preferences schema and API endpoints",
          "updatedAt": "2025-12-06T17:02:08.439269179Z"
        },
        {
          "createdAt": "2025-12-06T17:02:08.439272220Z",
          "dependencies": [
            "1"
          ],
          "description": "Configure email service infrastructure with SMTP transport, template engine integration, and environment-based configuration for sending emails.",
          "details": "1. Add dependencies to Cargo.toml: lettre = { version = \"0.11\", features = [\"tokio1-native-tls\"] }, tera = \"1.19\". 2. Create infra/email.rs with EmailService struct containing smtp_transport: SmtpTransport and templates: Tera fields. 3. Implement EmailService::new() to initialize SMTP from env vars (SMTP_HOST, SMTP_PORT, SMTP_USERNAME, SMTP_PASSWORD) and load templates from templates/emails/ directory. 4. Implement async send_email method with parameters: to (recipient email), subject, template_name, and context (HashMap or tera::Context). 5. Add connection pooling and timeout configuration for SMTP. 6. Create email configuration struct to validate env vars on startup. 7. Add email service to application state for dependency injection.",
          "id": 2,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for EmailService initialization with mock env vars, integration tests using SMTP test server (like MailHog or smtp4dev), test template loading errors, verify connection timeout handling",
          "title": "Set up email service with SMTP configuration and template engine",
          "updatedAt": "2025-12-06T17:02:08.439272220Z"
        },
        {
          "createdAt": "2025-12-06T17:02:08.439273679Z",
          "dependencies": [
            "2"
          ],
          "description": "Create HTML email templates using Tera templating engine for mentions, due date reminders, and task assignment notifications with proper styling and dynamic content.",
          "details": "1. Create templates/emails/ directory structure. 2. Implement mention.html template with variables: mentioned_by_name, task_title, task_description, task_url, comment_text. Include responsive HTML/CSS styling. 3. Implement due_date_reminder.html template with variables: task_title, task_description, due_date, time_remaining, task_url. 4. Implement task_assigned.html template with variables: assigned_by_name, task_title, task_description, priority, due_date, task_url. 5. Create base_email.html layout template with header, footer, and consistent branding that other templates extend. 6. Add plain text alternatives for each template (mention.txt, due_date_reminder.txt, task_assigned.txt) for email clients that don't support HTML. 7. Include unsubscribe link in footer pointing to notification preferences page.",
          "id": 3,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Template rendering tests with sample context data, verify all variables are properly substituted, test with missing optional variables, validate HTML structure and CSS compatibility across email clients using tools like Litmus or Email on Acid, test plain text fallbacks",
          "title": "Design and implement email templates for all notification types",
          "updatedAt": "2025-12-06T17:02:08.439273679Z"
        },
        {
          "createdAt": "2025-12-06T17:02:08.439273929Z",
          "dependencies": [
            "1",
            "2",
            "3"
          ],
          "description": "Add event-driven email notifications that check user preferences before sending emails asynchronously when tasks are assigned, users are mentioned, or other relevant events occur.",
          "details": "1. Create infra/events/notification_events.rs with event structs: TaskAssignedEvent, UserMentionedEvent containing relevant task and user data. 2. In task assignment handler, emit TaskAssignedEvent and spawn async task to: fetch assignee's notification preferences, check email_assignments flag, render task_assigned template, send email via EmailService. 3. In comment creation handler, parse @mentions from comment text, emit UserMentionedEvent for each mentioned user, spawn async tasks to check email_mentions preferences and send mention emails. 4. Implement notification queue using tokio channels to decouple email sending from request handling. 5. Create background worker that processes notification queue with configurable concurrency. 6. Add structured logging for all email sends (success/failure) with correlation IDs. 7. Ensure async email sending doesn't block API responses.",
          "id": 4,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests simulating task assignment and mention events, verify emails are sent only when preferences allow, test async processing doesn't block requests, mock SMTP to verify email content and recipients, test queue processing under load, verify logging output",
          "title": "Implement preference checking and async email sending on task events",
          "updatedAt": "2025-12-06T17:02:08.439273929Z"
        },
        {
          "createdAt": "2025-12-06T17:02:08.439274720Z",
          "dependencies": [
            "1",
            "2",
            "3"
          ],
          "description": "Create a scheduled background job that runs daily to identify tasks with approaching due dates and sends reminder emails to assigned users based on their notification preferences.",
          "details": "1. Create infra/jobs/notification_job.rs with DueDateReminderJob struct. 2. Implement job logic: query tasks with due_date within next 24 hours that haven't been reminded today, join with users and notification_preferences. 3. Filter users with email_due_dates enabled. 4. For each task, render due_date_reminder template with task details and time remaining. 5. Send emails asynchronously with batching to avoid overwhelming SMTP server. 6. Track last_reminded_at timestamp in tasks table to prevent duplicate reminders. 7. Use tokio-cron-scheduler crate to schedule job execution daily at configured time (default 9 AM). 8. Add job configuration via env vars: REMINDER_CRON_SCHEDULE, REMINDER_HOURS_BEFORE (default 24). 9. Implement graceful shutdown to complete in-flight emails before stopping.",
          "id": 5,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for due date calculation logic, integration tests with test database containing tasks with various due dates, mock current time to test 24-hour window, verify only enabled preferences receive emails, test batch processing, verify last_reminded_at prevents duplicates, test cron scheduling with accelerated time",
          "title": "Build due date reminder background job with daily scheduling",
          "updatedAt": "2025-12-06T17:02:08.439274720Z"
        },
        {
          "createdAt": "2025-12-06T17:02:08.439275929Z",
          "dependencies": [
            "4",
            "5"
          ],
          "description": "Implement robust error handling, retry mechanisms, and monitoring for email delivery failures to ensure reliable notification delivery and graceful degradation.",
          "details": "1. Create custom error types in infra/email.rs: SmtpConnectionError, TemplateRenderError, EmailSendError with detailed context. 2. Implement exponential backoff retry logic (3 attempts with 1s, 5s, 15s delays) for transient SMTP failures. 3. Add dead letter queue for permanently failed emails stored in failed_notifications table with failure reason and retry count. 4. Implement circuit breaker pattern to temporarily disable email sending if SMTP server is consistently failing (10 failures in 5 minutes). 5. Add health check endpoint /api/health/email that verifies SMTP connectivity. 6. Implement fallback behavior: log notification details when email fails so manual follow-up is possible. 7. Add metrics for email send success/failure rates, retry counts, and circuit breaker state. 8. Create admin endpoint to view and retry failed notifications. 9. Add alerting configuration for sustained email delivery failures.",
          "id": 6,
          "parentId": "25",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for retry logic with mock SMTP failures, test exponential backoff timing, integration tests simulating SMTP server downtime, verify circuit breaker triggers and recovers correctly, test dead letter queue storage and retrieval, load tests to verify error handling under high volume, test health check endpoint accuracy",
          "title": "Add comprehensive error handling and retry logic for SMTP failures",
          "updatedAt": "2025-12-06T17:02:08.439275929Z"
        }
      ],
      "testStrategy": "Unit tests with mock SMTP, integration tests for preference checks, verify emails sent on task assignment, test due date reminder job, verify unsubscribe respects preferences",
      "title": "Implement email notification system with user preferences"
    },
    {
      "agentHint": "tap",
      "dependencies": [
        "25"
      ],
      "description": "Add Firebase Cloud Messaging support for mobile push notifications with device token management",
      "details": "1. Add dependency: fcm = \"0.9\"\n2. Add device_tokens table: id, user_id, token, platform (ios/android), created_at\n3. Create api/devices.rs:\n   - POST /api/devices/register { token, platform } -> store device token for authenticated user\n   - DELETE /api/devices/:id -> remove device token\n4. Create infra/push.rs with:\n   - struct PushService { fcm_client: fcm::Client }\n   - async fn send_push(user_id: Uuid, notification: PushNotification) -> Result<()>\n   - Query device_tokens for user, send to all devices\n5. Integrate with task events: on task assignment/mention, send push if user enabled\n6. Handle FCM errors: remove invalid tokens from database\n7. Configure FCM via FIREBASE_SERVER_KEY env var",
      "id": "26",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:02:52.371986213Z",
          "dependencies": [],
          "description": "Create database schema for device tokens and implement REST API endpoints for device registration and deletion. This includes adding the device_tokens table with columns for id, user_id, token, platform (ios/android), and created_at timestamp. Implement POST /api/devices/register endpoint to store device tokens for authenticated users and DELETE /api/devices/:id endpoint to remove tokens.",
          "details": "1. Add fcm = \"0.9\" dependency to Cargo.toml\n2. Create migration for device_tokens table with fields: id (UUID primary key), user_id (UUID foreign key to users), token (VARCHAR unique), platform (ENUM 'ios'/'android'), created_at (TIMESTAMP)\n3. Create models/device_token.rs with DeviceToken struct and database operations (insert, delete, find_by_user)\n4. Create api/devices.rs with:\n   - POST /api/devices/register handler accepting { token: String, platform: String } in request body\n   - Validate platform is either 'ios' or 'android'\n   - Extract authenticated user_id from JWT/session\n   - Store device token in database (upsert to handle re-registration)\n   - Return 201 Created with device token id\n   - DELETE /api/devices/:id handler to remove device token\n   - Verify token belongs to authenticated user before deletion\n   - Return 204 No Content on success\n5. Add route registration in main.rs\n6. Write unit tests for device token model operations\n7. Write integration tests for registration endpoint with multiple devices per user and deletion endpoint with authorization checks",
          "id": 1,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for device token CRUD operations. Integration tests for POST /api/devices/register with valid/invalid platforms, duplicate token handling, and unauthenticated requests. Integration tests for DELETE endpoint with authorization verification and non-existent token handling. Test multi-device registration for single user.",
          "title": "Implement device token schema and registration endpoints",
          "updatedAt": "2025-12-06T17:02:52.371986213Z"
        },
        {
          "createdAt": "2025-12-06T17:02:52.371989379Z",
          "dependencies": [
            "1"
          ],
          "description": "Set up Firebase Cloud Messaging client and implement push notification sending logic. Create the PushService struct with FCM client initialization, implement the send_push function to query user device tokens and send notifications to all registered devices. Configure FCM authentication using FIREBASE_SERVER_KEY environment variable.",
          "details": "1. Create infra/push.rs module\n2. Define PushNotification struct with fields: title (String), body (String), data (HashMap<String, String>)\n3. Create PushService struct:\n   - fcm_client: fcm::Client field\n   - new() constructor that reads FIREBASE_SERVER_KEY from env and initializes FCM client\n   - Panic with clear error message if FIREBASE_SERVER_KEY is not set\n4. Implement async fn send_push(user_id: Uuid, notification: PushNotification) -> Result<()>:\n   - Query device_tokens table for all tokens belonging to user_id\n   - Iterate through each device token\n   - Build FCM message with notification payload (title, body) and data payload\n   - Set platform-specific options (priority: high, content_available for iOS)\n   - Call fcm_client.send() for each device token\n   - Collect results and log any failures\n   - Return Ok(()) if at least one notification sent successfully\n5. Add PushService to application state/dependency injection\n6. Write unit tests with mocked FCM client\n7. Test send_push with user having multiple devices (2 iOS, 1 Android)\n8. Test send_push with user having no registered devices (should return Ok without error)",
          "id": 2,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mocked FCM client to verify correct message construction for iOS and Android platforms. Test sending to multiple devices and verify all receive notifications. Test user with no devices returns gracefully. Mock FCM responses to test success and failure scenarios. Integration test with test FCM credentials if available.",
          "title": "Create FCM client and push notification service",
          "updatedAt": "2025-12-06T17:02:52.371989379Z"
        },
        {
          "createdAt": "2025-12-06T17:02:52.371990963Z",
          "dependencies": [
            "2"
          ],
          "description": "Connect push notification service to task assignment and mention events. Implement notification triggers when users are assigned to tasks or mentioned in comments, respecting user notification preferences. Ensure push notifications are only sent to users who have enabled push notification settings.",
          "details": "1. Identify existing task event handlers for assignment and mentions (likely in tasks service or event handlers)\n2. Add push notification preference field to user settings if not exists (enable_push_notifications: BOOLEAN, default true)\n3. In task assignment handler:\n   - After task is assigned to user, check if user has enable_push_notifications = true\n   - If enabled, call PushService.send_push() with notification:\n     - title: \"New Task Assignment\"\n     - body: \"You've been assigned to: {task_title}\"\n     - data: { \"type\": \"task_assignment\", \"task_id\": \"{task_id}\" }\n4. In mention handler (when user is @mentioned in comment):\n   - Check if mentioned user has enable_push_notifications = true\n   - If enabled, call PushService.send_push() with notification:\n     - title: \"You were mentioned\"\n     - body: \"{commenter_name} mentioned you in {task_title}\"\n     - data: { \"type\": \"mention\", \"task_id\": \"{task_id}\", \"comment_id\": \"{comment_id}\" }\n5. Handle push sending errors gracefully (log but don't fail the main operation)\n6. Write integration tests for task assignment triggering push notification\n7. Write integration tests for mention triggering push notification\n8. Test that users with push disabled don't receive notifications\n9. Test notification data payload contains correct task/comment references",
          "id": 3,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests for task assignment flow: create task, assign to user with push enabled, verify send_push called with correct parameters. Test with push disabled user, verify no notification sent. Integration tests for mention flow: create comment with @mention, verify notification sent. Test notification preferences are respected. Verify notification payload data is correct for deep linking.",
          "title": "Integrate push notifications with task events",
          "updatedAt": "2025-12-06T17:02:52.371990963Z"
        },
        {
          "createdAt": "2025-12-06T17:02:52.371991754Z",
          "dependencies": [
            "2"
          ],
          "description": "Add robust error handling for FCM service failures and implement automatic cleanup of invalid device tokens. Handle FCM-specific errors like invalid registration tokens, unregistered devices, and service unavailability. Remove invalid tokens from the database to maintain data hygiene and avoid repeated failed send attempts.",
          "details": "1. In PushService.send_push(), enhance error handling:\n   - Wrap FCM client calls in proper error handling\n   - Parse FCM error responses to identify error types\n2. Implement token cleanup logic:\n   - For FCM error \"InvalidRegistration\" or \"NotRegistered\": delete token from device_tokens table\n   - For FCM error \"MismatchSenderId\": delete token (app was reinstalled with different FCM config)\n   - Log token deletion with user_id and token_id for audit\n3. Handle transient errors:\n   - For \"Unavailable\" or \"InternalServerError\": log warning but keep token\n   - Consider implementing retry logic with exponential backoff (optional)\n4. Implement batch cleanup function:\n   - async fn cleanup_invalid_tokens(token_ids: Vec<Uuid>) -> Result<usize>\n   - Delete multiple tokens in single database query\n   - Return count of deleted tokens\n5. Add metrics/logging:\n   - Log successful push notification count\n   - Log failed notification count by error type\n   - Log invalid token cleanup count\n6. Write unit tests for each FCM error type response\n7. Test invalid token is deleted from database after FCM returns InvalidRegistration\n8. Test valid tokens are retained after transient errors\n9. Test batch cleanup with multiple invalid tokens\n10. Write integration test simulating token expiration scenario",
          "id": 4,
          "parentId": "26",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with mocked FCM client returning different error types (InvalidRegistration, NotRegistered, Unavailable, InternalServerError). Verify invalid tokens are deleted and valid tokens retained. Integration tests: register device token, mock FCM error response, verify token deleted from database. Test batch cleanup with multiple expired tokens. Test metrics/logging for push notification attempts and failures.",
          "title": "Implement FCM error handling and invalid token cleanup",
          "updatedAt": "2025-12-06T17:02:52.371991754Z"
        }
      ],
      "testStrategy": "Unit tests with mock FCM client, integration tests for device registration, verify push sent on task events, test invalid token cleanup",
      "title": "Implement FCM push notification integration"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "22",
        "24"
      ],
      "description": "Create React 18 TypeScript frontend with Tailwind CSS featuring a drag-and-drop Kanban board for task management",
      "details": "1. Initialize React app: npx create-react-app frontend --template typescript\n2. Install dependencies: npm install @dnd-kit/core @dnd-kit/sortable tailwindcss axios react-router-dom zustand\n3. Setup Tailwind: npx tailwindcss init, configure tailwind.config.js\n4. Create components:\n   - src/components/KanbanBoard.tsx: use @dnd-kit for drag-and-drop\n   - src/components/TaskCard.tsx: display task with status badge\n   - src/components/TaskModal.tsx: create/edit task form\n5. Create stores with Zustand:\n   - src/stores/taskStore.ts: manage tasks state, WebSocket integration\n   - src/stores/authStore.ts: JWT token management\n6. Implement WebSocket connection in taskStore:\n   - Connect on mount, reconnect on disconnect\n   - Update tasks state on WebSocket messages\n7. API client in src/api/client.ts: axios instance with JWT interceptor\n8. Implement drag-and-drop: on drop, PATCH /api/tasks/:id with new status\n9. Add optimistic updates: update UI immediately, rollback on API error",
      "id": "27",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:03:47.868073419Z",
          "dependencies": [],
          "description": "Set up the React 18 TypeScript application using create-react-app template and configure Tailwind CSS for styling. Install core dependencies including routing and HTTP client libraries.",
          "details": "1. Run 'npx create-react-app frontend --template typescript' to bootstrap the project\n2. Install dependencies: 'npm install tailwindcss postcss autoprefixer axios react-router-dom'\n3. Initialize Tailwind: 'npx tailwindcss init -p'\n4. Configure tailwind.config.js with content paths: ['./src/**/*.{js,jsx,ts,tsx}']\n5. Add Tailwind directives to src/index.css: @tailwind base; @tailwind components; @tailwind utilities;\n6. Set up project structure: create folders src/components, src/stores, src/api, src/types, src/utils\n7. Configure tsconfig.json with strict mode and path aliases\n8. Test the setup by running 'npm start' and verifying Tailwind styles work",
          "id": 1,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Manual verification: Start dev server and create a test component with Tailwind classes to confirm styling is applied correctly. Verify TypeScript compilation works without errors.",
          "title": "Initialize React TypeScript project with Tailwind CSS",
          "updatedAt": "2025-12-06T17:03:47.868073419Z"
        },
        {
          "createdAt": "2025-12-06T17:03:47.868075294Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement centralized state management using Zustand for authentication tokens and task data. Include TypeScript interfaces for type safety and state persistence logic.",
          "details": "1. Install Zustand: 'npm install zustand'\n2. Create src/types/index.ts with Task, TaskStatus, User interfaces\n3. Implement src/stores/authStore.ts:\n   - State: token (string | null), user (User | null), isAuthenticated (boolean)\n   - Actions: setToken, clearToken, setUser\n   - Persist token to localStorage\n   - Include getAuthHeader() helper method\n4. Implement src/stores/taskStore.ts:\n   - State: tasks (Task[]), loading (boolean), error (string | null), wsConnected (boolean)\n   - Actions: setTasks, addTask, updateTask, deleteTask, setLoading, setError, setWsConnected\n   - Include optimistic update helpers: optimisticUpdate, rollbackUpdate\n5. Add TypeScript types for all store actions and state\n6. Implement store devtools integration for debugging",
          "id": 2,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: Test each store action in isolation. Verify state updates correctly. Test localStorage persistence for authStore. Mock localStorage and verify token is saved/retrieved correctly.",
          "title": "Create Zustand stores for authentication and task state management",
          "updatedAt": "2025-12-06T17:03:47.868075294Z"
        },
        {
          "createdAt": "2025-12-06T17:03:47.868076210Z",
          "dependencies": [
            "2"
          ],
          "description": "Create an axios-based API client with JWT authentication interceptor, request/response interceptors for error handling, and typed API methods for all backend endpoints.",
          "details": "1. Create src/api/client.ts with axios instance:\n   - Base URL from environment variable (REACT_APP_API_URL)\n   - Request interceptor to attach JWT token from authStore\n   - Response interceptor for error handling (401 -> clear auth, network errors)\n2. Implement src/api/tasks.ts with typed methods:\n   - getTasks(): Promise<Task[]>\n   - getTask(id: string): Promise<Task>\n   - createTask(data: CreateTaskDto): Promise<Task>\n   - updateTask(id: string, data: UpdateTaskDto): Promise<Task>\n   - deleteTask(id: string): Promise<void>\n   - updateTaskStatus(id: string, status: TaskStatus): Promise<Task>\n3. Implement src/api/auth.ts:\n   - login(credentials): Promise<{ token: string, user: User }>\n   - register(data): Promise<{ token: string, user: User }>\n4. Add error types and custom error handling\n5. Implement retry logic for failed requests (exponential backoff)\n6. Add request/response logging in development mode",
          "id": 3,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests with axios-mock-adapter: Mock API responses and verify correct headers are sent. Test error handling for 401, 500, network errors. Verify retry logic works correctly. Test that JWT token is attached to requests.",
          "title": "Build API client with JWT interceptor and error handling",
          "updatedAt": "2025-12-06T17:03:47.868076210Z"
        },
        {
          "createdAt": "2025-12-06T17:03:47.868076502Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Add real-time task updates via WebSocket connection in the task store. Implement automatic reconnection with exponential backoff, handle connection state, and process incoming task update messages.",
          "details": "1. Create src/utils/websocket.ts with WebSocket manager class:\n   - Constructor accepts URL and auth token\n   - Methods: connect(), disconnect(), send(message), on(event, handler)\n   - Implement reconnection logic with exponential backoff (max 5 attempts)\n   - Handle connection states: connecting, connected, disconnected, error\n   - Emit events for connection state changes\n2. Integrate WebSocket into taskStore:\n   - Initialize WebSocket connection in connectWebSocket() action\n   - Listen for 'task:created', 'task:updated', 'task:deleted' events\n   - Update tasks state when messages received\n   - Handle conflicts between WebSocket updates and optimistic updates\n   - Disconnect on logout or component unmount\n3. Add connection status indicator data to store\n4. Implement message queue for offline scenarios\n5. Add heartbeat/ping-pong to detect stale connections\n6. Handle authentication errors (close and don't reconnect)",
          "id": 4,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests with mock WebSocket server: Test connection establishment, reconnection after disconnect, message handling. Verify state updates correctly on WebSocket messages. Test conflict resolution between optimistic updates and WebSocket updates. Test heartbeat mechanism.",
          "title": "Implement WebSocket integration with reconnection logic",
          "updatedAt": "2025-12-06T17:03:47.868076502Z"
        },
        {
          "createdAt": "2025-12-06T17:03:47.868076919Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Build the main KanbanBoard component using @dnd-kit library for smooth drag-and-drop functionality. Implement column-based layout with drag sensors, drop zones, and visual feedback during dragging.",
          "details": "1. Install @dnd-kit: 'npm install @dnd-kit/core @dnd-kit/sortable @dnd-kit/utilities'\n2. Create src/components/KanbanBoard.tsx:\n   - Define columns: TODO, IN_PROGRESS, DONE\n   - Use DndContext from @dnd-kit/core as wrapper\n   - Implement SortableContext for each column\n   - Use sensors: useSensor(PointerSensor, KeyboardSensor) for accessibility\n   - Implement handleDragStart, handleDragOver, handleDragEnd\n3. In handleDragEnd:\n   - Extract task ID and new status from event\n   - Call taskStore optimisticUpdate to update UI immediately\n   - Call API client updateTaskStatus()\n   - On API success: commit the change\n   - On API error: rollback using taskStore.rollbackUpdate()\n4. Add drag overlay with DragOverlay component for smooth visual feedback\n5. Style columns with Tailwind: flex layout, min-height, background colors\n6. Add loading skeleton while tasks are fetching\n7. Implement empty state for columns with no tasks\n8. Add column headers with task counts",
          "id": 5,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests with React Testing Library: Render board with mock tasks, simulate drag events using @dnd-kit testing utilities. E2E tests with Cypress: Test full drag-and-drop flow, verify API calls are made, test optimistic updates and rollback scenarios.",
          "title": "Create Kanban board component with @dnd-kit drag-and-drop",
          "updatedAt": "2025-12-06T17:03:47.868076919Z"
        },
        {
          "createdAt": "2025-12-06T17:03:47.868077419Z",
          "dependencies": [
            "2",
            "3"
          ],
          "description": "Create reusable TaskCard component for displaying tasks in the Kanban board and TaskModal component for creating/editing tasks with comprehensive form validation and error handling.",
          "details": "1. Create src/components/TaskCard.tsx:\n   - Accept task prop and onClick handler\n   - Display task title, description (truncated), status badge, priority indicator\n   - Use useSortable hook from @dnd-kit/sortable for drag handle\n   - Add CSS transform and transition for smooth animations\n   - Style with Tailwind: card layout, hover effects, shadow\n   - Add action buttons: edit, delete (with confirmation)\n2. Create src/components/TaskModal.tsx:\n   - Accept mode ('create' | 'edit'), task (optional), onClose, onSubmit props\n   - Implement form with controlled inputs: title, description, status, priority, assignee\n   - Add form validation: title required (max 200 chars), description optional (max 2000 chars)\n   - Display validation errors inline\n   - Implement loading state during submission\n   - Handle API errors and display error messages\n   - Use React portals for modal overlay\n   - Add keyboard shortcuts: ESC to close, CMD+Enter to submit\n3. Create src/components/StatusBadge.tsx for reusable status display\n4. Add proper TypeScript types for all props\n5. Implement accessibility: proper ARIA labels, focus management",
          "id": 6,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests: Test TaskCard renders correctly with different task data. Test TaskModal form validation, submission, error handling. Test keyboard shortcuts work. Test accessibility with jest-axe. Verify modal closes on ESC and backdrop click.",
          "title": "Build TaskCard and TaskModal components with form validation",
          "updatedAt": "2025-12-06T17:03:47.868077419Z"
        },
        {
          "createdAt": "2025-12-06T17:03:47.868077669Z",
          "dependencies": [
            "4",
            "5",
            "6"
          ],
          "description": "Add sophisticated optimistic update mechanism that immediately updates the UI when users drag tasks, with automatic rollback on API failures and conflict resolution for concurrent WebSocket updates.",
          "details": "1. Enhance taskStore with optimistic update queue:\n   - Add pendingUpdates Map<taskId, { original: Task, optimistic: Task, timestamp: number }>\n   - Implement optimisticUpdate(taskId, updates) action that stores original state\n   - Implement commitUpdate(taskId) to remove from pending queue\n   - Implement rollbackUpdate(taskId) to restore original state\n2. Add conflict resolution logic in WebSocket message handler:\n   - Check if incoming update conflicts with pending optimistic update\n   - If conflict: compare timestamps, keep most recent\n   - If WebSocket update is newer: apply it and clear pending update\n   - If optimistic update is newer: ignore WebSocket update temporarily\n3. Implement error handling in KanbanBoard:\n   - On API error: show toast notification with error message\n   - Automatically rollback the task to original column\n   - Add retry button in error toast\n4. Add visual indicators:\n   - Show loading spinner on task card during pending update\n   - Show error indicator if update failed\n   - Dim task card slightly during optimistic update\n5. Implement debouncing for rapid successive updates\n6. Add comprehensive error boundaries\n7. Create src/components/Toast.tsx for user notifications",
          "id": 7,
          "parentId": "27",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: Test optimistic update flow end-to-end. Mock API to simulate success/failure. Test rollback occurs on API error. Test conflict resolution with mock WebSocket messages. E2E tests: Test user drags task, verify immediate UI update, verify API call, test rollback on network failure.",
          "title": "Implement optimistic updates with rollback and conflict resolution",
          "updatedAt": "2025-12-06T17:03:47.868077669Z"
        }
      ],
      "testStrategy": "Component tests with React Testing Library, E2E tests with Playwright for drag-and-drop, verify WebSocket reconnection, test optimistic updates rollback",
      "title": "Build React dashboard with Kanban board and drag-and-drop"
    },
    {
      "agentHint": "blaze",
      "dependencies": [
        "27"
      ],
      "description": "Create activity timeline showing recent team actions and member management interface with role assignment",
      "details": "1. Create components:\n   - src/components/ActivityFeed.tsx: timeline of recent task/member events\n   - src/components/MemberList.tsx: table of team members with roles\n   - src/components/InviteModal.tsx: generate and display invite link\n2. Add activity_log table to schema: id, team_id, user_id, action (enum), entity_type, entity_id, created_at\n3. Create API endpoint GET /api/teams/:id/activity -> return last 50 events\n4. Log activities in task/member operations: task.created, task.updated, member.joined, member.role_changed\n5. Member management features:\n   - Display member list with role badges\n   - Owner/admin can change member roles via PATCH /api/teams/:team_id/members/:user_id\n   - Owner/admin can remove members via DELETE /api/teams/:team_id/members/:user_id\n6. Implement invite link generation: click button, show modal with copyable link\n7. Style with Tailwind: use timeline component for activity feed",
      "id": "28",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:04:28.858643716Z",
          "dependencies": [],
          "description": "Create the activity_log database table and implement logging functions to track team activities across task and member operations.",
          "details": "1. Add activity_log table to database schema with columns: id (uuid/serial primary key), team_id (foreign key), user_id (foreign key), action (enum: task.created, task.updated, task.deleted, member.joined, member.left, member.role_changed), entity_type (string), entity_id (string), metadata (jsonb for additional context), created_at (timestamp). 2. Create database migration file. 3. Implement server-side logging utility function logActivity(teamId, userId, action, entityType, entityId, metadata) that inserts records into activity_log table. 4. Add activity logging calls to existing endpoints: POST /api/tasks (task.created), PATCH /api/tasks/:id (task.updated), DELETE /api/tasks/:id (task.deleted), POST /api/teams/:id/members (member.joined). 5. Add indexes on team_id and created_at for efficient querying. 6. Test logging by performing operations and verifying database entries.",
          "id": 1,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for logActivity function with various action types. Integration tests verifying activity logs are created when performing task/member operations. Database query performance tests for activity retrieval.",
          "title": "Implement activity log schema and backend logging infrastructure",
          "updatedAt": "2025-12-06T17:04:28.858643716Z"
        },
        {
          "createdAt": "2025-12-06T17:04:28.858649007Z",
          "dependencies": [
            "1"
          ],
          "description": "Build GET /api/teams/:id/activity endpoint that returns paginated activity logs with user and entity details.",
          "details": "1. Create GET /api/teams/:team_id/activity endpoint in backend. 2. Implement authorization check: verify requesting user is a member of the team. 3. Query activity_log table filtered by team_id, ordered by created_at DESC. 4. Implement pagination with query parameters: limit (default 50, max 100), offset or cursor-based pagination. 5. Join with users table to include actor details (name, avatar). 6. Format response with activity objects including: id, action, actor {id, name, avatar}, entity {type, id, name}, metadata, timestamp. 7. Add optional filtering by action type via query parameter. 8. Implement error handling for invalid team_id or unauthorized access. 9. Add rate limiting to prevent abuse.",
          "id": 2,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "API integration tests for authenticated requests returning correct activities. Test pagination with various limit/offset values. Verify authorization blocks non-members. Test filtering by action type. Performance test with large activity datasets.",
          "title": "Create activity feed API endpoint with pagination and filtering",
          "updatedAt": "2025-12-06T17:04:28.858649007Z"
        },
        {
          "createdAt": "2025-12-06T17:04:28.858650632Z",
          "dependencies": [
            "2"
          ],
          "description": "Create React component that displays team activities in a visual timeline format with appropriate icons and formatting for each event type.",
          "details": "1. Create src/components/ActivityFeed.tsx component. 2. Fetch activities from GET /api/teams/:id/activity using React Query or similar. 3. Implement timeline UI using Tailwind CSS with vertical line and event nodes. 4. Create event renderer that displays different formats based on action type: task.created (show task title with link), task.updated (show what changed), member.joined (show member name), member.role_changed (show old->new role). 5. Display user avatar, name, and relative timestamp for each event. 6. Implement infinite scroll or 'Load More' button for pagination. 7. Add loading skeleton while fetching. 8. Handle empty state with friendly message. 9. Add icons for each action type (task icon, user icon, etc.). 10. Make event entities clickable (tasks link to task detail, members link to profile).",
          "id": 3,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests rendering various activity types correctly. Test pagination behavior with mock data. Verify loading and empty states display properly. Test click handlers for entity links. Visual regression tests for timeline styling.",
          "title": "Build ActivityFeed timeline component with event rendering",
          "updatedAt": "2025-12-06T17:04:28.858650632Z"
        },
        {
          "createdAt": "2025-12-06T17:04:28.858650799Z",
          "dependencies": [
            "1"
          ],
          "description": "Create member management UI displaying team members with role badges and controls for owners/admins to change roles or remove members.",
          "details": "1. Create src/components/MemberList.tsx component displaying table/list of team members. 2. Fetch members from existing GET /api/teams/:id/members endpoint. 3. Display each member with: avatar, name, email, role badge (styled with Tailwind), join date. 4. Implement role dropdown for owners/admins: show current role, allow changing to owner/admin/member. 5. Create PATCH /api/teams/:team_id/members/:user_id endpoint with body {role: string}, authorization check (only owner/admin can change roles, cannot demote last owner), validation (role must be valid enum), log member.role_changed activity. 6. Add remove member button (trash icon) for owners/admins. 7. Create DELETE /api/teams/:team_id/members/:user_id endpoint with authorization (only owner/admin, cannot remove self if last owner, cannot remove last owner), log member.left activity. 8. Show confirmation dialog before removing member. 9. Disable controls for current user's own role if they're the last owner. 10. Add loading states and optimistic updates. 11. Display member count and role distribution.",
          "id": 4,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests for member list rendering and role badge display. Test role change flow with mock API. Test remove member confirmation dialog. API integration tests for PATCH/DELETE endpoints with authorization rules. Test edge cases: last owner protection, self-removal prevention. Verify activity logging for role changes and removals.",
          "title": "Implement member list component with role management and removal",
          "updatedAt": "2025-12-06T17:04:28.858650799Z"
        },
        {
          "createdAt": "2025-12-06T17:04:28.858651049Z",
          "dependencies": [],
          "description": "Build modal component that generates shareable team invite links and provides easy copy-to-clipboard functionality.",
          "details": "1. Create src/components/InviteModal.tsx modal component with clean UI. 2. Create POST /api/teams/:team_id/invites endpoint that generates unique invite token (uuid or secure random string), stores in team_invites table (id, team_id, token, created_by, expires_at, used_at), returns invite URL (e.g., /join/:token). 3. Add authorization check: only team members can generate invites. 4. Display full invite URL in modal with copy button. 5. Implement copy-to-clipboard functionality using navigator.clipboard API with fallback. 6. Show success toast/message after copying. 7. Add expiration info (e.g., 'Expires in 7 days'). 8. Create GET /api/invites/:token endpoint to validate and retrieve invite details. 9. Implement invite redemption flow: user clicks link, sees team info, clicks join, POST /api/invites/:token/accept adds user to team. 10. Style modal with Tailwind: centered, responsive, with close button. 11. Add option to regenerate link (invalidates old token). 12. Handle error states (generation failed, copy failed).",
          "id": 5,
          "parentId": "28",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Component tests for modal rendering and copy functionality. Test invite generation API endpoint. Test invite validation and redemption flow end-to-end. Verify authorization prevents non-members from generating invites. Test token expiration logic. Test clipboard API with and without browser support. Verify invite redemption adds user to team and logs activity.",
          "title": "Create invite modal with link generation and copy functionality",
          "updatedAt": "2025-12-06T17:04:28.858651049Z"
        }
      ],
      "testStrategy": "Component tests for activity feed rendering, test role change authorization, verify activity logging on operations, E2E test for invite flow",
      "title": "Implement team activity feed and member management UI"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "27"
      ],
      "description": "Add theme switching capability with localStorage persistence and system preference detection",
      "details": "1. Create src/contexts/ThemeContext.tsx:\n   - type Theme = 'light' | 'dark' | 'system'\n   - useTheme hook: provides theme state and toggle function\n   - Detect system preference with window.matchMedia('(prefers-color-scheme: dark)')\n2. Store preference in localStorage: theme key\n3. Apply theme via Tailwind dark mode class on <html> element\n4. Create src/components/ThemeToggle.tsx: button with sun/moon icons\n5. Update tailwind.config.js: darkMode: 'class'\n6. Define color palette in tailwind.config.js:\n   - Light: bg-white, text-gray-900\n   - Dark: bg-gray-900, text-gray-100\n7. Update all components to use theme-aware Tailwind classes\n8. Add theme toggle to navigation bar",
      "id": "29",
      "priority": "low",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:04:49.717417086Z",
          "dependencies": [],
          "description": "Implement a React context for managing theme state with localStorage persistence and automatic system preference detection. This provides the foundation for theme switching across the application.",
          "details": "Create src/contexts/ThemeContext.tsx with:\n- Type definition: type Theme = 'light' | 'dark' | 'system'\n- ThemeProvider component that wraps the app\n- useTheme hook returning { theme, setTheme, resolvedTheme }\n- localStorage integration to persist user preference under 'theme' key\n- System preference detection using window.matchMedia('(prefers-color-scheme: dark)')\n- Event listener for system preference changes\n- Apply theme by adding/removing 'dark' class on document.documentElement\n- Initialize theme on mount: check localStorage first, fall back to system preference\n- Ensure SSR compatibility with useEffect for client-side only code\n- Export ThemeProvider and useTheme hook",
          "id": 1,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests: (1) Verify theme state changes correctly, (2) Mock localStorage to test persistence, (3) Mock matchMedia to test system preference detection, (4) Verify 'dark' class is applied/removed from html element, (5) Test theme initialization order (localStorage > system > default)",
          "title": "Create ThemeContext with localStorage persistence and system preference detection",
          "updatedAt": "2025-12-06T17:04:49.717417086Z"
        },
        {
          "createdAt": "2025-12-06T17:04:49.717421461Z",
          "dependencies": [
            "1"
          ],
          "description": "Set up Tailwind CSS for dark mode support using class-based strategy and define a comprehensive color palette for both light and dark themes to ensure consistent styling.",
          "details": "Update tailwind.config.js:\n- Set darkMode: 'class' to enable class-based dark mode\n- Define color palette in theme.extend.colors:\n  * Light theme: background colors (bg-white, bg-gray-50, bg-gray-100), text colors (text-gray-900, text-gray-700, text-gray-600)\n  * Dark theme: background colors (dark:bg-gray-900, dark:bg-gray-800, dark:bg-gray-700), text colors (dark:text-gray-100, dark:text-gray-200, dark:text-gray-300)\n- Define semantic color tokens for borders, shadows, and interactive elements\n- Ensure proper contrast ratios for accessibility in both themes\n- Document color usage guidelines for consistency",
          "id": 2,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Manual testing: (1) Verify Tailwind generates dark mode classes correctly, (2) Check color contrast meets WCAG AA standards in both themes, (3) Visual inspection of color palette in Storybook or style guide",
          "title": "Configure Tailwind dark mode and define color palette",
          "updatedAt": "2025-12-06T17:04:49.717421461Z"
        },
        {
          "createdAt": "2025-12-06T17:04:49.717423045Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Build a theme toggle button component with sun/moon icons and systematically update all existing components to use theme-aware Tailwind classes for seamless light/dark mode switching.",
          "details": "Create src/components/ThemeToggle.tsx:\n- Button component using useTheme hook\n- Sun icon for light mode, moon icon for dark mode\n- Smooth transition animations between states\n- Accessible button with aria-label\n- Keyboard navigation support\n\nUpdate all existing components:\n- Replace hardcoded colors with theme-aware Tailwind classes (bg-white -> bg-white dark:bg-gray-900)\n- Update text colors (text-gray-900 -> text-gray-900 dark:text-gray-100)\n- Update border colors (border-gray-200 -> border-gray-200 dark:border-gray-700)\n- Update hover/focus states for both themes\n- Add smooth transitions with transition-colors class\n\nIntegrate ThemeToggle:\n- Add ThemeToggle to navigation bar/header component\n- Ensure proper positioning and styling\n- Wrap app with ThemeProvider in main entry point",
          "id": 3,
          "parentId": "29",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Integration tests: (1) Verify ThemeToggle renders correct icon based on theme, (2) Test clicking toggle changes theme across all components, (3) Visual regression tests for all components in both themes, (4) Test keyboard accessibility of toggle button, (5) Verify smooth transitions when switching themes",
          "title": "Create ThemeToggle component and update all components with theme-aware styling",
          "updatedAt": "2025-12-06T17:04:49.717423045Z"
        }
      ],
      "testStrategy": "Test theme persistence across page reloads, verify system preference detection, test all components in both themes, verify localStorage updates",
      "title": "Implement dark/light theme toggle with persistence"
    },
    {
      "agentHint": "rex",
      "dependencies": [
        "16"
      ],
      "description": "Add observability with Prometheus metrics endpoint, structured JSON logging, and distributed tracing with trace IDs",
      "details": "1. Add dependencies: prometheus = \"0.13\", tracing = \"0.1\", tracing-subscriber = { version = \"0.3\", features = [\"json\"] }, uuid = \"1.6\"\n2. Create infra/metrics.rs:\n   - Register metrics: http_requests_total (counter), http_request_duration_seconds (histogram), websocket_connections (gauge), rate_limit_rejections_total (counter)\n   - GET /metrics endpoint returning Prometheus format\n3. Create api/middleware/metrics.rs:\n   - Record http_requests_total with labels: method, path, status\n   - Record http_request_duration_seconds histogram\n4. Setup structured logging in main.rs:\n   - tracing_subscriber::fmt().json().with_target(false).init()\n   - Log format: {\"timestamp\", \"level\", \"message\", \"trace_id\", \"span\"}\n5. Create api/middleware/tracing.rs:\n   - Generate trace_id (UUID) for each request\n   - Add to response header: X-Trace-Id\n   - Include in all log entries via tracing span\n6. Instrument key functions with #[tracing::instrument]\n7. Log important events: auth attempts, rate limit hits, errors",
      "id": "30",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:05:22.583358921Z",
          "dependencies": [],
          "description": "Create the infrastructure for Prometheus metrics by adding the metrics module, registering all required metrics (http_requests_total counter, http_request_duration_seconds histogram, websocket_connections gauge, rate_limit_rejections_total counter), and implementing the GET /metrics endpoint that returns metrics in Prometheus text format.",
          "details": "Add prometheus = \"0.13\" and lazy_static = \"1.4\" to Cargo.toml. Create src/infra/metrics.rs module. Define and register metrics using lazy_static: http_requests_total (IntCounterVec with labels: method, path, status), http_request_duration_seconds (HistogramVec with labels: method, path, buckets: [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0, 5.0]), websocket_connections (IntGauge), rate_limit_rejections_total (IntCounterVec with label: endpoint). Create metrics_handler() async function that returns Response with prometheus::TextEncoder output. Register the GET /metrics route in the router. Ensure metrics are properly initialized at application startup.",
          "id": 1,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write unit tests to verify metrics are registered correctly. Test the /metrics endpoint returns valid Prometheus format by parsing output. Verify all metric types (counter, histogram, gauge) can be incremented/set. Use curl or HTTP client to validate endpoint accessibility and response format matches Prometheus specification.",
          "title": "Implement Prometheus metrics registration and /metrics endpoint",
          "updatedAt": "2025-12-06T17:05:22.583358921Z"
        },
        {
          "createdAt": "2025-12-06T17:05:22.583362129Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement middleware that automatically records HTTP request metrics including request counts with method/path/status labels and request duration histogram. This middleware should wrap all HTTP handlers and record metrics without impacting request processing performance.",
          "details": "Create src/api/middleware/metrics.rs module. Implement metrics_middleware function using axum::middleware::from_fn. Capture request start time using std::time::Instant::now(). Extract method and path from request. After response is generated, record metrics: increment http_requests_total with labels (method, path, status_code), observe http_request_duration_seconds with duration in seconds. Handle error cases gracefully to ensure metrics are always recorded. Apply middleware to router using .layer(middleware::from_fn(metrics_middleware)). Ensure middleware ordering places this after error handling but before route handlers.",
          "id": 2,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Create integration tests that make HTTP requests and verify metrics are incremented correctly. Test various HTTP methods (GET, POST, PUT, DELETE) and status codes (200, 400, 404, 500). Verify histogram buckets are populated correctly for different request durations. Load test with multiple concurrent requests to ensure no race conditions in metric recording.",
          "title": "Create metrics middleware for HTTP request tracking",
          "updatedAt": "2025-12-06T17:05:22.583362129Z"
        },
        {
          "createdAt": "2025-12-06T17:05:22.583363504Z",
          "dependencies": [],
          "description": "Configure the tracing infrastructure to output structured JSON logs with proper formatting including timestamp, log level, message, trace_id, and span information. Instrument key application functions to automatically include contextual information in logs.",
          "details": "Add to Cargo.toml: tracing = \"0.1\", tracing-subscriber = { version = \"0.3\", features = [\"json\", \"env-filter\"] }. In main.rs before server start, initialize: tracing_subscriber::fmt().json().with_target(false).with_current_span(true).with_span_list(true).with_level(true).with_timer(tracing_subscriber::fmt::time::UtcTime::rfc_3339()).init(). Add #[tracing::instrument] attribute to key functions: authentication handlers, rate limiting logic, WebSocket handlers, error handlers. Use tracing macros (info!, warn!, error!, debug!) to log important events: successful/failed auth attempts, rate limit hits, WebSocket connections/disconnections, API errors. Ensure all logs include structured fields using tracing::field syntax.",
          "id": 3,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Verify log output is valid JSON by parsing with jq or JSON parser. Test that instrumented functions automatically include function name and parameters in logs. Trigger various application events (auth, rate limiting, errors) and verify corresponding log entries with correct log levels. Validate timestamp format is RFC3339. Check that nested spans create proper hierarchical log structure.",
          "title": "Setup structured JSON logging with tracing-subscriber",
          "updatedAt": "2025-12-06T17:05:22.583363504Z"
        },
        {
          "createdAt": "2025-12-06T17:05:22.583363921Z",
          "dependencies": [
            "3"
          ],
          "description": "Create middleware that generates unique trace IDs for each incoming request, propagates them through the request lifecycle via tracing spans, includes them in response headers, and ensures they appear in all log entries associated with that request.",
          "details": "Add uuid = { version = \"1.6\", features = [\"v4\"] } to Cargo.toml. Create src/api/middleware/tracing.rs module. Implement tracing_middleware function that: generates UUID v4 as trace_id for each request, creates a tracing span with trace_id field using tracing::info_span!(\"http_request\", trace_id = %trace_id), enters the span for request processing, adds X-Trace-Id header to response with the trace_id value, ensures span is properly dropped after response. Store trace_id in request extensions for access by handlers if needed. Apply middleware early in the stack using .layer(middleware::from_fn(tracing_middleware)). Verify trace_id appears in all log entries within the request context through span inheritance.",
          "id": 4,
          "parentId": "30",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test that each request receives a unique trace_id in the X-Trace-Id response header. Verify trace_id format is valid UUID v4. Make a request and check that all log entries for that request contain the same trace_id. Test concurrent requests to ensure trace IDs don't leak between requests. Verify trace_id propagates correctly through async operations and nested function calls with #[instrument] attributes.",
          "title": "Implement tracing middleware for trace ID generation and propagation",
          "updatedAt": "2025-12-06T17:05:22.583363921Z"
        }
      ],
      "testStrategy": "Verify /metrics endpoint returns valid Prometheus format, test metrics increment on requests, verify trace_id in logs and headers, load test to validate histogram buckets",
      "title": "Implement Prometheus metrics and structured logging"
    },
    {
      "agentHint": "bolt",
      "dependencies": [
        "30"
      ],
      "description": "Build production-ready Docker image with multi-stage build and create Kubernetes deployment with HPA and health checks",
      "details": "1. Create Dockerfile:\n   - Stage 1 (builder): FROM rust:1.75-alpine, cargo build --release\n   - Stage 2 (runtime): FROM alpine:3.19, copy binary, add ca-certificates\n   - EXPOSE 8080\n2. Create .dockerignore: target/, node_modules/, .git/\n3. Create infra/k8s/deployment.yaml:\n   - Deployment with 2 replicas\n   - Resources: requests (cpu: 100m, memory: 128Mi), limits (cpu: 500m, memory: 512Mi)\n   - livenessProbe: GET /health/live, initialDelaySeconds: 10\n   - readinessProbe: GET /health/ready, initialDelaySeconds: 5\n   - Env vars from ConfigMap and Secret\n4. Create infra/k8s/service.yaml: ClusterIP service on port 80 -> 8080\n5. Create infra/k8s/hpa.yaml:\n   - HorizontalPodAutoscaler: min 2, max 10 replicas\n   - Target: 70% CPU utilization\n6. Create infra/k8s/configmap.yaml: non-sensitive config\n7. Create infra/k8s/secret.yaml: DATABASE_URL, REDIS_URL, JWT_SECRET\n8. Implement health endpoints in api/health.rs:\n   - GET /health/live -> 200 OK\n   - GET /health/ready -> check DB and Redis connections, return 200 or 503",
      "id": "31",
      "priority": "high",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:06:07.958968095Z",
          "dependencies": [],
          "description": "Implement a production-ready multi-stage Dockerfile using Rust builder and Alpine runtime, including .dockerignore for build optimization",
          "details": "Create Dockerfile with two stages: (1) Builder stage using rust:1.75-alpine as base, copy source code, run cargo build --release with optimization flags, (2) Runtime stage using alpine:3.19, install ca-certificates and required runtime dependencies, copy only the compiled binary from builder stage, set non-root user for security, EXPOSE port 8080, and define ENTRYPOINT. Create .dockerignore file excluding target/, node_modules/, .git/, *.md, .env*, and test files to reduce build context size. Add LABEL metadata for image versioning and maintainer info. Optimize layer caching by copying Cargo.toml and Cargo.lock first, then building dependencies separately before copying source code.",
          "id": 1,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Build the Docker image locally using 'docker build -t app:test .', verify image size is under 50MB, test running container with 'docker run -p 8080:8080 app:test', confirm application starts successfully and responds to requests, verify no unnecessary files are included in final image using 'docker history' and 'dive' tool",
          "title": "Create multi-stage Dockerfile with optimized build and runtime stages",
          "updatedAt": "2025-12-06T17:06:07.958968095Z"
        },
        {
          "createdAt": "2025-12-06T17:06:07.958972095Z",
          "dependencies": [],
          "description": "Create health check endpoints for liveness and readiness probes with database and Redis connection validation",
          "details": "Create api/health.rs module with two endpoints: (1) GET /health/live - simple liveness check that returns 200 OK with JSON {\"status\": \"alive\"} to indicate the process is running, (2) GET /health/ready - readiness check that validates database connection using a lightweight query (SELECT 1) and Redis connection using PING command, returns 200 OK with JSON {\"status\": \"ready\", \"checks\": {\"database\": \"ok\", \"redis\": \"ok\"}} if all checks pass, or 503 Service Unavailable with details of failed checks. Implement connection pooling health checks with timeout of 2 seconds per check. Add proper error handling and logging for failed health checks. Register routes in main application router.",
          "id": 2,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Write unit tests for health check logic with mocked database and Redis connections, test both success and failure scenarios, verify correct HTTP status codes and response bodies, manually test endpoints with curl after starting application, simulate database/Redis failures to verify readiness probe returns 503, use load testing tool to verify health endpoints can handle high request rates without impacting main application performance",
          "title": "Implement health check endpoints in backend API",
          "updatedAt": "2025-12-06T17:06:07.958972095Z"
        },
        {
          "createdAt": "2025-12-06T17:06:07.958972803Z",
          "dependencies": [
            "1",
            "2"
          ],
          "description": "Build Kubernetes Deployment configuration with replica management, resource allocation, and integrated health check probes",
          "details": "Create infra/k8s/deployment.yaml with Deployment resource: set replicas to 2 for high availability, define container spec with image pull policy IfNotPresent, configure resource requests (cpu: 100m, memory: 128Mi) and limits (cpu: 500m, memory: 512Mi), add livenessProbe using httpGet on /health/live with initialDelaySeconds: 10, periodSeconds: 30, timeoutSeconds: 5, failureThreshold: 3, add readinessProbe using httpGet on /health/ready with initialDelaySeconds: 5, periodSeconds: 10, timeoutSeconds: 3, failureThreshold: 3. Configure environment variables from ConfigMap (non-sensitive config) and Secret (DATABASE_URL, REDIS_URL, JWT_SECRET) using envFrom and valueFrom. Add pod labels for service selector and monitoring. Set security context with runAsNonRoot: true and readOnlyRootFilesystem: true where possible. Include rolling update strategy with maxSurge: 1 and maxUnavailable: 0 for zero-downtime deployments.",
          "id": 3,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Validate YAML syntax using 'kubectl apply --dry-run=client -f deployment.yaml', deploy to test Kubernetes cluster, verify pods start successfully with 'kubectl get pods', check pod logs for startup errors, test liveness probe by exec into pod and killing process to verify automatic restart, test readiness probe by temporarily breaking database connection to verify pod removed from service endpoints, verify resource limits are enforced using 'kubectl top pods', test rolling update by deploying new image version",
          "title": "Create Kubernetes Deployment manifest with resource limits and health probes",
          "updatedAt": "2025-12-06T17:06:07.958972803Z"
        },
        {
          "createdAt": "2025-12-06T17:06:07.958975178Z",
          "dependencies": [
            "3"
          ],
          "description": "Define Service for internal routing, ConfigMap for application configuration, and Secret for sensitive credentials",
          "details": "Create infra/k8s/service.yaml with Service resource type ClusterIP, selector matching deployment pod labels, port mapping from 80 (service) to 8080 (container targetPort), add sessionAffinity if needed. Create infra/k8s/configmap.yaml with ConfigMap containing non-sensitive configuration: LOG_LEVEL, API_TIMEOUT, CORS_ORIGINS, RATE_LIMIT_REQUESTS, CACHE_TTL, and any environment-specific settings. Create infra/k8s/secret.yaml template (for documentation) showing required secret keys: DATABASE_URL, REDIS_URL, JWT_SECRET with placeholder values, add comments indicating secrets should be created using 'kubectl create secret' or sealed-secrets in production. Include namespace specification in all manifests. Add annotations for documentation and monitoring integration.",
          "id": 4,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Validate all YAML files with 'kubectl apply --dry-run=client', create actual secrets in test cluster using 'kubectl create secret generic app-secrets --from-literal=DATABASE_URL=test --from-literal=REDIS_URL=test --from-literal=JWT_SECRET=test', apply ConfigMap and verify with 'kubectl get configmap', apply Service and verify with 'kubectl get svc', test service connectivity from within cluster using temporary pod with curl, verify environment variables are properly injected into pods by checking 'kubectl exec' output, test configuration changes by updating ConfigMap and verifying pod picks up changes after restart",
          "title": "Create Kubernetes Service, ConfigMap, and Secret manifests",
          "updatedAt": "2025-12-06T17:06:07.958975178Z"
        },
        {
          "createdAt": "2025-12-06T17:06:07.958975720Z",
          "dependencies": [
            "3",
            "4"
          ],
          "description": "Create HPA manifest for automatic scaling based on CPU utilization and test scaling behavior under load",
          "details": "Create infra/k8s/hpa.yaml with HorizontalPodAutoscaler resource: set scaleTargetRef to the deployment name, configure minReplicas: 2 and maxReplicas: 10, set target CPU utilization to 70% using metrics.type: Resource and metrics.resource.name: cpu with target.type: Utilization and target.averageUtilization: 70. Add behavior configuration for controlled scaling: scaleDown stabilizationWindowSeconds: 300 to prevent flapping, scaleUp policies with periodSeconds: 15 and value: 2 for faster scale-up. Include annotations for monitoring and alerting. Create a load testing script or documentation using tools like 'kubectl run -it --rm load-generator --image=busybox -- /bin/sh' with while loop hitting the service endpoint, or using hey/ab tools. Document expected scaling behavior and thresholds.",
          "id": 5,
          "parentId": "31",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Apply HPA manifest and verify with 'kubectl get hpa', ensure metrics-server is installed and working in cluster with 'kubectl top nodes', monitor baseline CPU usage with no load, generate load using load testing tool targeting the service endpoint, observe HPA scaling up pods with 'kubectl get hpa -w' and 'kubectl get pods -w', verify new pods are created when CPU exceeds 70%, confirm pods reach Ready state and start receiving traffic, stop load generation and verify scale-down after stabilization window, test that HPA respects min/max replica bounds, verify scaling events in 'kubectl describe hpa', document actual scaling behavior and tune thresholds if needed",
          "title": "Configure HorizontalPodAutoscaler and validate autoscaling behavior",
          "updatedAt": "2025-12-06T17:06:07.958975720Z"
        }
      ],
      "testStrategy": "Build Docker image and verify size <50MB, test container startup, deploy to local k8s (minikube), verify HPA scales on load, test health endpoints, verify rolling updates work",
      "title": "Create Docker multi-stage build and Kubernetes manifests"
    },
    {
      "agentHint": "tap",
      "dependencies": [
        "21",
        "22"
      ],
      "description": "Add data export and deletion endpoints to comply with GDPR requirements for user data portability and right to be forgotten",
      "details": "1. Create api/gdpr.rs:\n   - GET /api/users/me/export -> generate JSON export of all user data\n   - POST /api/users/me/delete -> initiate account deletion\n2. Data export includes:\n   - User profile (email, created_at)\n   - Teams owned/member of\n   - All tasks created or assigned\n   - Activity history\n   - Format: { \"user\": {...}, \"teams\": [...], \"tasks\": [...], \"activity\": [...] }\n3. Account deletion process:\n   - Soft delete user (set deleted_at)\n   - Anonymize user data in tasks (set assignee_id to NULL, created_by to 'deleted_user')\n   - Remove from all teams\n   - Delete OAuth tokens\n   - Delete device tokens\n   - Schedule permanent deletion after 30 days\n4. Add deletion confirmation: require password re-entry\n5. Send email confirmation of deletion request\n6. Create infra/jobs/gdpr_cleanup.rs: permanently delete users after 30 days",
      "id": "32",
      "priority": "medium",
      "status": "pending",
      "subtasks": [
        {
          "createdAt": "2025-12-06T17:06:52.596701463Z",
          "dependencies": [],
          "description": "Create GET /api/users/me/export endpoint that aggregates all user data from multiple tables including profile, teams, tasks, and activity history into a structured JSON format for GDPR data portability compliance.",
          "details": "Create api/gdpr.rs module with export_user_data handler. Query database to collect: user profile (id, email, name, created_at, updated_at), teams (owned and member relationships with roles), all tasks (created by user or assigned to user with full details), and activity/audit logs. Structure response as: { \"user\": {...}, \"teams\": [...], \"tasks\": [...], \"activity\": [...] }. Ensure all timestamps are ISO 8601 formatted. Add authentication middleware to verify user identity. Include pagination handling if data volume is large. Add response headers for Content-Disposition to suggest filename with timestamp.",
          "id": 1,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for data aggregation from each table, integration tests verifying complete data export with test user having diverse data (multiple teams, tasks, activity), verify JSON structure matches specification, test with empty data sets, verify authentication requirements, test response headers and content type",
          "title": "Implement data export endpoint with comprehensive user data aggregation",
          "updatedAt": "2025-12-06T17:06:52.596701463Z"
        },
        {
          "createdAt": "2025-12-06T17:06:52.596703713Z",
          "dependencies": [
            "1"
          ],
          "description": "Implement POST /api/users/me/delete endpoint that initiates the account deletion process with mandatory password re-entry for security confirmation before proceeding with soft deletion.",
          "details": "Add delete_account_request handler in api/gdpr.rs accepting password in request body. Validate current password using existing authentication logic (bcrypt verification). Return 401 if password is incorrect with appropriate error message. On successful validation, set deleted_at timestamp on user record (soft delete). Create deletion_requests table to track deletion state with fields: user_id, requested_at, scheduled_deletion_at (30 days future), confirmation_token. Generate unique confirmation token for email verification. Return success response with deletion scheduled date. Add rate limiting to prevent abuse (max 3 attempts per hour).",
          "id": 2,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test password validation (correct/incorrect passwords), verify soft delete flag is set, test rate limiting enforcement, verify deletion_requests record creation with correct timestamps, test with already-deleted accounts, verify error handling for invalid requests, test authentication requirements",
          "title": "Create account deletion initiation endpoint with password confirmation",
          "updatedAt": "2025-12-06T17:06:52.596703713Z"
        },
        {
          "createdAt": "2025-12-06T17:06:52.596704671Z",
          "dependencies": [
            "2"
          ],
          "description": "Create comprehensive data anonymization functions that remove or pseudonymize user personal data from tasks, comments, and other related entities while maintaining data integrity and referential relationships.",
          "details": "Create anonymize_user_data function in api/gdpr.rs. For tasks table: set assignee_id to NULL where user is assigned, update created_by field to 'deleted_user' string identifier, preserve task content but remove personal references. For comments/activity: replace user references with 'deleted_user', maintain timestamps for audit purposes. For any user-generated content fields, replace with anonymized placeholder while keeping structure intact. Use database transactions to ensure atomicity. Log anonymization actions for compliance audit trail. Ensure foreign key constraints are handled properly (SET NULL where appropriate). Do not delete records, only anonymize to preserve business data integrity.",
          "id": 3,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Unit tests for anonymization functions on each entity type, verify NULL assignments and 'deleted_user' replacements, test transaction rollback on failure, verify referential integrity maintained, test with complex data relationships, verify audit logs created, integration tests with real data scenarios, verify no personal data remains after anonymization",
          "title": "Implement data anonymization logic for tasks and related entities",
          "updatedAt": "2025-12-06T17:06:52.596704671Z"
        },
        {
          "createdAt": "2025-12-06T17:06:52.596704921Z",
          "dependencies": [
            "3"
          ],
          "description": "Create cleanup logic to remove user from all team memberships, revoke OAuth tokens, delete device tokens, and handle team ownership transfers during the account deletion process.",
          "details": "Extend deletion logic in api/gdpr.rs with cleanup_user_associations function. Remove all team_members records where user_id matches. For teams where user is owner: if team has other members, transfer ownership to oldest admin member; if no other members, mark team for deletion. Delete all oauth_tokens records for user. Delete all device_tokens/push notification tokens. Delete all user_sessions. Remove user from any notification subscriptions. Use database transactions to ensure all-or-nothing cleanup. Create audit log entries for each cleanup action. Handle edge cases like sole team owner gracefully with appropriate warnings in deletion confirmation.",
          "id": 4,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test team membership removal from multiple teams, verify ownership transfer logic (with/without other admins), test token deletion (OAuth and device tokens), verify session cleanup, test transaction atomicity, test edge cases (sole owner scenarios), verify audit logging, integration tests with complex team structures",
          "title": "Implement team and token cleanup on account deletion",
          "updatedAt": "2025-12-06T17:06:52.596704921Z"
        },
        {
          "createdAt": "2025-12-06T17:06:52.596705254Z",
          "dependencies": [
            "2"
          ],
          "description": "Create email notification system that sends confirmation emails when account deletion is requested, including deletion details, cancellation instructions, and scheduled permanent deletion date.",
          "details": "Create email template for deletion confirmation with: deletion request timestamp, scheduled permanent deletion date (30 days), summary of what will be deleted, cancellation link with token, contact information for support. Implement send_deletion_confirmation_email function using existing email service. Include cancellation endpoint POST /api/users/me/delete/cancel accepting confirmation_token that removes deleted_at flag and clears deletion_requests record if called before 30 days. Email should be sent immediately after successful deletion request. Use HTML and plain text versions. Include legally required GDPR compliance language. Log email sending status for audit purposes.",
          "id": 5,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test email generation with correct content and formatting, verify email delivery (mock email service), test cancellation endpoint with valid/invalid tokens, test cancellation after 30-day window, verify deletion_requests cleanup on cancellation, test HTML and plain text rendering, verify all required legal language present",
          "title": "Implement email confirmation system for deletion requests",
          "updatedAt": "2025-12-06T17:06:52.596705254Z"
        },
        {
          "createdAt": "2025-12-06T17:06:52.596705504Z",
          "dependencies": [
            "3",
            "4",
            "5"
          ],
          "description": "Implement background job in infra/jobs/gdpr_cleanup.rs that runs daily to permanently delete user accounts and associated data after the 30-day grace period has elapsed since deletion request.",
          "details": "Create infra/jobs/gdpr_cleanup.rs with permanent_deletion_job function. Query deletion_requests table for records where scheduled_deletion_at <= current_timestamp. For each user: verify deleted_at is still set (not cancelled), permanently DELETE user record (hard delete), delete all anonymized records if required by policy, delete deletion_requests record, send final confirmation email. Use job scheduler (cron or similar) to run daily at off-peak hours. Implement idempotency to handle job failures. Add comprehensive logging for compliance audit trail. Include metrics for monitoring (users deleted, errors). Add alerting for job failures. Ensure database cascading deletes are configured properly or handle manually.",
          "id": 6,
          "parentId": "32",
          "priority": "medium",
          "status": "pending",
          "testStrategy": "Test job execution with users past 30-day window, verify permanent deletion of user records, test with cancelled deletion requests (should skip), verify final email sent, test job idempotency with duplicate runs, test error handling and rollback, verify logging and metrics collection, integration test full 30-day lifecycle (mock time), test cascading deletes",
          "title": "Create scheduled job for permanent deletion after 30-day grace period",
          "updatedAt": "2025-12-06T17:06:52.596705504Z"
        }
      ],
      "testStrategy": "Test data export completeness, verify account deletion anonymizes data correctly, test 30-day deletion delay, verify email confirmation sent, test deletion cancellation",
      "title": "Implement GDPR compliance features"
    }
  ]
}