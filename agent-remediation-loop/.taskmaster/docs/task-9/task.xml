<prompt>
    <role>You are a senior site reliability engineer specializing in observability, monitoring systems, and distributed system instrumentation.</role>
    <task>
        <id>9</id>
        <title>Create Monitoring and Observability</title>
        <description>Implement comprehensive monitoring with Prometheus metrics, structured logging, and alerting</description>
        <priority>medium</priority>
        <status>pending</status>
        <dependencies>3, 6, 8</dependencies>
    </task>
    <technical_specifications>
        <spec>Implement Prometheus metrics with comprehensive coverage: remediation_cycles_total, remediation_iteration_count, remediation_duration_seconds</spec>
        <spec>Build structured JSON logging using zap with context awareness and event-specific functions</spec>
        <spec>Create Grafana dashboards with overview, performance, and task-specific visualizations</spec>
        <spec>Set up Prometheus alerting rules with proper severity levels and runbook documentation</spec>
        <spec>Implement OpenTelemetry distributed tracing with Jaeger integration and span propagation</spec>
        <spec>Build health check system with component-specific checks and HTTP endpoint exposure</spec>
        <spec>Create observability client providing unified interface for metrics, logging, and tracing</spec>
        <spec>Ensure monitoring performance overhead remains below 5% of system resources</spec>
        <spec>Implement proper data retention policies: 30 days metrics, 7 days logs, 3 days traces</spec>
        <spec>Integrate with existing monitoring infrastructure and Kubernetes deployment patterns</spec>
    </technical_specifications>
    <implementation_details>
        Build a comprehensive monitoring and observability system for the Agent Remediation Loop:

        1. Define and implement Prometheus metrics:
           - Core remediation metrics: remediation_cycles_total with task_id/outcome/severity labels
           - Performance metrics: remediation_duration_seconds histogram with exponential buckets
           - Agent metrics: agent_cancellations_total, active_agents_count by type and task
           - Escalation tracking: escalations_total with reason and notification channel labels
           - State management: state_operations_total, state_operation_duration_seconds, configmap_size_bytes
           - GitHub integration: github_api_requests_total, github_api_rate_limit_remaining
           - System health: system_health_score, error_rate_percent by component
           - Use appropriate metric types and avoid high-cardinality labels

        2. Implement structured logging framework:
           - Configure zap logger with production settings and JSON output
           - Create context-aware logging functions extracting task_id, correlation_id
           - Build event-specific functions: LogRemediationStarted, LogRemediationCompleted, LogEscalation
           - Add operation logging: LogStateOperation, LogLabelOperation, LogGitHubAPICall
           - Implement comprehensive error logging with metadata and error context
           - Create system health logging with component status and check results
           - Use consistent timestamp formatting and structured field naming

        3. Create Grafana dashboard configurations:
           - Overview dashboard: remediation rates, active tasks, duration distributions
           - Performance dashboard: operation durations, error rates, health scores
           - Agent operations dashboard: cancellation rates, active counts, success metrics
           - Task-specific dashboard with templating for individual task analysis
           - Use appropriate visualizations: time series, gauges, heatmaps, pie charts
           - Configure proper time ranges, refresh intervals, and alert annotations

        4. Set up Prometheus alerting rules:
           - Iteration alerts: ExcessiveRemediationCycles (>7), RemediationStuck (no progress 2h)
           - System health: StateOperationFailures, SystemHealthDegraded, ConfigMapSizeTooLarge
           - API health: GitHubAPIRateLimitExhausted, GitHubAPIErrors with rate thresholds
           - Escalation monitoring: HighEscalationRate with trend detection
           - Configure severity levels (info/warning/critical) and for clauses
           - Include runbook URLs and actionable descriptions in annotations

        5. Implement OpenTelemetry distributed tracing:
           - Set up Jaeger exporter with proper resource attributes and service naming
           - Create tracing functions: TraceRemediationCycle, TraceAgentOperation, TraceStateOperation
           - Add GitHub API call tracing with HTTP method and URL attributes
           - Implement span event creation for significant state changes
           - Build error status setting with proper error recording
           - Create context propagation across service boundaries

        6. Build comprehensive health check system:
           - Implement HealthChecker with pluggable health check interface
           - Create component checks: GitHubAPIHealthCheck, StateManagerHealthCheck, KubernetesHealthCheck
           - Build overall health aggregation with status determination logic
           - Expose HTTP endpoints: /health, /healthz, /ready, /live for Kubernetes
           - Implement timeout handling and concurrent execution
           - Add health check result caching to prevent excessive load

        7. Create integrated observability client:
           - Build ObservabilityClient providing unified metrics, logging, and tracing
           - Implement context propagation across all observability components
           - Create convenience methods: StartRemediationCycle, CompleteRemediationCycle
           - Add error handling ensuring monitoring doesn't affect primary functionality
           - Integrate throughout application with minimal code changes

        The system must provide comprehensive visibility while maintaining minimal performance impact and integrating seamlessly with existing infrastructure.
    </implementation_details>
    <acceptance_criteria>
        <criterion>Prometheus metrics provide comprehensive coverage of all system components</criterion>
        <criterion>Structured logging captures all significant events with proper context</criterion>
        <criterion>Grafana dashboards enable effective monitoring and issue identification</criterion>
        <criterion>Alerting rules trigger appropriately with minimal false positives</criterion>
        <criterion>Distributed tracing enables end-to-end request flow analysis</criterion>
        <criterion>Health checks accurately reflect component and system status</criterion>
        <criterion>Performance overhead remains below 5% of system resources</criterion>
        <criterion>Integration with existing monitoring infrastructure works seamlessly</criterion>
        <criterion>Data retention policies implemented: 30d metrics, 7d logs, 3d traces</criterion>
        <criterion>Security requirements met with no sensitive data exposure</criterion>
        <criterion>Observability client provides easy integration throughout codebase</criterion>
        <criterion>HTTP endpoints support Kubernetes probes and external monitoring</criterion>
        <criterion>Error handling ensures monitoring failures don't cascade to primary system</criterion>
        <criterion>Documentation includes runbooks and troubleshooting guides</criterion>
        <criterion>Testing validates monitoring accuracy and performance under load</criterion>
    </acceptance_criteria>
    <test_strategy>
        1. Metrics Testing:
           - Validate metric accuracy through comparison with known system behavior
           - Test metric collection under high load conditions (100+ concurrent operations)
           - Verify histogram bucket configuration with realistic value distributions
           - Test metric cardinality to prevent Prometheus performance issues
           - Validate label consistency and naming conventions across related metrics

        2. Logging Testing:
           - Test structured logging with various context scenarios
           - Validate log completeness for all significant system events
           - Test log performance under high-frequency logging scenarios
           - Verify context propagation and correlation ID tracking
           - Test error logging with comprehensive metadata capture

        3. Dashboard Testing:
           - Validate dashboard functionality with various data scenarios
           - Test dashboard query performance with realistic data volumes
           - Verify visualization accuracy and usefulness for operations
           - Test templating functionality with multiple task scenarios
           - Validate dashboard refresh behavior and data freshness

        4. Alerting Testing:
           - Test alert rules with synthetic failure scenarios
           - Validate alert accuracy and timing with known conditions
           - Test alert severity levels and notification routing
           - Verify alert descriptions and runbook URL accessibility
           - Test alert recovery and acknowledgment workflows

        5. Tracing Testing:
           - Validate trace completeness for end-to-end request flows
           - Test trace correlation with logs using span context
           - Verify span attribute accuracy and error status setting
           - Test trace sampling strategies and performance impact
           - Validate context propagation across service boundaries

        6. Health Check Testing:
           - Test individual health checks with component failures
           - Validate overall health aggregation logic
           - Test health check timeout handling and error scenarios
           - Verify HTTP endpoint functionality with various status codes
           - Test health check performance and caching behavior

        7. Performance Testing:
           - Measure monitoring system overhead under various load conditions
           - Test memory usage growth with sustained operation
           - Validate monitoring system resilience during primary system stress
           - Test storage usage and retention policy enforcement
           - Benchmark query performance with growing data volumes

        8. Integration Testing:
           - Test integration with existing Prometheus and Grafana infrastructure
           - Validate Kubernetes deployment and service discovery
           - Test observability client integration throughout application
           - Verify monitoring system behavior during primary system failures
           - Test configuration management and deployment procedures

        9. Security Testing:
           - Validate no sensitive data exposure in logs, metrics, or traces
           - Test access control for monitoring endpoints and dashboards
           - Verify monitoring data encryption and transmission security
           - Test audit logging for monitoring configuration changes
           - Validate compliance with data retention and privacy requirements

        10. Operational Testing:
             - Test runbook procedures with real system scenarios
             - Validate alert response workflows and escalation paths
             - Test monitoring system maintenance and upgrade procedures
             - Verify backup and recovery procedures for monitoring data
             - Test operational procedures with team members
    </test_strategy>
    <instructions>
        Begin by understanding the complete system architecture and identifying all components that need observability coverage. Design a comprehensive metrics schema that balances visibility with performance impact.

        Focus on operational needs when designing dashboards and alerts. The monitoring system should help operators understand system behavior, identify issues quickly, and take appropriate action.

        Pay careful attention to performance impact throughout the implementation. Monitoring should never significantly degrade the system it's monitoring. Use asynchronous operations, batching, and sampling where appropriate.

        Implement proper error handling in all monitoring code. Monitoring system failures should not cascade to the primary application. Build resilience and graceful degradation into the monitoring infrastructure.

        Design for correlation across metrics, logs, and traces. Operators should be able to follow a single request or issue across all three observability pillars for effective troubleshooting.

        Test thoroughly with realistic data volumes and scenarios. Monitoring systems often behave differently under production loads than in development environments.

        Consider operational excellence from the start. Build monitoring that helps rather than hinders operations. Include proper documentation, runbooks, and training materials.

        Integrate security and privacy considerations throughout. Monitoring systems often have access to sensitive system information that must be protected.

        Build for maintainability and evolution. Monitoring requirements change as systems evolve, so design for easy extension and modification.

        Focus on actionable monitoring. Every alert should have a clear response procedure, and every dashboard should enable specific operational decisions.
    </instructions>
</prompt>