# Implementation Agent Design Document

**Generated from conversation analysis - Living document for implementation planning**

## üéØ **Overview**
Design specification for the TaskRun implementation agent that executes code changes based on documentation generated by the docs agent. The implementation agent operates independently from TaskMaster's task management system and focuses solely on code implementation.

## üó£Ô∏è **CTO ‚Üî Claude Conversation Examples**

*These examples illustrate the intended user experience when interacting with Claude (with MCP server access) to manage implementation tasks.*

### **Scenario 1: Starting First Implementation Task**

**CTO**:
> "I just finished generating docs for the trader service. The docs are merged into main. I want to start implementing task 1 using the claude-agent-1 GitHub account. The target repo is git@github.com:myorg/trader.git and I want to use the sonnet model."

**Claude (me) with MCP access**:
> I'll submit the first implementation task for your trader service. Let me use the orchestrator to set this up.

*[Uses MCP tool: `mcp_orchestrator_submit_implementation_task`]*

> ‚úÖ **Implementation task submitted successfully!**
>
> **Details:**
> - **Task**: 1 (trader service)
> - **Agent**: claude-agent-1
> - **Target Repo**: git@github.com:myorg/trader.git
> - **Model**: sonnet
> - **Working Directory**: trader/ (default)
> - **TaskRun**: `trader-claude-agent-1-task1-attempt1`
>
> The agent will:
> 1. Clone the platform repo to access task documentation
> 2. Clone/setup the trader repository
> 3. Use Rust-specific MCP tools (rustdocs, brave-search, memory)
> 4. Follow the tool usage patterns for efficient implementation
>
> You can monitor progress with:
> ```bash
> kubectl get taskrun trader-claude-agent-1-task1-attempt1 -n orchestrator
> ```

---

### **Scenario 2: Task Failure - Need for Retry**

**CTO**:
> "Task 1 failed. I looked at the logs and the agent chose the wrong HTTP client library. It used 'curl' crate but should use 'reqwest'. Can you retry this with better guidance?"

**Claude (me)**:
> I'll retry task 1 with specific guidance about using reqwest instead of curl. Let me submit a corrected attempt.

*[Uses MCP tool with prompt modification]*

> ‚úÖ **Task 1 retry submitted with corrections!**
>
> **Retry Strategy:**
> - **Preserved**: Same feature branch for continuity
> - **Added Guidance**: "Previous attempt failed because agent chose 'curl' crate. Use 'reqwest' crate instead for HTTP requests. Query rustdocs for reqwest examples before implementing."
> - **Tool Pattern**: Agent will query rustdocs for reqwest before coding
> - **TaskRun**: `trader-claude-agent-1-task1-attempt2`
>
> The agent will see the previous attempt's work and build on it with the corrected approach.

---

### **Scenario 3: Moving to Next Task**

**CTO**:
> "Great! Task 1 completed successfully. Let's move on to task 2, but I want to use a custom working directory 'trading-engine' instead of the default 'trader'."

**Claude (me)**:
> Perfect! I'll submit task 2 with the custom working directory. The system will automatically clean up from task 1 and create a fresh branch.

*[Uses MCP tool with working directory override]*

> ‚úÖ **Task 2 submitted with custom working directory!**
>
> **Setup:**
> - **Task**: 2 (trader service)
> - **Working Directory**: trading-engine/ (custom)
> - **Repository**: Will update existing repo, create new feature branch
> - **Cleanup**: Automatic cleanup from task 1 branch
> - **TaskRun**: `trader-claude-agent-1-task2-attempt1`
>
> The agent will work in `/workspace/target-repo/trading-engine/` and create that directory if it doesn't exist.

---

### **Scenario 4: Different Service**

**CTO**:
> "Now I want to start working on the auth service. This is a completely separate service with its own repo at git@github.com:myorg/platform-auth.git. Start with task 1."

**Claude (me)**:
> I'll submit the first task for the auth service. This will create a separate workspace and use different tool configurations.

*[Uses MCP tool with new service]*

> ‚úÖ **Auth service task 1 submitted!**
>
> **New Service Setup:**
> - **Service**: auth (separate PVC: `workspace-auth`)
> - **Repository**: git@github.com:myorg/platform-auth.git
> - **Fresh Environment**: New workspace, fresh repository clone
> - **Tool Configuration**: Will use auth-service specific tools
> - **TaskRun**: `auth-claude-agent-1-task1-attempt1`
>
> This is completely isolated from the trader service workspace.

---

### **Scenario 5: Checking Status**

**CTO**:
> "How's that auth task coming along?"

**Claude (me)**:
> Let me check the status for you.

*[Could use kubectl or monitoring tools if available]*

> **Task Status Update:**
> - **TaskRun**: auth-claude-agent-1-task1-attempt1
> - **Status**: Running (started 15 minutes ago)
> - **Phase**: Implementation in progress
> - **Logs**: Agent successfully queried rustdocs and is implementing authentication middleware
>
> The task appears to be progressing well. The agent followed the tool usage patterns and researched the appropriate authentication patterns before coding.

---

### **Scenario 6: Emergency Stop**

**CTO**:
> "Wait, I need to stop that auth task. I just realized we're changing the architecture."

**Claude (me)**:
> I can help you stop the running task. However, the current MCP tools don't include a stop function - you'll need to cancel it directly:

> ```bash
> kubectl delete taskrun auth-claude-agent-1-task1-attempt1 -n orchestrator
> ```
>
> The workspace will be preserved in the `workspace-auth` PVC, so when you're ready to restart with the new architecture, just let me know and I can submit a new task with updated guidance.

---

### **Key Natural Language Patterns**

The key is that you just **talk naturally** about what you want to implement, and I:

1. **Parse your intent** (which task, service, any special requirements)
2. **Use MCP tools** to submit the implementation
3. **Provide feedback** on what's happening
4. **Handle retries** with your guidance when things fail
5. **Track progress** across multiple services and tasks

You don't need to remember MCP syntax or TaskRun details - just tell me what you want to build, and I'll handle the orchestration!

## üèóÔ∏è **Agent Architecture**

### **Core Principles**
- **Documentation-Driven**: Reads docs agent output, ignores `tasks.json`
- **Repository Isolation**: Works in separate target repository
- **Multi-Attempt Support**: Leverages PVC persistence for retry scenarios
- **Tool-Specific**: Gets tailored MCP tools for each task type
- **Standards-Compliant**: Follows coding and GitHub guidelines

### **Input Sources**
- **task.md**: Core task requirements and description
- **acceptance-criteria.md**: Success criteria and validation requirements
- **architecture.md**: Technical design and implementation guidance
- **CLAUDE.md**: Project memory and context (generated from template)
- **Coding Guidelines**: Rust best practices and standards
- **GitHub Guidelines**: Commit message and PR workflow standards

### **Agent Workflow**
```
1. Container Setup ‚Üí Repository Cloning ‚Üí Documentation Loading
2. MCP Tool Configuration ‚Üí Claude Code Initialization
3. Code Implementation ‚Üí Testing ‚Üí Validation in Real Environment
4. Success Declaration Only After Observed Testing ‚Üí Commit Creation
5. Success/Failure Handling ‚Üí PVC Persistence for Retries
```

## üõ°Ô∏è **Agent Behavior Modifications**

### **System Prompt Enhancement** (Initial Implementation)
**Problem**: Claude agents tend to be overly optimistic and declare task completion without proper validation or testing.

**Solution**: Append specialized system prompt to make agents more pessimistic and paranoid about success declaration.

#### **Enhanced System Prompt Requirements**
- **Mandate Testing**: Never declare success without running and observing actual tests
- **Real Environment Validation**: Strongly prefer testing in actual execution environments over theoretical validation
- **Paranoid Verification**: Assume implementation might be broken until proven working through observation
- **Evidence-Based Success**: Require concrete evidence (test outputs, logs, working demos) before claiming completion

#### **Implementation**
- **CLI Flag Integration**: System prompt appended via `--append-system-prompt` CLI flag
- **Consistent Application**: Applied to all implementation tasks automatically via container startup script
- **Behavioral Change**: Shifts agent from "optimistic completion" to "pessimistic validation" with senior engineer mindset

#### **Enhanced System Prompt** (via `--append-system-prompt` CLI flag)
```
You are a highly capable, highly critical, somewhat paranoid, super senior principal Rust engineer.

CRITICAL: You are implementing code that MUST work in production. Be extremely paranoid about declaring success.

NEVER say you are "done" or "completed" unless you have:
1. Actually executed your code in a real environment
2. Observed it working correctly with your own eyes
3. Run regression tests and seen them pass
4. Verified all edge cases and error conditions

If you cannot test in a real environment, explicitly state what testing you were unable to perform and why the implementation should be considered incomplete until proper testing is done.

As a senior principal engineer, you understand that production failures are extremely costly.
Err on the side of being too cautious rather than too confident.
Apply the highest standards of code quality, testing, and verification.
```

### **Testing Integration Requirements**
This behavioral change directly connects to the Quality Validation Strategy and requires:
- **Live Testing Environment**: Access to real Kubernetes cluster or test environment
- **Regression Test Execution**: Ability to run comprehensive test suites
- **Result Observation**: Agent must see and interpret actual test outputs
- **Validation Tools**: MCP tools for environment interaction and test execution

## üìÅ **Documentation Integration**

### **Required Documentation Files** (from Platform Repo Git)
- **task.md**: Task description and requirements (committed by docs agent)
- **acceptance-criteria.md**: Definition of done (committed by docs agent)
- **architecture.md**: Technical approach and design (committed by docs agent)

### **Generated Context Files** (from ConfigMap Templates)
- **CLAUDE.md**: Project memory from `implementation/memory.md.hbs` template
- **coding-guidelines.md**: Rust-specific best practices and standards (template)
- **github-guidelines.md**: Commit message format and PR workflow (template)

### **Documentation Flow**
```
Platform Repo Git Clone ‚Üí Copy Task Docs ‚Üí ConfigMap Templates ‚Üí Claude Context
```

## üîß **MCP Tool Integration**

### **Architecture Components**
- **Toolman Server**: Remote pod in cluster (`http://toolman.mcp.svc.cluster.local:3000/mcp`)
- **Toolman Client**: Binary built into Claude container (`toolman-client`)
- **Task-Specific Filtering**: Each task gets relevant tools via client configuration

### **Configuration Structure**

#### **Claude Code MCP Configuration** (`.mcp.json`)
```json
{
  "mcpServers": {
    "toolman": {
      "type": "stdio",
      "command": "/usr/local/bin/toolman-client",
      "args": ["--working-dir", "/workspace"],
      "env": {
        "TOOLMAN_SERVER_URL": "${TOOLMAN_SERVER_URL:-http://toolman.mcp.svc.cluster.local:3000/mcp}"
      }
    }
  }
}
```

#### **Toolman Client Configuration** (`client-config.json`)
*Source: Platform repo tasks folder (e.g., `.taskmaster/tasks/task-1/client-config.json`)*
```json
{
  "remoteTools": [
    "brave-search_brave_web_search",
    "memory_create_entities",
    "kubernetes_listResources",
    "rustdocs_query_rust_docs"
  ],
  "localServers": {
    "filesystem": {
      "command": "npx",
      "args": ["-y", "@modelcontextprotocol/server-filesystem", "/workspace"],
      "tools": ["read_file", "write_file", "list_directory", "create_directory"],
      "workingDirectory": "project_root"
    }
  }
}
```

#### **Claude Settings Integration** (in `settings.json`)
```json
{
  "enableAllProjectMcpServers": true,
  "permissions": {
    "defaultMode": "acceptEdits",
    "allow": ["*"]
  }
}
```

### **Tool Selection & Usage Strategy**
- **Phase 1**: Manual tool selection via `client-config.json` files stored in platform repo tasks folder
- **Phase 2**: Enhanced tool configurations based on service patterns and success metrics
- **Phase 3**: Self-optimizing tool usage based on implementation outcomes

#### **Token Conservation Approach**
- **Explicit Workflows**: CLAUDE.md provides step-by-step tool usage patterns
- **Usage Examples**: Concrete examples of when and how to use each tool
- **Efficiency Patterns**: Pre-defined sequences like "research ‚Üí query docs ‚Üí implement"
- **Waste Prevention**: Clear guidance prevents exploratory tool usage

#### **Task-Specific Tool Patterns**
- **Rust Backend**:
  - Primary: `rustdocs_query_rust_docs` (always use before coding)
  - Research: `brave-search_brave_web_search` (crate ecosystem)
  - Memory: `memory_create_entities` (architectural decisions)
  - Tracking: `github_create_issue` (technical debt/bugs)
- **Frontend Services**:
  - Research: `brave-search_brave_web_search` (React/TS patterns)
  - Memory: `memory_create_entities` (component decisions)
  - Tracking: `github_create_issue` (UI/UX improvements)
- **Kubernetes/Infrastructure**:
  - Query: `kubernetes_listResources`, `kubernetes_getResource`
  - Research: `brave-search_brave_web_search` (K8s best practices)
  - Documentation: `memory_create_entities` (cluster decisions)

## üóÇÔ∏è **Repository Strategy**

### **Target Repository Workflow**
- **Dual Repository Access**: Clone both platform repo (for docs) and target repo (for implementation)
- **Documentation Source**: Task docs accessed from platform repo's `.taskmaster/docs/task-{id}/` directory
- **Documentation Transfer**: Copy docs from platform repo to workspace for easy Claude access
- **Tool Configuration Source**: Copy `client-config.json` from platform repo tasks folder
- **Implementation Target**: All code changes happen in target repo feature branch
- **Branch Strategy**: Each implementation creates feature branch in target repo (e.g., `feature/task-1-implementation`)
- **Authentication**: Uses repository-specific GitHub tokens and SSH keys for both repos

### **Workspace Structure**
```
/workspace/
‚îú‚îÄ‚îÄ platform-repo/       # Cloned TaskMaster platform repository
‚îÇ   ‚îú‚îÄ‚îÄ .taskmaster/docs/task-1/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ task.md
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ acceptance-criteria.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ architecture.md
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ target-repo/          # Cloned target implementation repository
‚îÇ   ‚îú‚îÄ‚îÄ .git/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ task-docs/           # Copied from platform-repo for easy access
‚îÇ   ‚îú‚îÄ‚îÄ task.md
‚îÇ   ‚îú‚îÄ‚îÄ acceptance-criteria.md
‚îÇ   ‚îî‚îÄ‚îÄ architecture.md
‚îú‚îÄ‚îÄ .claude/
‚îÇ   ‚îî‚îÄ‚îÄ settings.local.json
‚îú‚îÄ‚îÄ .mcp.json           # Claude Code MCP server configuration (template)
‚îú‚îÄ‚îÄ client-config.json  # Toolman client tool configuration (copied from platform repo)
‚îú‚îÄ‚îÄ CLAUDE.md           # Generated project memory (template)
‚îú‚îÄ‚îÄ coding-guidelines.md # Rust best practices (template)
‚îî‚îÄ‚îÄ github-guidelines.md # Git/PR workflow (template)
```

## üíæ **Multi-Attempt Support**

### **Retry Strategy Philosophy**
- **No Automatic Retries**: Preserve tokens, require manual intervention after failures
- **Manual Decision Process**: CTO/user analyzes failure and determines corrective actions
- **Token Conservation**: Avoid wasteful automatic retry loops until system is fine-tuned
- **Future Enhancement**: Eventually evolve to autonomous retry with learned corrections

### **Prompt Management for Attempts**

#### **First Attempt Prompt**
- **Standard Task Execution**: "Complete this task, here's your documentation"
- **Documentation-Driven**: Focus on task.md, acceptance-criteria.md, architecture.md
- **Clear Instructions**: Straightforward implementation guidance

#### **Subsequent Attempt Prompts**
- **Modified Prompts**: Include corrections, additional context, and lessons learned
- **Failure Analysis**: Incorporate understanding of what went wrong in previous attempts
- **Refinement Instructions**: Specific guidance on how to address identified issues

#### **Prompt Source Management**
- **MCP Server Control**: Can append or replace prompt content for retry attempts
- **CLI Integration**: Supports prompt modifications when submitting subsequent attempts
- **Manual Corrections**: User-provided context about failures and remediation approaches
- **Contextual Refinement**: Each attempt gets increasingly specific guidance

### **PVC Persistence Strategy**
- **Workspace PVC**: `workspace-{service_name}` for persistent storage
- **Historical Context**: Previous attempts remain available for learning
- **Directory Structure**: Organized by attempt number for clarity

### **Retry Workflow**
1. **Failed Implementation**: Agent completes with failure status
2. **Manual Analysis**: Human reviews failure, determines corrective action
3. **Prompt Modification**: Update prompt with specific corrections and guidance
4. **Context Preservation**: All previous work saved to PVC for reference
5. **New Attempt**: New container mounts same PVC with modified prompt
6. **Repository Cleanup**: Smart cleanup detects task retry vs new task:
   - **Same Task**: Preserves feature branch, resets to clean state for retry
   - **New Task**: Returns to base branch, creates fresh feature branch
7. **Historical Learning**: Agent can review previous attempt failures and build incrementally

### **Attempt Management**
```
/workspace/
‚îú‚îÄ‚îÄ platform-repo/      # Platform repo (fresh clone each time for latest docs)
‚îú‚îÄ‚îÄ target-repo/        # Target repo (persistent, updated each task)
‚îÇ   ‚îú‚îÄ‚îÄ .git/           # Persistent git history
‚îÇ   ‚îú‚îÄ‚îÄ trader/         # Service directory (persistent)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ attempt-1/          # First implementation attempt logs/artifacts
‚îú‚îÄ‚îÄ attempt-2/          # Retry with context from attempt-1 + corrections
‚îú‚îÄ‚îÄ current/           # Symlink to active attempt
‚îî‚îÄ‚îÄ shared/            # Shared resources across attempts
```

## üìã **Template System Integration**

### **Template Structure**
```
templates/implementation/
‚îú‚îÄ‚îÄ container.sh.hbs        # Container startup script (includes system prompt via CLI flag)
‚îú‚îÄ‚îÄ memory.md.hbs           # CLAUDE.md generation
‚îú‚îÄ‚îÄ settings.json.hbs       # Claude Code configuration
‚îú‚îÄ‚îÄ prompt.hbs             # Task-specific prompt content
‚îú‚îÄ‚îÄ mcp.json.hbs           # Claude Code MCP server configuration
‚îî‚îÄ‚îÄ hooks/                 # Lifecycle hooks
    ‚îú‚îÄ‚îÄ early-test.sh.hbs
    ‚îî‚îÄ‚îÄ stop-commit.sh.hbs
```

### **Template Data Context**
- **repository**: Target repo info (URL, branch, GitHub user)
- **working_directory**: Workspace path structure
- **task_id**: Specific task being implemented
- **service_name**: Target service/project name
- **attempts**: Current and previous attempt information

### **Memory Template Features**
- **Project Information**: Repository details and context
- **Implementation Context**: Current task and attempt information
- **Coding Standards**: Reference to guidelines and best practices
- **Common Commands**: Frequently used development commands
- **Tool Usage Patterns**: Explicit guidance on when and how to use available MCP tools

### **Tool Usage Guidance Templates**

#### **Rust Development Tool Patterns** (in `implementation/memory.md.hbs`)
```markdown
## Tool Usage Guidelines

### **Before Writing Code**
1. **Query Rust Documentation**: Always use `rustdocs_query_rust_docs` to research APIs before implementation
   - Example: Query for "HashMap insert method" before implementing data structures
   - Check for recent changes or best practices for the specific Rust version

2. **Research Crate Ecosystem**: Use `brave-search_brave_web_search` for crate research
   - Search for "rust {feature} crate recommendations 2024"
   - Verify crate maintenance status and recent updates

### **During Implementation**
1. **Architecture Decisions**: Use `memory_create_entities` to store key decisions
   - Document chosen patterns, rejected alternatives, and reasoning
   - Create entities for complex business logic explanations

2. **Error Handling**: Query `rustdocs_query_rust_docs` for error handling patterns
   - Research Result<T, E> and Option<T> best practices
   - Look up specific error types before implementing error handling

### **Testing & Validation**
1. **Test Patterns**: Use `rustdocs_query_rust_docs` for testing approaches
   - Research testing macros and assertion patterns
   - Query for async testing if implementing async code

2. **Documentation**: Use `github_create_issue` for discovered bugs or improvements
   - Document any workarounds or technical debt discovered
   - Track integration issues for future resolution
```

#### **Task-Specific Tool Selection** (in platform repo `client-config.json`)
*Task-specific tool configurations are stored in `.taskmaster/tasks/task-{id}/client-config.json` and copied during container setup.*

**Example for Rust microservice tasks:**
```json
{
  "remoteTools": [
    "rustdocs_query_rust_docs",      // Primary: Always query before coding
    "memory_create_entities",        // Store architectural decisions
    "brave-search_brave_web_search", // Research crates and patterns
    "github_create_issue"            // Track discovered issues
  ]
}
```

**Example for frontend service tasks:**
```json
{
  "remoteTools": [
    "brave-search_brave_web_search", // Research React patterns
    "memory_create_entities",        // Store component decisions
    "github_create_issue"            // Track UI/UX issues
  ]
}
```

## üîÑ **Container Setup Process**

### **Initialization Sequence**
1. **Environment Setup**: Configure Git authentication and workspace
2. **Platform Repository Operations**:
   - Clone platform repo (TaskMaster) to access task documentation
   - Copy task docs from `.taskmaster/docs/task-{id}/` to workspace
3. **Target Repository Operations** (Smart Detection & Cleanup):
   - **First Task for Service**: Clone target implementation repo (e.g., Trader) fresh
   - **Subsequent Tasks**: Repository already exists in PVC, smart cleanup and update:
     - **Cleanup Phase**: Reset uncommitted changes, clean untracked files
     - **Task Detection**: Check if continuing same task vs starting new task
     - **Same Task Retry**: Preserve feature branch, clean it up for retry
     - **New Task**: Return to base branch, cleanup old feature branches (keep last 3)
     - **Repository Update**: `git fetch origin`, `git pull origin {base_branch}`
   - Create or reuse feature branch for current implementation work
4. **Working Directory Setup**:
   - Navigate to service-specific directory (e.g., `/workspace/target-repo/trader/`)
   - Create directory if it doesn't exist (new service in monorepo)
5. **Template-Based Configuration**:
   - Copy `.mcp.json` to workspace root (from ConfigMap template)
   - Copy `client-config.json` to workspace (from ConfigMap template)
   - Copy guideline documents (from ConfigMap templates)
   - Ensure `toolman-client` binary is available in container
6. **Claude Preparation**: Generate CLAUDE.md from template, configure settings with MCP approval
7. **Implementation Start**: Execute Claude with task prompt and MCP tools available

### **Authentication Handling**
- **GitHub Tokens**: Repository-specific PAT from Kubernetes secrets
- **SSH Keys**: User-specific SSH keys for Git operations
- **Multi-Protocol**: Support both HTTPS and SSH Git URLs
- **Toolman Server**: Environment variable for remote server URL

### **MCP Integration Points**
- **`.mcp.json` Template**: Configures Claude Code to connect to toolman-client stdio
- **`client-config.json` Template**: Defines task-specific remote/local tools
- **Settings Template**: Auto-approves MCP servers (`enableAllProjectMcpServers: true`)
- **Container Script**: Places MCP configs in correct locations before Claude starts
- **Environment Variables**: `TOOLMAN_SERVER_URL` for dynamic server endpoint

## üéØ **Success Criteria**

### **Implementation Goals**
- **Documentation-First**: Agent reads generated docs, not JSON
- **Tool Optimization**: Right tools for each task type
- **Standards Compliance**: Follows coding and GitHub guidelines
- **Persistence**: Multi-attempt support with historical context
- **Repository Isolation**: Clean separation between TaskMaster and target repos

### **Future Enhancements**
- **Automated Tool Selection**: Docs agent recommends MCP tools
- **Template Library**: Predefined tool sets for common task types
- **Advanced Persistence**: Intelligent context sharing between attempts
- **Quality Gates**: Automated validation of implementation results

## üìù **Implementation Notes**

### **Current State**
- Template system restructured for docs/implementation separation
- Prep job eliminated, logic moved to container template
- Hook system made generic and extensible
- Memory and prompt templates unified for both agent types

### **Next Steps**
1. **Create MCP Templates**: Add `mcp.json.hbs` and `client-config.json.hbs` templates
2. **Update Settings Template**: Add `enableAllProjectMcpServers: true` to auto-approve MCP
3. **Update Container Template**: Add MCP config file placement and repository cleanup logic
4. **Create Guideline Documents**: Coding and GitHub standards templates
5. **Add Tool Usage Guidance**: Create explicit tool usage patterns in `implementation/memory.md.hbs`
   - Rust development workflows (rustdocs ‚Üí research ‚Üí implement ‚Üí document)
   - Frontend development patterns (React/TS research and component decisions)
   - Generic service patterns (minimal but effective tool usage)
6. **Test Tool Integration**: Verify Toolman client/server connectivity with task-specific tools
7. **Validate Repository Strategy**: Ensure target repo workflow and cleanup logic functions correctly
8. **Test Cleanup Scenarios**: Verify repository cleanup works for both new tasks and retries
9. **Implement Multi-Attempt Logic**: PVC-based persistence and context sharing

### **ConfigMap vs Git Source Clarification**

#### **ConfigMap Contents** (Templates Only)
- **CLAUDE.md**: Generated from `implementation/memory.md.hbs`
- **settings-local.json**: Generated from `implementation/settings.json.hbs`
- **mcp.json**: Generated from `implementation/mcp.json.hbs`
- **coding-guidelines.md**: Static Rust best practices template
- **github-guidelines.md**: Static Git/PR workflow template
- **Hook scripts**: Generated from `implementation/hooks/*.hbs`

#### **Git Repository Sources**
- **Task Documentation**: `task.md`, `acceptance-criteria.md`, `architecture.md` from platform repo
- **Target Codebase**: All implementation work happens in target repo
- **Platform Context**: Access to full TaskMaster project history and structure

## üîß **Component Analysis & Required Changes**

### **1. TaskRun Controller (`taskrun.rs`)**

#### **Current State:**
- ‚úÖ Template system organized into `docs/` and `implementation/` directories
- ‚úÖ Prep job eliminated and logic moved to container scripts
- ‚úÖ Generic hook system implemented
- ‚úÖ PVC management for persistent workspaces

#### **Required Changes:**
1. **Add Working Directory Support**:
   - Add `working_directory: Option<String>` to `TaskRunSpec`
   - Update CRD schema generation
   - Pass working directory to container template

2. **Repository Strategy Updates**:
   - Remove `markdown_files` from TaskRun spec (docs come from git clone)
   - Update ConfigMap to only include templates (CLAUDE.md, settings, MCP configs)
   - Add platform repository cloning logic to templates

3. **MCP Configuration Support**:
   - Add MCP config template generation to ConfigMap
   - Ensure templates include `.mcp.json` and `client-config.json`

#### **Template Data Updates:**
```rust
// Add to template context
data["working_directory"] = json!(service_working_dir);
data["platform_repo"] = json!(platform_repository_info);
data["target_repo"] = json!(tr.spec.repository);
```

---

### **2. Templates (`orchestrator-core/templates/`)**

#### **Current State:**
- ‚úÖ Implementation templates: `container.sh.hbs`, `memory.md.hbs`, `settings.json.hbs`, `prompt.hbs`
- ‚úÖ Hook system in place
- ‚ùå Missing MCP configuration templates

#### **Required New Templates:**

##### **`implementation/mcp.json.hbs`**
```json
{
  "mcpServers": {
    "toolman": {
      "type": "stdio",
      "command": "/usr/local/bin/toolman-client",
      "args": ["--working-dir", "/workspace"],
      "env": {
        "TOOLMAN_SERVER_URL": "${TOOLMAN_SERVER_URL:-http://toolman.mcp.svc.cluster.local:3000/mcp}"
      }
    }
  }
}
```



#### **Required Template Updates:**

##### **`implementation/settings.json.hbs`** - Add MCP Configuration
```json
{
  "enableAllProjectMcpServers": true,
  // ... existing settings
}
```

##### **`implementation/container.sh.hbs`** - Add System Prompt and Repository Cloning
```bash
# 1. Clone Platform Repository (for docs)
echo "=== PLATFORM REPOSITORY SETUP ==="
git clone {{platform_repo.url}} platform-repo
cd platform-repo && git checkout {{platform_repo.branch}}

# 2. Copy Task Documentation and Configuration
cp .taskmaster/docs/task-{{task_id}}/task.md /workspace/task-docs/
cp .taskmaster/docs/task-{{task_id}}/acceptance-criteria.md /workspace/task-docs/
cp .taskmaster/docs/task-{{task_id}}/architecture.md /workspace/task-docs/
cp .taskmaster/tasks/task-{{task_id}}/client-config.json /workspace/client-config.json

# 3. Target Repository Setup (Smart Detection)
cd /workspace
if [ -d "target-repo/.git" ]; then
    echo "=== EXISTING REPOSITORY DETECTED ==="
    cd target-repo

    # Cleanup Phase - Handle previous agent state
    echo "=== REPOSITORY CLEANUP ==="

    # Save current branch for logging
    CURRENT_BRANCH=$(git branch --show-current 2>/dev/null || echo "unknown")
    echo "Found repository on branch: $CURRENT_BRANCH"

    # Reset any uncommitted changes
    git reset --hard HEAD 2>/dev/null || true
    git clean -fd 2>/dev/null || true

    # Check if we're retrying the same task
    FEATURE_BRANCH="feature/task-{{task_id}}-implementation"
    if [ "$CURRENT_BRANCH" = "$FEATURE_BRANCH" ]; then
        echo "=== CONTINUING EXISTING TASK {{task_id}} ==="
        # For task retry - preserve the feature branch but clean it up
        git reset --hard HEAD
        git clean -fd
        echo "Cleaned up existing feature branch for retry"
    else
        echo "=== STARTING NEW TASK {{task_id}} ==="
        # For new task - return to base branch and cleanup old feature branches
        git fetch origin
        git checkout {{repository.branch}} 2>/dev/null || git checkout -b {{repository.branch}} origin/{{repository.branch}}
        git pull origin {{repository.branch}}

        # Clean up old feature branches (keep last 3 for debugging)
        git branch | grep "feature/task-" | head -n -3 | xargs -r git branch -D 2>/dev/null || true

        # Create fresh feature branch for new task
        git checkout -b "$FEATURE_BRANCH"
        echo "Created fresh feature branch: $FEATURE_BRANCH"
    fi
else
    echo "=== CLONING FRESH REPOSITORY ==="
    git clone {{repository.url}} target-repo
    cd target-repo

    # Create feature branch for this implementation
    git checkout -b "feature/task-{{task_id}}-implementation"
    echo "Created initial feature branch for task {{task_id}}"
fi

# 4. Set Working Directory
{{#if working_directory}}
WORK_DIR="{{working_directory}}"
{{else}}
WORK_DIR="{{service_name}}"
{{/if}}
mkdir -p "$WORK_DIR" && cd "$WORK_DIR"

# 5. MCP Configuration Setup
cp /config/mcp.json /workspace/.mcp.json
# client-config.json already copied from platform repo in step 2

# 6. Execute Claude with Enhanced System Prompt
SYSTEM_PROMPT="You are a highly capable, highly critical, somewhat paranoid, super senior principal Rust engineer.

CRITICAL: You are implementing code that MUST work in production. Be extremely paranoid about declaring success.

NEVER say you are \"done\" or \"completed\" unless you have:
1. Actually executed your code in a real environment
2. Observed it working correctly with your own eyes
3. Run regression tests and seen them pass
4. Verified all edge cases and error conditions

If you cannot test in a real environment, explicitly state what testing you were unable to perform and why the implementation should be considered incomplete until proper testing is done.

As a senior principal engineer, you understand that production failures are extremely costly.
Err on the side of being too cautious rather than too confident.
Apply the highest standards of code quality, testing, and verification."

claude --append-system-prompt "$SYSTEM_PROMPT" "{{{prompt_content}}}"
```

##### **Guideline Templates** (New)
- **`implementation/coding-guidelines.md.hbs`** - Rust best practices
- **`implementation/github-guidelines.md.hbs`** - Commit/PR workflow

---

### **3. CLI (`orchestrator-cli`)**

#### **Current State:**
- ‚úÖ `task submit` command with TaskMaster integration
- ‚úÖ Repository and GitHub user support
- ‚ùå Missing working directory parameter

#### **Required Changes:**

##### **Add Working Directory and Prompt Modification Parameters**
```rust
// In TaskCommands::Submit
#[arg(long, short = 'w')]
working_directory: Option<String>,

#[arg(long)]
prompt_modification: Option<String>,

#[arg(long, value_enum, default_value = "append")]
prompt_mode: PromptMode,
```

##### **Update Task Submission Logic**
```rust
// In submit_task_simplified()
let repository = RepositorySpec {
    url: repo_url.to_string(),
    branch: branch.to_string(),
    github_user: github_user.to_string(),
    working_directory: working_directory.clone(), // NEW
    platform_repository: detect_platform_repo()?, // NEW
};
```

##### **Add Platform Repository Detection**
```rust
fn detect_platform_repo() -> Result<RepositorySpec> {
    // Auto-detect current git repo for platform docs
    let output = Command::new("git")
        .args(&["remote", "get-url", "origin"])
        .output()?;
    let url = String::from_utf8(output.stdout)?.trim().to_string();

    Ok(RepositorySpec {
        url,
        branch: current_branch()?,
        github_user: extract_github_user(&url)?,
    })
}
```

---

### **4. MCP Server (`mcp-server`)**

#### **Current State:**
- ‚úÖ `submit_implementation_task` tool implemented
- ‚úÖ CLI parameter passing
- ‚ùå Missing working directory support
- ‚ùå Missing platform repository detection

#### **Required Changes:**

##### **Update Tool Schema**
```rust
// In tools.rs - add working_directory and prompt modification parameters
"working_directory": {
    "type": "string",
    "description": "Working directory within target repository (defaults to service name)"
},
"platform_repository_url": {
    "type": "string",
    "description": "Platform repository URL for documentation access"
},
"prompt_modification": {
    "type": "string",
    "description": "Additional prompt content for retry attempts (appends to or replaces base prompt)"
},
"prompt_mode": {
    "type": "string",
    "enum": ["append", "replace"],
    "description": "Whether to append to or replace the base prompt for retry attempts"
}
```

##### **Update Tool Implementation**
```rust
// In main.rs - extract working directory
let working_directory = params_map
    .get("working_directory")
    .and_then(|v| v.as_str());

// Add to CLI args if specified
if let Some(wd) = working_directory {
    args.extend(&["--working-directory", wd]);
}
```

---

### **5. TaskRun CRD**

#### **Current State:**
- ‚úÖ Repository specification with URL, branch, github_user
- ‚ùå Missing working directory field
- ‚ùå Still includes markdown_files (should be removed)

#### **Required Schema Changes:**
```rust
pub struct TaskRunSpec {
    // ... existing fields

    /// Optional working directory within target repository (defaults to service_name)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub working_directory: Option<String>,

    /// Platform repository for documentation access
    #[serde(skip_serializing_if = "Option::is_none")]
    pub platform_repository: Option<RepositorySpec>,

    // REMOVE: markdown_files (docs come from git clone)
    // pub markdown_files: Vec<MarkdownFile>, // DELETE THIS
}
```

---

### **Legacy Cleanup Required:**
- **Remove**: `markdown_files` field from TaskRun CRD
- **Remove**: ConfigMap markdown file injection (except templates)
- **Remove**: Prep job references (already done)
- **Update**: All template constants to use new MCP templates

### **Naming Inconsistencies to Fix:**
- **ConfigMap Key Names**: Should match exact file names stored on disk
  - Current: ConfigMap uses "settings-local.json" but disk uses ".claude/settings.local.json"
  - Fix: Use consistent naming pattern between ConfigMap keys and destination file names
  - Apply same principle to all template-generated files

---

## üìã **Implementation Priority Matrix**

### **Phase 1: Core Infrastructure**
1. **TaskRun CRD Schema Updates** - Add working_directory, platform_repository fields
2. **New MCP Templates** - Create `mcp.json.hbs`
3. **System Prompt Enhancement** - Add paranoid verification via `--append-system-prompt` CLI flag in container template
4. **Container Template Updates** - Add platform repo cloning, working directory logic, and client-config.json copying
5. **Settings Template Updates** - Add MCP auto-approval configuration and Claude MCP server enablement
6. **Claude MCP Server Access** - Enable external MCP server and create Kubernetes service for future sidecar monitoring

### **Phase 2: CLI & MCP Integration**
1. **CLI Parameter Addition** - Add working directory support
2. **MCP Server Updates** - Add working directory and platform repo support
3. **Platform Repository Detection** - Auto-detect current repo for docs access
4. **Guideline Templates** - Create coding and GitHub standards documents
5. **Sidecar Research** - Investigate Claude MCP server capabilities and unattended operation feasibility

### **Phase 3: Legacy Cleanup & Testing**
1. **Remove markdown_files** - Clean up TaskRun CRD and controller logic
2. **Template Constant Updates** - Update all template references
3. **Telemetry Verification** - Verify Grafana, Victoria Logs, and Victoria Metrics still function after architectural changes
4. **Integration Testing** - Verify end-to-end flow works
5. **Documentation Updates** - Update all references to new flow

### **Phase 4: Enhanced Project Management** (Future)
1. **Quality Validation Process** - Implement chosen QA strategy (integrated, dedicated, or hybrid)
2. **GitHub Projects Integration** - Choose and implement sync approach (GitHub Actions, Orchestrator, CLI, or Hybrid)
3. **Agent Assignment Automation** - Configure agent-to-GitHub-user mapping and assignment rules
4. **Visual Progress Tracking** - Enable stakeholder visibility through GitHub Projects kanban boards
5. **Sidecar Agent Monitoring** - Implement supervisory agent system based on Claude performance analysis

---

## üéØ **Success Criteria & Verification**

### **Functional Requirements:**
- ‚úÖ Platform repo cloned for task documentation access
- ‚úÖ Target repo cloned with proper working directory setup
- ‚úÖ MCP tools available and task-specific
- ‚úÖ Claude Code auto-approves MCP servers
- ‚úÖ Monorepo working directory support (service-name or custom)
- ‚úÖ Multi-attempt persistence via PVC

### **Technical Requirements:**
- ‚úÖ No breaking changes to existing task submission flow
- ‚úÖ Backward compatibility for existing TaskRuns
- ‚úÖ Template-based configuration (no hardcoded values)
- ‚úÖ Proper authentication for both repositories
- ‚úÖ Error handling for missing directories/repos

### **Verification Steps:**
1. **Submit Implementation Task** via MCP tool
2. **Verify Platform Repo Clone** and doc copying
3. **Verify Target Repo Clone** and working directory setup
4. **Verify MCP Configuration** and tool availability
5. **Test Working Directory Logic** (default vs custom)
6. **Test Multi-Attempt Scenarios** with PVC persistence
7. **Verify Telemetry Pipeline** - Ensure Grafana, Victoria Logs, and Victoria Metrics are collecting data from updated TaskRun controller

## üìä **Telemetry & Monitoring Verification**

### **Existing Telemetry Stack**
The platform includes a comprehensive monitoring setup that needs verification after architectural changes:

#### **Components**
- **Grafana**: Visual dashboards and alerting for metrics and logs
- **Victoria Logs**: High-performance log aggregation and storage
- **Victoria Metrics**: Time-series metrics collection and storage
- **OTLP (OpenTelemetry)**: Telemetry data collection from TaskRun controllers

#### **Verification Requirements**
After the significant controller and template changes, verify:

1. **TaskRun Controller Telemetry**:
   - Controller metrics are being collected (task success/failure rates, duration, etc.)
   - Log output from new container templates is being captured
   - Claude agent execution logs are flowing to Victoria Logs
   - Error telemetry from failed implementations is being tracked

2. **Grafana Dashboards**:
   - Executive overview dashboard displays current task status
   - Engineering metrics dashboard shows implementation agent performance
   - Operations monitoring dashboard tracks resource usage and failures
   - Cost management dashboard reflects updated resource allocation

3. **Alert Rules**:
   - Task failure alerts trigger correctly
   - Resource exhaustion alerts are functional
   - Agent timeout alerts work with new deadlines
   - Quality gate failure alerts (when implemented)

4. **Log Collection**:
   - Implementation agent logs are captured and searchable
   - MCP tool usage logs are available for token optimization
   - Repository operation logs are collected for debugging
   - Container startup and shutdown logs are preserved

#### **Telemetry Integration Points**
Verify telemetry is working at these critical points:
- **TaskRun Creation**: Metrics on task submission and configuration
- **Container Startup**: Logs from repository cloning and MCP setup
- **Claude Execution**: Agent output, tool usage, and completion status
- **Repository Operations**: Git operations, branch management, and commit creation
- **Error Conditions**: Failed attempts, timeout scenarios, and retry logic

#### **Performance Metrics to Validate**
- **Task Duration**: Time from submission to completion
- **Repository Operations**: Clone/update times for platform and target repos
- **Agent Performance**: Token usage, tool efficiency, success rates
- **Resource Utilization**: PVC usage, memory consumption, CPU utilization
- **Failure Analysis**: Common failure patterns and retry success rates

### **Telemetry Verification Process**
1. **Submit test implementation task** and monitor telemetry flow
2. **Check Grafana dashboards** for real-time data updates
3. **Verify log aggregation** in Victoria Logs with search functionality
4. **Test alert rules** by triggering failure conditions
5. **Validate metrics collection** in Victoria Metrics with proper labels
6. **Review telemetry configuration** in updated settings templates

## üîç **Quality Validation Strategy**

### **Future Implementation Approaches**
Quality validation will be implemented after initial system validation. Two primary approaches are under consideration:

#### **Approach 1: Integrated Agent Testing** (Aligns with System Prompt Requirements)
- **Implementation Agent Responsibility**: Agent performs full regression testing against live Kubernetes environment
- **Advantages**: Single agent ownership, immediate feedback loop, context-aware testing, satisfies paranoid verification requirements
- **Enhanced System Prompt Support**: Agent can directly observe test execution and results, enabling confident success declaration
- **Testing Requirements**: Live Kubernetes access, regression test execution, real environment validation
- **Concerns**: Potential context overload, token consumption, complexity management
- **Use Case**: Smaller services where testing context is manageable

#### **Approach 2: Dedicated QA Agent** (Requires Coordination for System Prompt Compliance)
- **Separate QA Agent**: Specialized agent handles testing and validation after implementation completion
- **Advantages**: Focused responsibility, specialized tooling, reduced implementation agent complexity
- **System Prompt Challenge**: Implementation agent cannot declare success without observing tests, requires handoff coordination
- **Implementation**: QA agent receives implementation results and runs comprehensive test suites, reports back to implementation agent
- **Coordination Required**: Implementation agent must wait for and observe QA agent test results before success declaration
- **Use Case**: Complex services requiring extensive testing scenarios

#### **Hybrid Approach** (Optimal for System Prompt Requirements)
- **Basic Validation**: Implementation agent runs unit tests and basic smoke tests, can observe immediate results
- **Comprehensive Testing**: QA agent handles integration and regression testing, reports detailed results back
- **Quality Gates**: Configurable validation levels based on service criticality
- **System Prompt Satisfaction**: Implementation agent observes basic tests directly, receives comprehensive test evidence from QA agent
- **Optimal Balance**: Reduces context overload while ensuring paranoid verification requirements are met

### **Decision Timeline**
Quality validation approach will be determined after first implementation attempts based on:
- Agent performance with testing context
- Token consumption patterns
- Implementation complexity observations
- Success/failure rates of different approaches
- **System Prompt Effectiveness**: How well the enhanced paranoid verification prompt works with different testing approaches

### **Initial Implementation**
First implementations will rely on manual validation to establish baseline patterns before automating quality gates.

## ‚ùì **Resolved Design Questions**

### **Tool Discovery** ‚úÖ
**Solution**: Manual tool selection via `client-config.json` files stored in platform repo tasks folder (`.taskmaster/tasks/task-{id}/client-config.json`). No automated tool recommendation needed initially.

### **Attempt Coordination** ‚úÖ
**Solution**: Manual coordination by CTO via MCP server. No automatic concurrent attempt handling - relies on human decision-making for retry scenarios.

### **Quality Validation** üìã
**Status**: Future implementation - see Quality Validation Strategy section above. Decision pending after initial system validation.

## üîÑ **GitHub Projects Integration** (Future Enhancement)

### **TaskMaster Sync Utility**
**Status**: Lower priority, depends on QA process implementation. Multiple integration options under consideration.

#### **Overview**
The platform includes a high-performance Rust CLI tool ([TaskMaster Sync](../tasks/)) that synchronizes TaskMaster tasks with GitHub Projects v2 for visual project management and team collaboration.

#### **Core Capabilities**
- **üöÄ Delta Sync**: Intelligent synchronization with state tracking to minimize API calls
- **üìä GitHub Projects Integration**: Full support for GitHub Projects v2 with custom fields
- **üè∑Ô∏è Multi-tag Support**: Map different TaskMaster tags to different GitHub projects
- **üë• Agent Assignment**: Automatically assign tasks to team members based on service ownership
- **üìà Progress Tracking**: Visual progress bars and detailed sync statistics
- **üéØ Subtask Hierarchy**: Preserves task hierarchies and relationships in GitHub
- **‚ö° Cross Platform**: Native binaries for macOS and Linux

#### **Field Mappings**
| TaskMaster Field | GitHub Project Field | Type | Description |
|-----------------|---------------------|------|-------------|
| `id` | TM_ID | Text | Unique TaskMaster task identifier |
| `status` | Status | Single Select | Task status (pending, in-progress, done, QA Review) |
| `priority` | Priority | Single Select | Task priority (high, medium, low) |
| `assignee` | Assignee | Text | GitHub username of assigned team member |
| `dependencies` | Dependencies | Text | Comma-separated list of dependent task IDs |
| `testStrategy` | Test Strategy | Text | Testing approach and validation steps |

#### **Integration Options Under Consideration**

##### **Option 1: GitHub Actions Integration** (GitHub CI/CD Pipeline)
```yaml
# Auto-sync when TaskMaster tasks change
- uses: 5dlabs/taskmaster-sync@v1
  with:
    project-number: 123
    tag: ${{ github.ref_name }}
```

**Advantages:**
- ‚úÖ Automatic synchronization on every TaskMaster change
- ‚úÖ No additional infrastructure required
- ‚úÖ Leverages existing GitHub workflow triggers
- ‚úÖ Built-in authentication and permissions

**Considerations:**
- ‚ö†Ô∏è Requires GitHub Actions to be triggered by TaskMaster changes
- ‚ö†Ô∏è May have timing dependencies with docs generation workflow

##### **Option 2: Orchestrator Integration** (Platform-Native)
Integrate sync functionality directly into the TaskRun controller:

```rust
// In TaskRun controller after successful task completion
if task_run.spec.sync_to_github {
    sync_task_to_github_project(&task_run).await?;
}
```

**Advantages:**
- ‚úÖ Real-time synchronization with task lifecycle
- ‚úÖ Direct integration with implementation workflow
- ‚úÖ Can sync individual task completions immediately
- ‚úÖ Leverages existing Kubernetes authentication

**Considerations:**
- ‚ö†Ô∏è Adds complexity to core orchestrator
- ‚ö†Ô∏è Requires GitHub API credentials in cluster
- ‚ö†Ô∏è May impact TaskRun performance

##### **Option 3: CLI Integration** (Manual/Scripted)
Add sync commands to the orchestrator CLI:

```bash
# Manual sync after task completion
orchestrator sync-github --service trader --tag master --project 123

# Bulk sync all services
orchestrator sync-github --all --project 123
```

**Advantages:**
- ‚úÖ Manual control over synchronization timing
- ‚úÖ Can be scripted or integrated into custom workflows
- ‚úÖ Minimal impact on core platform performance
- ‚úÖ Easy to test and validate before automation

**Considerations:**
- ‚ö†Ô∏è Requires manual intervention or custom automation
- ‚ö†Ô∏è May lead to inconsistent synchronization

##### **Option 4: Hybrid Approach** (Recommended for Future)
Combine multiple integration methods based on use case:

- **Real-time sync**: Orchestrator integration for immediate task completion updates
- **Bulk sync**: GitHub Actions for comprehensive project synchronization
- **Manual sync**: CLI commands for testing and ad-hoc synchronization

#### **Implementation Dependencies**
The GitHub Projects integration has several dependencies that align with the QA process:

1. **Quality Validation Process**: Sync should occur after successful quality validation
2. **Task Status Management**: "QA Review" status field requires QA workflow definition
3. **Agent Assignment Rules**: Automatic assignment needs agent-to-GitHub-user mapping
4. **Project Lifecycle**: Project creation and field setup need to be automated

#### **Configuration Requirements**
```json
{
  "version": "1.0.0",
  "organization": "your-org",
  "project_mappings": {
    "master": {
      "project_number": 123,
      "project_id": "PVT_kwDOAM8-ec4AKnVx",
      "last_sync": "2024-01-15T10:30:00Z"
    }
  },
  "agent_mapping": {
    "claude-agent-1": {
      "github_username": "claude-agent-1-5dlabs",
      "services": ["trader", "auth", "analytics"],
      "role": "Senior Software Engineer"
    },
    "qa": {
      "github_username": "qa0-5dlabs",
      "services": ["*"],
      "role": "Quality Assurance Engineer"
    }
  }
}
```

#### **Future Implementation Timeline**
1. **Phase 1**: Implement core QA validation workflow
2. **Phase 2**: Define agent assignment and project lifecycle management
3. **Phase 3**: Choose integration approach based on operational experience
4. **Phase 4**: Implement selected GitHub Projects synchronization method

#### **Benefits for Project Management**
- **Visual Progress Tracking**: GitHub Projects provides kanban-style visual management
- **Team Collaboration**: Native GitHub integration with notifications and mentions
- **Stakeholder Visibility**: Non-technical stakeholders can track progress without CLI tools
- **Audit Trail**: Complete history of task changes and assignments
- **Integration**: Seamless integration with existing GitHub workflows and tooling

## ü§ñ **Sidecar Agent Monitoring System** (Future Enhancement)

### **Supervisory Agent Concept**
**Status**: Future implementation based on Claude performance analysis. Immediate infrastructure preparation required.

#### **Overview**
A secondary AI agent (Gemini, Grok, or alternative) that monitors Claude's implementation behavior to prevent confusion, endless loops, and off-track execution. The sidecar agent acts as a supervisor, not a replacement, ensuring Claude stays focused and productive.

#### **Monitoring Data Sources**
The sidecar agent would analyze multiple data streams:

1. **Codebase Analysis**:
   - Real-time code changes and patterns
   - Implementation progress against acceptance criteria
   - Code quality and adherence to standards
   - Repetitive or contradictory changes

2. **Acceptance Criteria Tracking**:
   - Progress against defined success criteria
   - Gap analysis between current state and requirements
   - Identification of overlooked requirements

3. **Metrics & Telemetry**:
   - Token usage patterns and efficiency
   - Implementation time vs. task complexity
   - Tool usage effectiveness
   - Error rates and retry patterns

4. **Log Analysis**:
   - Claude's reasoning and decision patterns
   - Repeated error messages or stuck behavior
   - Tool usage patterns and effectiveness
   - Testing and validation attempts

#### **Intervention Scenarios**
The sidecar agent would intervene when detecting:

- **Confusion Patterns**: Claude making contradictory changes or expressing uncertainty repeatedly
- **Infinite Loops**: Attempting the same failed approach multiple times
- **Scope Creep**: Working on tasks outside acceptance criteria
- **Stuck Behavior**: No meaningful progress for extended periods
- **Tool Misuse**: Inefficient or inappropriate tool usage patterns
- **Testing Avoidance**: Declaring success without proper validation (aligns with system prompt requirements)

#### **Intervention Methods**
- **Context Injection**: Provide additional context or clarification to Claude
- **Approach Redirection**: Suggest alternative implementation strategies
- **Reset Recommendation**: Recommend task restart with lessons learned
- **Escalation**: Alert human operators for complex issues
- **Resource Optimization**: Suggest tool usage improvements

### **Immediate Implementation Requirements**

#### **Claude MCP Server Access** (Phase 1 Implementation)
Enable external access to Claude's MCP server for monitoring:

##### **Settings Configuration**
```json
{
  "enableAllProjectMcpServers": true,
  "mcpServers": {
    "internal": {
      "type": "stdio",
      "command": "claude-mcp-server",
      "args": ["--port", "8080", "--host", "0.0.0.0"],
      "env": {
        "MCP_SERVER_MODE": "external"
      }
    }
  }
}
```

##### **Kubernetes Service Setup**
Create direct pod access for sidecar agent connectivity:

```yaml
# Direct pod service for MCP access
apiVersion: v1
kind: Service
metadata:
  name: claude-agent-direct
  namespace: orchestrator
spec:
  selector:
    app: claude-agent
    task-id: "{{task_id}}"  # Target specific agent instance
  ports:
  - name: mcp-server
    port: 8080
    targetPort: 8080
    protocol: TCP
  type: ClusterIP
```

##### **Container Template Updates**
Update `implementation/container.sh.hbs` to start Claude's MCP server:

```bash
# Start Claude MCP server for external monitoring
claude --mcp-server --port 8080 --host 0.0.0.0 &
MCP_SERVER_PID=$!

# Continue with normal Claude execution
claude --append-system-prompt "$SYSTEM_PROMPT" "{{{prompt_content}}}"

# Cleanup MCP server on exit
kill $MCP_SERVER_PID 2>/dev/null || true
```

#### **Research & Discovery Requirements**
Before implementing the sidecar agent, research needed on:

1. **Claude MCP Server Capabilities**:
   - Can Claude's MCP server run in unattended mode?
   - What monitoring APIs are available?
   - How to access real-time execution state?
   - Authentication and security considerations?

2. **Multi-Agent Coordination**:
   - How to coordinate between Claude and sidecar agent?
   - Shared state management between agents?
   - Conflict resolution when agents disagree?
   - Resource sharing and isolation?

3. **Integration Architecture**:
   - Sidecar container vs. separate pod deployment?
   - Communication protocols between agents?
   - Monitoring data collection and storage?
   - Performance impact on main implementation?

4. **Unattended Operation**:
   - Can supervision work without human oversight?
   - Automated escalation and fallback procedures?
   - Decision authority hierarchy between agents?
   - Audit trail for supervisory interventions?

### **Future Implementation Architecture**

#### **Sidecar Container Deployment**
```yaml
# Enhanced TaskRun with sidecar agent
spec:
  template:
    spec:
      containers:
      - name: claude-agent
        image: claude-code:latest
        # ... existing configuration

      - name: supervisor-agent
        image: supervisor-agent:latest
        env:
        - name: CLAUDE_MCP_ENDPOINT
          value: "http://localhost:8080/mcp"
        - name: SUPERVISOR_MODEL
          value: "gemini-pro"  # or grok, etc.
        - name: MONITORING_INTERVAL
          value: "30s"
        volumeMounts:
        - name: workspace
          mountPath: /workspace
          readOnly: true  # Monitor only, no modifications
```

#### **Monitoring Dashboard Integration**
Extend Grafana dashboards to include sidecar agent metrics:
- Intervention frequency and types
- Claude behavior patterns and improvements
- Supervisor effectiveness metrics
- Cost analysis (dual-agent vs. single-agent failures)

### **Decision Timeline**
1. **Phase 1**: Implement Claude MCP server access and Kubernetes service setup
2. **Research Phase**: Investigate Claude MCP capabilities and unattended operation feasibility
3. **Performance Analysis**: Monitor Claude's behavior patterns in real implementations
4. **Sidecar Decision**: Based on Claude performance, decide if sidecar agent is needed
5. **Implementation**: If needed, implement sidecar agent with chosen model (Gemini/Grok)

### **Success Criteria for Sidecar Implementation**
- **Reduced Task Failures**: Lower failure rates due to agent confusion or stuck behavior
- **Improved Efficiency**: Faster task completion through early intervention
- **Better Quality**: Higher implementation quality through continuous monitoring
- **Cost Effectiveness**: Sidecar agent cost justified by reduced failures and retries
- **Minimal Interference**: Sidecar doesn't slow down or confuse primary agent